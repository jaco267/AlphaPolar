# Program
program_seed=0 # assign a program seed
program_auto_seed=false # true for assigning a random seed automatically
program_quiet=false # true for silencing the error message

# Actor
actor_num_simulation=16 # simulation number of MCTS
actor_mcts_puct_base=19652 # hyperparameter for puct_bias in the PUCT formula of MCTS, determining the level of exploration
actor_mcts_puct_init=1.25 # hyperparameter for puct_bias in the PUCT formula of MCTS
actor_mcts_reward_discount=1 # discount factor for calculating Q values
actor_mcts_value_rescale=false # true for games whose rewards are not bounded in [-1, 1], e.g., Atari games
actor_mcts_think_batch_size=1 # the MCTS selection batch size; only works when running console
actor_mcts_think_time_limit=0 # the MCTS time limit in seconds, 0 represents disabling time limit (only uses actor_num_simulation); only works when running console
actor_select_action_by_count=false # true for selecting the action by the maximum MCTS count; should not be true together with actor_select_action_by_softmax_count
actor_select_action_by_softmax_count=true # true for selecting the action by the propotion of MCTS count; should not be true together with actor_select_action_by_count
actor_select_action_softmax_temperature=1 # the softmax temperature when using actor_select_action_by_softmax_count
# true for decaying the temperature based on training iteration; set 1, 0.5, and 0.25 for 0%-50%, 50%-75%, and 75%-100% of total iterations, respectively
actor_select_action_softmax_temperature_decay=false
actor_use_random_rotation_features=true # true for randomly rotating input features; only supports in alphazero
actor_use_dirichlet_noise=false # true for adding dirchlet noise to the policy
actor_dirichlet_noise_alpha=0.03 # hyperparameter for dirchlet noise, usually (1 / sqrt(number of actions))
actor_dirichlet_noise_epsilon=0.25 # hyperparameter for dirchlet noise
actor_use_gumbel=true # true for enabling Gumbel Zero
actor_use_gumbel_noise=true # true for adding Gumbel noise to the policy
actor_gumbel_sample_size=16 # hyperparameter for Gumbel Zero; the number of sampled actions
actor_gumbel_sigma_visit_c=50 # hyperparameter for the monotonically increasing transformation sigma in Gumbel Zero
actor_gumbel_sigma_scale_c=1 # hyperparameter for the monotonically increasing transformation sigma in Gumbel Zero
actor_resign_threshold=-0.9 # the threshold determining when to resign in the actor

# Zero
zero_num_threads=4 # the number of threads that the zero server uses for zero training
zero_num_parallel_games=32 # the number of games to be run in parallel for zero training
zero_server_port=9999 # the port number to host the server; workers should connect to this port number
zero_training_directory= # the output directory name for storing training results
zero_num_games_per_iteration=2000 # the nunmber of games to play in each iteration
zero_start_iteration=0 # the first iteration of training; usually 1 unless continuing with previous training
zero_end_iteration=50 # the last iteration of training
zero_replay_buffer=20 # hyperparameter for replay buffer; replay buffer stores (zero_replay_buffer x zero_num_games_per_iteration) games/sequences
zero_disable_resign_ratio=0.1 # the probability to keep playing when the winrate is below actor_resign_threshold
zero_actor_intermediate_sequence_length=0 # the max sequence length when running self-play; usually 0 (unlimited) for board games, 200 for atari games
zero_actor_ignored_command=reset_actors # the commands to ignore by the actor; format: command1 command2 ...
zero_server_accept_different_model_games=true # true for accepting self-play games generated by out-of-date model

# Learner
learner_use_per=false # true for enabling Prioritized Experience Replay
learner_per_alpha=1 # hyperparameter for PER that controlls the probability of sampling transition
learner_per_init_beta=1 # hyperparameter for PER that sets the initial beta value of linearly annealing
learner_per_beta_anneal=true # hyperparameter for PER that enables linearly anneal beta based on current training iteration
learner_training_step=500 # the number of training steps for updating the model in each iteration
learner_training_display_step=100 # the training step interval to display training information
learner_batch_size=1024 # the batch size for training
learner_muzero_unrolling_step=5 # the number of steps to unroll for muzero training
learner_n_step_return=10 # the number of steps to calculate the n-step value; usually 0 for board games, 10 for atari games
learner_learning_rate=0.02 # hyperparameter for learning rate
learner_momentum=0.9 # hyperparameter for momentum
learner_weight_decay=0.0001 # hyperparameter for weight decay
learner_value_loss_scale=1 # hyperparameter for scaling of the value loss
learner_num_thread=8 # the number of threads for training

# Network
nn_file_name= # the file name of model weights
nn_num_blocks=1 # hyperparameter for the model; the number of the residual blocks
nn_num_hidden_channels=256 # hyperparameter for the model; the size of the hidden channels in residual blocks
nn_num_value_hidden_channels=256 # hyperparameter for the model; the size of the hidden channels in the value network
nn_type_name=alphazero # the type of training algorithm and network: alphazero/muzero
network_type=conv # transformer or conv network

# Environment
env_board_size=16 # the size of board
kern_search_game_len=1000 # kern search max game len
rand_init_step=7 # randomly push this number of stone on the 2nd row after reset to add more data diversity

