[2025-04-09 17:31:07] [command] train weight_iter_0.pkl 1 1
[2025-04-09 17:31:30] nn step 100, lr: 0.02.
	loss_policy: 1.20338
	accuracy_policy: 0.26855
	loss_value: 1.36895
[2025-04-09 17:31:51] nn step 200, lr: 0.02.
	loss_policy: 0.94583
	accuracy_policy: 0.39696
	loss_value: 1.07465
[2025-04-09 17:32:12] nn step 300, lr: 0.02.
	loss_policy: 0.84708
	accuracy_policy: 0.42682
	loss_value: 1.06238
[2025-04-09 17:32:32] nn step 400, lr: 0.02.
	loss_policy: 0.81041
	accuracy_policy: 0.44137
	loss_value: 1.04953
[2025-04-09 17:32:53] nn step 500, lr: 0.02.
	loss_policy: 0.79066
	accuracy_policy: 0.44911
	loss_value: 1.03857
Optimization_Done 500
[2025-04-09 17:48:32] [command] train weight_iter_500.pkl 1 2
[2025-04-09 17:48:55] nn step 600, lr: 0.02.
	loss_policy: 1.22383
	accuracy_policy: 0.31987
	loss_value: 1.11294
[2025-04-09 17:49:19] nn step 700, lr: 0.02.
	loss_policy: 1.16311
	accuracy_policy: 0.33925
	loss_value: 1.09726
[2025-04-09 17:49:42] nn step 800, lr: 0.02.
	loss_policy: 1.14607
	accuracy_policy: 0.34304
	loss_value: 1.0907
[2025-04-09 17:50:06] nn step 900, lr: 0.02.
	loss_policy: 1.13839
	accuracy_policy: 0.34527
	loss_value: 1.08196
[2025-04-09 17:50:29] nn step 1000, lr: 0.02.
	loss_policy: 1.12382
	accuracy_policy: 0.35038
	loss_value: 1.07875
Optimization_Done 1000
[2025-04-09 18:07:15] [command] train weight_iter_1000.pkl 1 3
[2025-04-09 18:07:38] nn step 1100, lr: 0.02.
	loss_policy: 1.27789
	accuracy_policy: 0.3491
	loss_value: 1.07679
[2025-04-09 18:08:02] nn step 1200, lr: 0.02.
	loss_policy: 1.26225
	accuracy_policy: 0.35095
	loss_value: 1.0716
[2025-04-09 18:08:26] nn step 1300, lr: 0.02.
	loss_policy: 1.24733
	accuracy_policy: 0.35812
	loss_value: 1.06662
[2025-04-09 18:08:50] nn step 1400, lr: 0.02.
	loss_policy: 1.24738
	accuracy_policy: 0.35539
	loss_value: 1.05936
[2025-04-09 18:09:14] nn step 1500, lr: 0.02.
	loss_policy: 1.23445
	accuracy_policy: 0.36318
	loss_value: 1.05792
Optimization_Done 1500
[2025-04-09 18:16:22] [command] train weight_iter_1500.pkl 1 4
[2025-04-09 18:16:45] nn step 1600, lr: 0.02.
	loss_policy: 1.31641
	accuracy_policy: 0.33825
	loss_value: 1.06779
[2025-04-09 18:17:09] nn step 1700, lr: 0.02.
	loss_policy: 1.30998
	accuracy_policy: 0.34498
	loss_value: 1.06397
[2025-04-09 18:17:32] nn step 1800, lr: 0.02.
	loss_policy: 1.3058
	accuracy_policy: 0.34254
	loss_value: 1.05928
[2025-04-09 18:17:56] nn step 1900, lr: 0.02.
	loss_policy: 1.30269
	accuracy_policy: 0.34088
	loss_value: 1.06103
[2025-04-09 18:18:20] nn step 2000, lr: 0.02.
	loss_policy: 1.296
	accuracy_policy: 0.3455
	loss_value: 1.05533
Optimization_Done 2000
[2025-04-09 18:25:06] [command] train weight_iter_2000.pkl 1 5
[2025-04-09 18:25:29] nn step 2100, lr: 0.02.
	loss_policy: 1.35826
	accuracy_policy: 0.32837
	loss_value: 1.0709
[2025-04-09 18:25:52] nn step 2200, lr: 0.02.
	loss_policy: 1.35221
	accuracy_policy: 0.33115
	loss_value: 1.06978
[2025-04-09 18:26:16] nn step 2300, lr: 0.02.
	loss_policy: 1.34935
	accuracy_policy: 0.33042
	loss_value: 1.06359
[2025-04-09 18:26:38] nn step 2400, lr: 0.02.
	loss_policy: 1.34884
	accuracy_policy: 0.32919
	loss_value: 1.06872
[2025-04-09 18:27:02] nn step 2500, lr: 0.02.
	loss_policy: 1.33926
	accuracy_policy: 0.33375
	loss_value: 1.05975
Optimization_Done 2500
[2025-04-09 18:32:18] [command] train weight_iter_2500.pkl 1 6
[2025-04-09 18:32:41] nn step 2600, lr: 0.02.
	loss_policy: 1.38458
	accuracy_policy: 0.32469
	loss_value: 1.07241
[2025-04-09 18:33:05] nn step 2700, lr: 0.02.
	loss_policy: 1.38221
	accuracy_policy: 0.32761
	loss_value: 1.07099
[2025-04-09 18:33:30] nn step 2800, lr: 0.02.
	loss_policy: 1.37624
	accuracy_policy: 0.32771
	loss_value: 1.06709
[2025-04-09 18:33:53] nn step 2900, lr: 0.02.
	loss_policy: 1.3818
	accuracy_policy: 0.3251
	loss_value: 1.06751
[2025-04-09 18:34:17] nn step 3000, lr: 0.02.
	loss_policy: 1.37298
	accuracy_policy: 0.32999
	loss_value: 1.06786
Optimization_Done 3000
[2025-04-09 18:43:17] [command] train weight_iter_3000.pkl 1 7
[2025-04-09 18:43:41] nn step 3100, lr: 0.02.
	loss_policy: 1.41467
	accuracy_policy: 0.31578
	loss_value: 1.06806
[2025-04-09 18:44:04] nn step 3200, lr: 0.02.
	loss_policy: 1.41672
	accuracy_policy: 0.31446
	loss_value: 1.06764
[2025-04-09 18:44:28] nn step 3300, lr: 0.02.
	loss_policy: 1.40883
	accuracy_policy: 0.3163
	loss_value: 1.06811
[2025-04-09 18:44:52] nn step 3400, lr: 0.02.
	loss_policy: 1.41352
	accuracy_policy: 0.3172
	loss_value: 1.06428
[2025-04-09 18:45:15] nn step 3500, lr: 0.02.
	loss_policy: 1.41005
	accuracy_policy: 0.31662
	loss_value: 1.06992
Optimization_Done 3500
[2025-04-09 18:49:45] [command] train weight_iter_3500.pkl 1 8
[2025-04-09 18:50:08] nn step 3600, lr: 0.02.
	loss_policy: 1.4255
	accuracy_policy: 0.31642
	loss_value: 1.0652
[2025-04-09 18:50:31] nn step 3700, lr: 0.02.
	loss_policy: 1.42171
	accuracy_policy: 0.3149
	loss_value: 1.06969
[2025-04-09 18:50:55] nn step 3800, lr: 0.02.
	loss_policy: 1.42473
	accuracy_policy: 0.31548
	loss_value: 1.07073
[2025-04-09 18:51:19] nn step 3900, lr: 0.02.
	loss_policy: 1.4155
	accuracy_policy: 0.31905
	loss_value: 1.06485
[2025-04-09 18:51:42] nn step 4000, lr: 0.02.
	loss_policy: 1.41333
	accuracy_policy: 0.31916
	loss_value: 1.06376
Optimization_Done 4000
[2025-04-09 18:56:09] [command] train weight_iter_4000.pkl 1 9
[2025-04-09 18:56:33] nn step 4100, lr: 0.02.
	loss_policy: 1.42964
	accuracy_policy: 0.31604
	loss_value: 1.06401
[2025-04-09 18:56:56] nn step 4200, lr: 0.02.
	loss_policy: 1.42466
	accuracy_policy: 0.31644
	loss_value: 1.0626
[2025-04-09 18:57:19] nn step 4300, lr: 0.02.
	loss_policy: 1.41913
	accuracy_policy: 0.32085
	loss_value: 1.06449
[2025-04-09 18:57:41] nn step 4400, lr: 0.02.
	loss_policy: 1.41868
	accuracy_policy: 0.32342
	loss_value: 1.05974
[2025-04-09 18:58:05] nn step 4500, lr: 0.02.
	loss_policy: 1.42155
	accuracy_policy: 0.31738
	loss_value: 1.06531
Optimization_Done 4500
[2025-04-09 19:02:47] [command] train weight_iter_4500.pkl 1 10
[2025-04-09 19:03:10] nn step 4600, lr: 0.02.
	loss_policy: 1.43502
	accuracy_policy: 0.32273
	loss_value: 1.0603
[2025-04-09 19:03:34] nn step 4700, lr: 0.02.
	loss_policy: 1.43364
	accuracy_policy: 0.32148
	loss_value: 1.06225
[2025-04-09 19:03:58] nn step 4800, lr: 0.02.
	loss_policy: 1.43647
	accuracy_policy: 0.31869
	loss_value: 1.06625
[2025-04-09 19:04:22] nn step 4900, lr: 0.02.
	loss_policy: 1.43847
	accuracy_policy: 0.32086
	loss_value: 1.06324
[2025-04-09 19:04:46] nn step 5000, lr: 0.02.
	loss_policy: 1.43498
	accuracy_policy: 0.32
	loss_value: 1.06088
Optimization_Done 5000
[2025-04-09 19:08:59] [command] train weight_iter_5000.pkl 1 11
[2025-04-09 19:09:22] nn step 5100, lr: 0.02.
	loss_policy: 1.44299
	accuracy_policy: 0.32374
	loss_value: 1.05885
[2025-04-09 19:09:46] nn step 5200, lr: 0.02.
	loss_policy: 1.44072
	accuracy_policy: 0.32063
	loss_value: 1.05293
[2025-04-09 19:10:10] nn step 5300, lr: 0.02.
	loss_policy: 1.44665
	accuracy_policy: 0.32178
	loss_value: 1.06026
[2025-04-09 19:10:34] nn step 5400, lr: 0.02.
	loss_policy: 1.44538
	accuracy_policy: 0.32057
	loss_value: 1.05675
[2025-04-09 19:10:58] nn step 5500, lr: 0.02.
	loss_policy: 1.44281
	accuracy_policy: 0.32061
	loss_value: 1.05216
Optimization_Done 5500
[2025-04-09 19:15:22] [command] train weight_iter_5500.pkl 1 12
[2025-04-09 19:15:46] nn step 5600, lr: 0.02.
	loss_policy: 1.44976
	accuracy_policy: 0.32172
	loss_value: 1.05255
[2025-04-09 19:16:10] nn step 5700, lr: 0.02.
	loss_policy: 1.44806
	accuracy_policy: 0.32314
	loss_value: 1.05426
[2025-04-09 19:16:34] nn step 5800, lr: 0.02.
	loss_policy: 1.44798
	accuracy_policy: 0.3221
	loss_value: 1.0574
[2025-04-09 19:16:57] nn step 5900, lr: 0.02.
	loss_policy: 1.44883
	accuracy_policy: 0.32452
	loss_value: 1.05088
[2025-04-09 19:17:21] nn step 6000, lr: 0.02.
	loss_policy: 1.45155
	accuracy_policy: 0.32009
	loss_value: 1.05197
Optimization_Done 6000
[2025-04-09 19:25:06] [command] train weight_iter_6000.pkl 1 13
[2025-04-09 19:25:29] nn step 6100, lr: 0.02.
	loss_policy: 1.46324
	accuracy_policy: 0.31937
	loss_value: 1.05145
[2025-04-09 19:25:53] nn step 6200, lr: 0.02.
	loss_policy: 1.46706
	accuracy_policy: 0.31839
	loss_value: 1.05236
[2025-04-09 19:26:17] nn step 6300, lr: 0.02.
	loss_policy: 1.46015
	accuracy_policy: 0.32051
	loss_value: 1.05039
[2025-04-09 19:26:42] nn step 6400, lr: 0.02.
	loss_policy: 1.46465
	accuracy_policy: 0.31886
	loss_value: 1.04995
[2025-04-09 19:27:06] nn step 6500, lr: 0.02.
	loss_policy: 1.46034
	accuracy_policy: 0.32263
	loss_value: 1.05111
Optimization_Done 6500
[2025-04-09 19:30:59] [command] train weight_iter_6500.pkl 1 14
[2025-04-09 19:31:22] nn step 6600, lr: 0.02.
	loss_policy: 1.46767
	accuracy_policy: 0.32142
	loss_value: 1.04519
[2025-04-09 19:31:46] nn step 6700, lr: 0.02.
	loss_policy: 1.47205
	accuracy_policy: 0.31917
	loss_value: 1.05097
[2025-04-09 19:32:09] nn step 6800, lr: 0.02.
	loss_policy: 1.47231
	accuracy_policy: 0.32083
	loss_value: 1.0494
[2025-04-09 19:32:33] nn step 6900, lr: 0.02.
	loss_policy: 1.46859
	accuracy_policy: 0.32107
	loss_value: 1.04809
[2025-04-09 19:32:56] nn step 7000, lr: 0.02.
	loss_policy: 1.46945
	accuracy_policy: 0.31956
	loss_value: 1.0482
Optimization_Done 7000
[2025-04-09 19:37:25] [command] train weight_iter_7000.pkl 1 15
[2025-04-09 19:37:47] nn step 7100, lr: 0.02.
	loss_policy: 1.47686
	accuracy_policy: 0.3173
	loss_value: 1.0463
[2025-04-09 19:38:09] nn step 7200, lr: 0.02.
	loss_policy: 1.48613
	accuracy_policy: 0.31825
	loss_value: 1.0481
[2025-04-09 19:38:32] nn step 7300, lr: 0.02.
	loss_policy: 1.47975
	accuracy_policy: 0.31867
	loss_value: 1.04865
[2025-04-09 19:38:55] nn step 7400, lr: 0.02.
	loss_policy: 1.47792
	accuracy_policy: 0.32077
	loss_value: 1.0502
[2025-04-09 19:39:19] nn step 7500, lr: 0.02.
	loss_policy: 1.47642
	accuracy_policy: 0.32224
	loss_value: 1.04605
Optimization_Done 7500
[2025-04-09 19:43:51] [command] train weight_iter_7500.pkl 1 16
[2025-04-09 19:44:14] nn step 7600, lr: 0.02.
	loss_policy: 1.49166
	accuracy_policy: 0.31827
	loss_value: 1.04535
[2025-04-09 19:44:38] nn step 7700, lr: 0.02.
	loss_policy: 1.48401
	accuracy_policy: 0.31895
	loss_value: 1.04327
[2025-04-09 19:45:03] nn step 7800, lr: 0.02.
	loss_policy: 1.49113
	accuracy_policy: 0.31719
	loss_value: 1.04737
[2025-04-09 19:45:27] nn step 7900, lr: 0.02.
	loss_policy: 1.48031
	accuracy_policy: 0.31824
	loss_value: 1.04137
[2025-04-09 19:45:51] nn step 8000, lr: 0.02.
	loss_policy: 1.48751
	accuracy_policy: 0.32036
	loss_value: 1.04576
Optimization_Done 8000
[2025-04-09 19:50:16] [command] train weight_iter_8000.pkl 1 17
[2025-04-09 19:50:39] nn step 8100, lr: 0.02.
	loss_policy: 1.49322
	accuracy_policy: 0.31803
	loss_value: 1.044
[2025-04-09 19:51:03] nn step 8200, lr: 0.02.
	loss_policy: 1.48932
	accuracy_policy: 0.32166
	loss_value: 1.04467
[2025-04-09 19:51:27] nn step 8300, lr: 0.02.
	loss_policy: 1.49213
	accuracy_policy: 0.32145
	loss_value: 1.03833
[2025-04-09 19:51:52] nn step 8400, lr: 0.02.
	loss_policy: 1.49122
	accuracy_policy: 0.32263
	loss_value: 1.03949
[2025-04-09 19:52:15] nn step 8500, lr: 0.02.
	loss_policy: 1.4896
	accuracy_policy: 0.31994
	loss_value: 1.04018
Optimization_Done 8500
[2025-04-09 19:56:28] [command] train weight_iter_8500.pkl 1 18
[2025-04-09 19:56:51] nn step 8600, lr: 0.02.
	loss_policy: 1.49533
	accuracy_policy: 0.3203
	loss_value: 1.03831
[2025-04-09 19:57:16] nn step 8700, lr: 0.02.
	loss_policy: 1.49704
	accuracy_policy: 0.32086
	loss_value: 1.03688
[2025-04-09 19:57:40] nn step 8800, lr: 0.02.
	loss_policy: 1.49336
	accuracy_policy: 0.32076
	loss_value: 1.03822
[2025-04-09 19:58:04] nn step 8900, lr: 0.02.
	loss_policy: 1.49441
	accuracy_policy: 0.32294
	loss_value: 1.03395
[2025-04-09 19:58:28] nn step 9000, lr: 0.02.
	loss_policy: 1.48766
	accuracy_policy: 0.32314
	loss_value: 1.03692
Optimization_Done 9000
[2025-04-09 20:03:12] [command] train weight_iter_9000.pkl 1 19
[2025-04-09 20:03:36] nn step 9100, lr: 0.02.
	loss_policy: 1.50671
	accuracy_policy: 0.3217
	loss_value: 1.03743
[2025-04-09 20:04:00] nn step 9200, lr: 0.02.
	loss_policy: 1.51074
	accuracy_policy: 0.31644
	loss_value: 1.03725
[2025-04-09 20:04:24] nn step 9300, lr: 0.02.
	loss_policy: 1.50201
	accuracy_policy: 0.3259
	loss_value: 1.03262
[2025-04-09 20:04:47] nn step 9400, lr: 0.02.
	loss_policy: 1.50831
	accuracy_policy: 0.31976
	loss_value: 1.03566
[2025-04-09 20:05:11] nn step 9500, lr: 0.02.
	loss_policy: 1.50751
	accuracy_policy: 0.32125
	loss_value: 1.03772
Optimization_Done 9500
[2025-04-09 20:09:03] [command] train weight_iter_9500.pkl 1 20
[2025-04-09 20:09:25] nn step 9600, lr: 0.02.
	loss_policy: 1.5157
	accuracy_policy: 0.321
	loss_value: 1.03595
[2025-04-09 20:09:48] nn step 9700, lr: 0.02.
	loss_policy: 1.50963
	accuracy_policy: 0.32205
	loss_value: 1.03199
[2025-04-09 20:10:12] nn step 9800, lr: 0.02.
	loss_policy: 1.50921
	accuracy_policy: 0.32457
	loss_value: 1.02755
[2025-04-09 20:10:35] nn step 9900, lr: 0.02.
	loss_policy: 1.50977
	accuracy_policy: 0.32301
	loss_value: 1.0278
[2025-04-09 20:10:58] nn step 10000, lr: 0.02.
	loss_policy: 1.50131
	accuracy_policy: 0.32504
	loss_value: 1.02876
Optimization_Done 10000
[2025-04-09 20:19:42] [command] train weight_iter_10000.pkl 2 21
[2025-04-09 20:20:05] nn step 10100, lr: 0.02.
	loss_policy: 1.58343
	accuracy_policy: 0.32384
	loss_value: 1.02583
[2025-04-09 20:20:29] nn step 10200, lr: 0.02.
	loss_policy: 1.57421
	accuracy_policy: 0.32685
	loss_value: 1.02444
[2025-04-09 20:20:52] nn step 10300, lr: 0.02.
	loss_policy: 1.57065
	accuracy_policy: 0.32775
	loss_value: 1.02854
[2025-04-09 20:21:16] nn step 10400, lr: 0.02.
	loss_policy: 1.57248
	accuracy_policy: 0.32751
	loss_value: 1.02915
[2025-04-09 20:21:40] nn step 10500, lr: 0.02.
	loss_policy: 1.56907
	accuracy_policy: 0.32655
	loss_value: 1.02044
Optimization_Done 10500
[2025-04-09 20:32:25] [command] train weight_iter_10500.pkl 3 22
[2025-04-09 20:32:48] nn step 10600, lr: 0.02.
	loss_policy: 1.61273
	accuracy_policy: 0.32834
	loss_value: 1.02651
[2025-04-09 20:33:12] nn step 10700, lr: 0.02.
	loss_policy: 1.61757
	accuracy_policy: 0.32703
	loss_value: 1.0242
[2025-04-09 20:33:35] nn step 10800, lr: 0.02.
	loss_policy: 1.60862
	accuracy_policy: 0.32663
	loss_value: 1.02318
[2025-04-09 20:33:59] nn step 10900, lr: 0.02.
	loss_policy: 1.61018
	accuracy_policy: 0.32732
	loss_value: 1.02021
[2025-04-09 20:34:23] nn step 11000, lr: 0.02.
	loss_policy: 1.60691
	accuracy_policy: 0.32929
	loss_value: 1.02081
Optimization_Done 11000
[2025-04-09 20:40:05] [command] train weight_iter_11000.pkl 4 23
[2025-04-09 20:40:28] nn step 11100, lr: 0.02.
	loss_policy: 1.63029
	accuracy_policy: 0.32388
	loss_value: 1.01962
[2025-04-09 20:40:52] nn step 11200, lr: 0.02.
	loss_policy: 1.62409
	accuracy_policy: 0.32603
	loss_value: 1.01651
[2025-04-09 20:41:15] nn step 11300, lr: 0.02.
	loss_policy: 1.62426
	accuracy_policy: 0.32676
	loss_value: 1.0144
[2025-04-09 20:41:39] nn step 11400, lr: 0.02.
	loss_policy: 1.62073
	accuracy_policy: 0.327
	loss_value: 1.01932
[2025-04-09 20:42:02] nn step 11500, lr: 0.02.
	loss_policy: 1.61664
	accuracy_policy: 0.32784
	loss_value: 1.01748
Optimization_Done 11500
[2025-04-09 20:47:06] [command] train weight_iter_11500.pkl 5 24
[2025-04-09 20:47:29] nn step 11600, lr: 0.02.
	loss_policy: 1.62333
	accuracy_policy: 0.33247
	loss_value: 1.00571
[2025-04-09 20:47:50] nn step 11700, lr: 0.02.
	loss_policy: 1.61677
	accuracy_policy: 0.33356
	loss_value: 1.00314
[2025-04-09 20:48:12] nn step 11800, lr: 0.02.
	loss_policy: 1.61549
	accuracy_policy: 0.33364
	loss_value: 1.00513
[2025-04-09 20:48:35] nn step 11900, lr: 0.02.
	loss_policy: 1.62043
	accuracy_policy: 0.33079
	loss_value: 1.0021
[2025-04-09 20:48:59] nn step 12000, lr: 0.02.
	loss_policy: 1.61233
	accuracy_policy: 0.33256
	loss_value: 1.00426
Optimization_Done 12000
[2025-04-09 20:53:30] [command] train weight_iter_12000.pkl 6 25
[2025-04-09 20:53:54] nn step 12100, lr: 0.02.
	loss_policy: 1.6152
	accuracy_policy: 0.33633
	loss_value: 0.99415
[2025-04-09 20:54:18] nn step 12200, lr: 0.02.
	loss_policy: 1.61511
	accuracy_policy: 0.33646
	loss_value: 0.99953
[2025-04-09 20:54:42] nn step 12300, lr: 0.02.
	loss_policy: 1.61248
	accuracy_policy: 0.33698
	loss_value: 0.9952
[2025-04-09 20:55:06] nn step 12400, lr: 0.02.
	loss_policy: 1.6108
	accuracy_policy: 0.3385
	loss_value: 0.99687
[2025-04-09 20:55:31] nn step 12500, lr: 0.02.
	loss_policy: 1.61028
	accuracy_policy: 0.33793
	loss_value: 0.99097
Optimization_Done 12500
[2025-04-09 21:00:47] [command] train weight_iter_12500.pkl 7 26
[2025-04-09 21:01:11] nn step 12600, lr: 0.02.
	loss_policy: 1.61784
	accuracy_policy: 0.34035
	loss_value: 0.98958
[2025-04-09 21:01:35] nn step 12700, lr: 0.02.
	loss_policy: 1.61485
	accuracy_policy: 0.33877
	loss_value: 0.99174
[2025-04-09 21:01:59] nn step 12800, lr: 0.02.
	loss_policy: 1.61258
	accuracy_policy: 0.33931
	loss_value: 0.99363
[2025-04-09 21:02:24] nn step 12900, lr: 0.02.
	loss_policy: 1.61591
	accuracy_policy: 0.34026
	loss_value: 0.98951
[2025-04-09 21:02:48] nn step 13000, lr: 0.02.
	loss_policy: 1.61519
	accuracy_policy: 0.34015
	loss_value: 0.99186
Optimization_Done 13000
[2025-04-09 21:07:11] [command] train weight_iter_13000.pkl 8 27
[2025-04-09 21:07:35] nn step 13100, lr: 0.02.
	loss_policy: 1.63121
	accuracy_policy: 0.34118
	loss_value: 0.98097
[2025-04-09 21:07:59] nn step 13200, lr: 0.02.
	loss_policy: 1.61742
	accuracy_policy: 0.34681
	loss_value: 0.9806
[2025-04-09 21:08:23] nn step 13300, lr: 0.02.
	loss_policy: 1.61672
	accuracy_policy: 0.34347
	loss_value: 0.97868
[2025-04-09 21:08:47] nn step 13400, lr: 0.02.
	loss_policy: 1.61642
	accuracy_policy: 0.34472
	loss_value: 0.97958
[2025-04-09 21:09:11] nn step 13500, lr: 0.02.
	loss_policy: 1.61948
	accuracy_policy: 0.34753
	loss_value: 0.98086
Optimization_Done 13500
[2025-04-09 21:20:25] [command] train weight_iter_13500.pkl 9 28
[2025-04-09 21:20:49] nn step 13600, lr: 0.02.
	loss_policy: 1.6622
	accuracy_policy: 0.33457
	loss_value: 0.98257
[2025-04-09 21:21:12] nn step 13700, lr: 0.02.
	loss_policy: 1.65333
	accuracy_policy: 0.33632
	loss_value: 0.98054
[2025-04-09 21:21:37] nn step 13800, lr: 0.02.
	loss_policy: 1.65726
	accuracy_policy: 0.3381
	loss_value: 0.98191
[2025-04-09 21:22:00] nn step 13900, lr: 0.02.
	loss_policy: 1.6473
	accuracy_policy: 0.33781
	loss_value: 0.97834
[2025-04-09 21:22:24] nn step 14000, lr: 0.02.
	loss_policy: 1.64908
	accuracy_policy: 0.33872
	loss_value: 0.9797
Optimization_Done 14000
[2025-04-09 21:31:09] [command] train weight_iter_14000.pkl 10 29
[2025-04-09 21:31:31] nn step 14100, lr: 0.02.
	loss_policy: 1.67427
	accuracy_policy: 0.33204
	loss_value: 0.97925
[2025-04-09 21:31:55] nn step 14200, lr: 0.02.
	loss_policy: 1.67311
	accuracy_policy: 0.33248
	loss_value: 0.97899
[2025-04-09 21:32:18] nn step 14300, lr: 0.02.
	loss_policy: 1.67027
	accuracy_policy: 0.33545
	loss_value: 0.97698
[2025-04-09 21:32:41] nn step 14400, lr: 0.02.
	loss_policy: 1.66361
	accuracy_policy: 0.33724
	loss_value: 0.97587
[2025-04-09 21:33:05] nn step 14500, lr: 0.02.
	loss_policy: 1.66594
	accuracy_policy: 0.33359
	loss_value: 0.9767
Optimization_Done 14500
[2025-04-09 21:40:08] [command] train weight_iter_14500.pkl 11 30
[2025-04-09 21:40:32] nn step 14600, lr: 0.02.
	loss_policy: 1.68497
	accuracy_policy: 0.33095
	loss_value: 0.97431
[2025-04-09 21:40:56] nn step 14700, lr: 0.02.
	loss_policy: 1.67557
	accuracy_policy: 0.33304
	loss_value: 0.9749
[2025-04-09 21:41:20] nn step 14800, lr: 0.02.
	loss_policy: 1.68288
	accuracy_policy: 0.32824
	loss_value: 0.97595
[2025-04-09 21:41:44] nn step 14900, lr: 0.02.
	loss_policy: 1.67901
	accuracy_policy: 0.33215
	loss_value: 0.97652
[2025-04-09 21:42:08] nn step 15000, lr: 0.02.
	loss_policy: 1.67953
	accuracy_policy: 0.33247
	loss_value: 0.97132
Optimization_Done 15000
[2025-04-09 21:47:50] [command] train weight_iter_15000.pkl 12 31
[2025-04-09 21:48:14] nn step 15100, lr: 0.02.
	loss_policy: 1.6995
	accuracy_policy: 0.3276
	loss_value: 0.97156
[2025-04-09 21:48:37] nn step 15200, lr: 0.02.
	loss_policy: 1.7023
	accuracy_policy: 0.32563
	loss_value: 0.97474
[2025-04-09 21:49:02] nn step 15300, lr: 0.02.
	loss_policy: 1.69639
	accuracy_policy: 0.32737
	loss_value: 0.97243
[2025-04-09 21:49:26] nn step 15400, lr: 0.02.
	loss_policy: 1.69009
	accuracy_policy: 0.3289
	loss_value: 0.96845
[2025-04-09 21:49:50] nn step 15500, lr: 0.02.
	loss_policy: 1.68754
	accuracy_policy: 0.32971
	loss_value: 0.97433
Optimization_Done 15500
[2025-04-09 21:59:20] [command] train weight_iter_15500.pkl 13 32
[2025-04-09 21:59:42] nn step 15600, lr: 0.02.
	loss_policy: 1.71702
	accuracy_policy: 0.32104
	loss_value: 0.97827
[2025-04-09 22:00:06] nn step 15700, lr: 0.02.
	loss_policy: 1.71461
	accuracy_policy: 0.32356
	loss_value: 0.9763
[2025-04-09 22:00:29] nn step 15800, lr: 0.02.
	loss_policy: 1.71197
	accuracy_policy: 0.32405
	loss_value: 0.97448
[2025-04-09 22:00:52] nn step 15900, lr: 0.02.
	loss_policy: 1.71363
	accuracy_policy: 0.3212
	loss_value: 0.9721
[2025-04-09 22:01:16] nn step 16000, lr: 0.02.
	loss_policy: 1.70986
	accuracy_policy: 0.32595
	loss_value: 0.97273
Optimization_Done 16000
[2025-04-09 22:07:27] [command] train weight_iter_16000.pkl 14 33
[2025-04-09 22:07:50] nn step 16100, lr: 0.02.
	loss_policy: 1.72469
	accuracy_policy: 0.32512
	loss_value: 0.96893
[2025-04-09 22:08:14] nn step 16200, lr: 0.02.
	loss_policy: 1.72479
	accuracy_policy: 0.32518
	loss_value: 0.9657
[2025-04-09 22:08:39] nn step 16300, lr: 0.02.
	loss_policy: 1.71932
	accuracy_policy: 0.32583
	loss_value: 0.96563
[2025-04-09 22:09:02] nn step 16400, lr: 0.02.
	loss_policy: 1.72301
	accuracy_policy: 0.32464
	loss_value: 0.96213
[2025-04-09 22:09:26] nn step 16500, lr: 0.02.
	loss_policy: 1.72109
	accuracy_policy: 0.32706
	loss_value: 0.9614
Optimization_Done 16500
[2025-04-09 22:15:56] [command] train weight_iter_16500.pkl 15 34
[2025-04-09 22:16:19] nn step 16600, lr: 0.02.
	loss_policy: 1.73032
	accuracy_policy: 0.32486
	loss_value: 0.9638
[2025-04-09 22:16:43] nn step 16700, lr: 0.02.
	loss_policy: 1.72832
	accuracy_policy: 0.32357
	loss_value: 0.96272
[2025-04-09 22:17:07] nn step 16800, lr: 0.02.
	loss_policy: 1.73228
	accuracy_policy: 0.32204
	loss_value: 0.96351
[2025-04-09 22:17:31] nn step 16900, lr: 0.02.
	loss_policy: 1.72349
	accuracy_policy: 0.32303
	loss_value: 0.96213
[2025-04-09 22:17:54] nn step 17000, lr: 0.02.
	loss_policy: 1.72125
	accuracy_policy: 0.32728
	loss_value: 0.95802
Optimization_Done 17000
[2025-04-09 22:29:08] [command] train weight_iter_17000.pkl 16 35
[2025-04-09 22:29:31] nn step 17100, lr: 0.02.
	loss_policy: 1.74576
	accuracy_policy: 0.31861
	loss_value: 0.96469
[2025-04-09 22:29:55] nn step 17200, lr: 0.02.
	loss_policy: 1.74097
	accuracy_policy: 0.3202
	loss_value: 0.96652
[2025-04-09 22:30:19] nn step 17300, lr: 0.02.
	loss_policy: 1.74345
	accuracy_policy: 0.32061
	loss_value: 0.96806
[2025-04-09 22:30:44] nn step 17400, lr: 0.02.
	loss_policy: 1.74363
	accuracy_policy: 0.31855
	loss_value: 0.96605
[2025-04-09 22:31:07] nn step 17500, lr: 0.02.
	loss_policy: 1.74201
	accuracy_policy: 0.32119
	loss_value: 0.96415
Optimization_Done 17500
[2025-04-09 22:37:43] [command] train weight_iter_17500.pkl 17 36
[2025-04-09 22:38:07] nn step 17600, lr: 0.02.
	loss_policy: 1.75708
	accuracy_policy: 0.31632
	loss_value: 0.96936
[2025-04-09 22:38:30] nn step 17700, lr: 0.02.
	loss_policy: 1.75038
	accuracy_policy: 0.31952
	loss_value: 0.9651
[2025-04-09 22:38:54] nn step 17800, lr: 0.02.
	loss_policy: 1.74725
	accuracy_policy: 0.31973
	loss_value: 0.96514
[2025-04-09 22:39:19] nn step 17900, lr: 0.02.
	loss_policy: 1.74225
	accuracy_policy: 0.32047
	loss_value: 0.96419
[2025-04-09 22:39:43] nn step 18000, lr: 0.02.
	loss_policy: 1.74598
	accuracy_policy: 0.31881
	loss_value: 0.96399
Optimization_Done 18000
[2025-04-09 22:55:58] [command] train weight_iter_18000.pkl 18 37
[2025-04-09 22:56:20] nn step 18100, lr: 0.02.
	loss_policy: 1.77418
	accuracy_policy: 0.31062
	loss_value: 0.97844
[2025-04-09 22:56:42] nn step 18200, lr: 0.02.
	loss_policy: 1.77661
	accuracy_policy: 0.31176
	loss_value: 0.98341
[2025-04-09 22:57:05] nn step 18300, lr: 0.02.
	loss_policy: 1.77132
	accuracy_policy: 0.31137
	loss_value: 0.97947
[2025-04-09 22:57:27] nn step 18400, lr: 0.02.
	loss_policy: 1.77684
	accuracy_policy: 0.30963
	loss_value: 0.97981
[2025-04-09 22:57:50] nn step 18500, lr: 0.02.
	loss_policy: 1.77111
	accuracy_policy: 0.31342
	loss_value: 0.97873
Optimization_Done 18500
[2025-04-09 23:06:08] [command] train weight_iter_18500.pkl 19 38
[2025-04-09 23:06:30] nn step 18600, lr: 0.02.
	loss_policy: 1.78355
	accuracy_policy: 0.3108
	loss_value: 0.98523
[2025-04-09 23:06:52] nn step 18700, lr: 0.02.
	loss_policy: 1.78217
	accuracy_policy: 0.30774
	loss_value: 0.97697
[2025-04-09 23:07:15] nn step 18800, lr: 0.02.
	loss_policy: 1.77416
	accuracy_policy: 0.31177
	loss_value: 0.98219
[2025-04-09 23:07:36] nn step 18900, lr: 0.02.
	loss_policy: 1.77839
	accuracy_policy: 0.31092
	loss_value: 0.97699
[2025-04-09 23:07:59] nn step 19000, lr: 0.02.
	loss_policy: 1.7796
	accuracy_policy: 0.30896
	loss_value: 0.97801
Optimization_Done 19000
[2025-04-09 23:17:42] [command] train weight_iter_19000.pkl 20 39
[2025-04-09 23:18:04] nn step 19100, lr: 0.02.
	loss_policy: 1.78722
	accuracy_policy: 0.3065
	loss_value: 0.98448
[2025-04-09 23:18:25] nn step 19200, lr: 0.02.
	loss_policy: 1.79018
	accuracy_policy: 0.30609
	loss_value: 0.98575
[2025-04-09 23:18:47] nn step 19300, lr: 0.02.
	loss_policy: 1.77961
	accuracy_policy: 0.31066
	loss_value: 0.98446
[2025-04-09 23:19:09] nn step 19400, lr: 0.02.
	loss_policy: 1.78047
	accuracy_policy: 0.30979
	loss_value: 0.98298
[2025-04-09 23:19:31] nn step 19500, lr: 0.02.
	loss_policy: 1.78232
	accuracy_policy: 0.30596
	loss_value: 0.9772
Optimization_Done 19500
[2025-04-09 23:28:45] [command] train weight_iter_19500.pkl 21 40
[2025-04-09 23:29:08] nn step 19600, lr: 0.02.
	loss_policy: 1.79252
	accuracy_policy: 0.30776
	loss_value: 0.98814
[2025-04-09 23:29:33] nn step 19700, lr: 0.02.
	loss_policy: 1.79081
	accuracy_policy: 0.306
	loss_value: 0.98414
[2025-04-09 23:29:57] nn step 19800, lr: 0.02.
	loss_policy: 1.78719
	accuracy_policy: 0.30689
	loss_value: 0.98749
[2025-04-09 23:30:20] nn step 19900, lr: 0.02.
	loss_policy: 1.78831
	accuracy_policy: 0.3061
	loss_value: 0.98618
[2025-04-09 23:30:44] nn step 20000, lr: 0.02.
	loss_policy: 1.78247
	accuracy_policy: 0.30884
	loss_value: 0.98338
Optimization_Done 20000
[2025-04-09 23:47:44] [command] train weight_iter_20000.pkl 22 41
[2025-04-09 23:48:08] nn step 20100, lr: 0.02.
	loss_policy: 1.81251
	accuracy_policy: 0.30371
	loss_value: 1.00138
[2025-04-09 23:48:31] nn step 20200, lr: 0.02.
	loss_policy: 1.81195
	accuracy_policy: 0.30155
	loss_value: 0.99976
[2025-04-09 23:48:55] nn step 20300, lr: 0.02.
	loss_policy: 1.80305
	accuracy_policy: 0.30319
	loss_value: 0.99781
[2025-04-09 23:49:19] nn step 20400, lr: 0.02.
	loss_policy: 1.80035
	accuracy_policy: 0.30304
	loss_value: 0.99354
[2025-04-09 23:49:43] nn step 20500, lr: 0.02.
	loss_policy: 1.80078
	accuracy_policy: 0.30345
	loss_value: 0.99497
Optimization_Done 20500
[2025-04-10 00:04:03] [command] train weight_iter_20500.pkl 23 42
[2025-04-10 00:04:26] nn step 20600, lr: 0.02.
	loss_policy: 1.82004
	accuracy_policy: 0.29985
	loss_value: 0.99675
[2025-04-10 00:04:49] nn step 20700, lr: 0.02.
	loss_policy: 1.82042
	accuracy_policy: 0.29738
	loss_value: 0.99886
[2025-04-10 00:05:13] nn step 20800, lr: 0.02.
	loss_policy: 1.8111
	accuracy_policy: 0.30254
	loss_value: 0.99799
[2025-04-10 00:05:37] nn step 20900, lr: 0.02.
	loss_policy: 1.81686
	accuracy_policy: 0.29847
	loss_value: 0.99962
[2025-04-10 00:06:01] nn step 21000, lr: 0.02.
	loss_policy: 1.80281
	accuracy_policy: 0.30217
	loss_value: 1.00278
Optimization_Done 21000
[2025-04-10 00:18:52] [command] train weight_iter_21000.pkl 24 43
[2025-04-10 00:19:15] nn step 21100, lr: 0.02.
	loss_policy: 1.82488
	accuracy_policy: 0.2974
	loss_value: 1.00551
[2025-04-10 00:19:39] nn step 21200, lr: 0.02.
	loss_policy: 1.82323
	accuracy_policy: 0.2983
	loss_value: 1.00529
[2025-04-10 00:20:04] nn step 21300, lr: 0.02.
	loss_policy: 1.81705
	accuracy_policy: 0.30036
	loss_value: 1.01286
[2025-04-10 00:20:28] nn step 21400, lr: 0.02.
	loss_policy: 1.82213
	accuracy_policy: 0.29761
	loss_value: 1.00306
[2025-04-10 00:20:52] nn step 21500, lr: 0.02.
	loss_policy: 1.81983
	accuracy_policy: 0.29917
	loss_value: 1.00633
Optimization_Done 21500
[2025-04-10 00:33:50] [command] train weight_iter_21500.pkl 25 44
[2025-04-10 00:34:14] nn step 21600, lr: 0.02.
	loss_policy: 1.83723
	accuracy_policy: 0.29419
	loss_value: 1.00958
[2025-04-10 00:34:38] nn step 21700, lr: 0.02.
	loss_policy: 1.83443
	accuracy_policy: 0.29294
	loss_value: 1.00747
[2025-04-10 00:35:02] nn step 21800, lr: 0.02.
	loss_policy: 1.83195
	accuracy_policy: 0.29603
	loss_value: 1.00738
[2025-04-10 00:35:26] nn step 21900, lr: 0.02.
	loss_policy: 1.82741
	accuracy_policy: 0.29592
	loss_value: 1.00262
[2025-04-10 00:35:50] nn step 22000, lr: 0.02.
	loss_policy: 1.83245
	accuracy_policy: 0.29439
	loss_value: 1.00387
Optimization_Done 22000
[2025-04-10 00:52:39] [command] train weight_iter_22000.pkl 26 45
[2025-04-10 00:53:03] nn step 22100, lr: 0.02.
	loss_policy: 1.85408
	accuracy_policy: 0.2874
	loss_value: 1.01931
[2025-04-10 00:53:26] nn step 22200, lr: 0.02.
	loss_policy: 1.84586
	accuracy_policy: 0.29202
	loss_value: 1.01532
[2025-04-10 00:53:50] nn step 22300, lr: 0.02.
	loss_policy: 1.84866
	accuracy_policy: 0.28841
	loss_value: 1.01187
[2025-04-10 00:54:13] nn step 22400, lr: 0.02.
	loss_policy: 1.84287
	accuracy_policy: 0.2914
	loss_value: 1.01545
[2025-04-10 00:54:36] nn step 22500, lr: 0.02.
	loss_policy: 1.8443
	accuracy_policy: 0.29096
	loss_value: 1.01749
Optimization_Done 22500
[2025-04-10 01:09:51] [command] train weight_iter_22500.pkl 27 46
[2025-04-10 01:10:14] nn step 22600, lr: 0.02.
	loss_policy: 1.86357
	accuracy_policy: 0.28368
	loss_value: 1.02531
[2025-04-10 01:10:37] nn step 22700, lr: 0.02.
	loss_policy: 1.86131
	accuracy_policy: 0.28454
	loss_value: 1.02527
[2025-04-10 01:11:01] nn step 22800, lr: 0.02.
	loss_policy: 1.86116
	accuracy_policy: 0.28446
	loss_value: 1.02232
[2025-04-10 01:11:26] nn step 22900, lr: 0.02.
	loss_policy: 1.8539
	accuracy_policy: 0.28485
	loss_value: 1.02083
[2025-04-10 01:11:50] nn step 23000, lr: 0.02.
	loss_policy: 1.85737
	accuracy_policy: 0.28352
	loss_value: 1.02342
Optimization_Done 23000
[2025-04-10 01:28:12] [command] train weight_iter_23000.pkl 28 47
[2025-04-10 01:28:35] nn step 23100, lr: 0.02.
	loss_policy: 1.87607
	accuracy_policy: 0.2814
	loss_value: 1.02742
[2025-04-10 01:28:58] nn step 23200, lr: 0.02.
	loss_policy: 1.87081
	accuracy_policy: 0.2796
	loss_value: 1.02881
[2025-04-10 01:29:23] nn step 23300, lr: 0.02.
	loss_policy: 1.87428
	accuracy_policy: 0.2784
	loss_value: 1.02575
[2025-04-10 01:29:47] nn step 23400, lr: 0.02.
	loss_policy: 1.87039
	accuracy_policy: 0.2808
	loss_value: 1.02498
[2025-04-10 01:30:10] nn step 23500, lr: 0.02.
	loss_policy: 1.86002
	accuracy_policy: 0.28469
	loss_value: 1.02717
Optimization_Done 23500
[2025-04-10 01:47:19] [command] train weight_iter_23500.pkl 29 48
[2025-04-10 01:47:42] nn step 23600, lr: 0.02.
	loss_policy: 1.88135
	accuracy_policy: 0.27841
	loss_value: 1.03381
[2025-04-10 01:48:06] nn step 23700, lr: 0.02.
	loss_policy: 1.87342
	accuracy_policy: 0.27705
	loss_value: 1.03286
[2025-04-10 01:48:29] nn step 23800, lr: 0.02.
	loss_policy: 1.87359
	accuracy_policy: 0.27767
	loss_value: 1.03507
[2025-04-10 01:48:54] nn step 23900, lr: 0.02.
	loss_policy: 1.87449
	accuracy_policy: 0.27848
	loss_value: 1.03057
[2025-04-10 01:49:17] nn step 24000, lr: 0.02.
	loss_policy: 1.86943
	accuracy_policy: 0.28141
	loss_value: 1.02875
Optimization_Done 24000
[2025-04-10 02:05:17] [command] train weight_iter_24000.pkl 30 49
[2025-04-10 02:05:41] nn step 24100, lr: 0.02.
	loss_policy: 1.88271
	accuracy_policy: 0.27534
	loss_value: 1.03839
[2025-04-10 02:06:04] nn step 24200, lr: 0.02.
	loss_policy: 1.88293
	accuracy_policy: 0.27706
	loss_value: 1.0362
[2025-04-10 02:06:28] nn step 24300, lr: 0.02.
	loss_policy: 1.87776
	accuracy_policy: 0.27791
	loss_value: 1.03483
[2025-04-10 02:06:52] nn step 24400, lr: 0.02.
	loss_policy: 1.87553
	accuracy_policy: 0.2786
	loss_value: 1.03154
[2025-04-10 02:07:16] nn step 24500, lr: 0.02.
	loss_policy: 1.87613
	accuracy_policy: 0.27858
	loss_value: 1.03562
Optimization_Done 24500
[2025-04-10 02:23:17] [command] train weight_iter_24500.pkl 31 50
[2025-04-10 02:23:40] nn step 24600, lr: 0.02.
	loss_policy: 1.8888
	accuracy_policy: 0.27341
	loss_value: 1.03826
[2025-04-10 02:24:04] nn step 24700, lr: 0.02.
	loss_policy: 1.88422
	accuracy_policy: 0.27481
	loss_value: 1.03668
[2025-04-10 02:24:27] nn step 24800, lr: 0.02.
	loss_policy: 1.88737
	accuracy_policy: 0.2731
	loss_value: 1.03803
[2025-04-10 02:24:51] nn step 24900, lr: 0.02.
	loss_policy: 1.88063
	accuracy_policy: 0.27531
	loss_value: 1.03713
[2025-04-10 02:25:14] nn step 25000, lr: 0.02.
	loss_policy: 1.87987
	accuracy_policy: 0.27589
	loss_value: 1.03515
Optimization_Done 25000
[2025-04-10 02:41:37] [command] train weight_iter_25000.pkl 32 51
[2025-04-10 02:42:00] nn step 25100, lr: 0.02.
	loss_policy: 1.89122
	accuracy_policy: 0.27173
	loss_value: 1.04214
[2025-04-10 02:42:24] nn step 25200, lr: 0.02.
	loss_policy: 1.88394
	accuracy_policy: 0.27227
	loss_value: 1.04202
[2025-04-10 02:42:47] nn step 25300, lr: 0.02.
	loss_policy: 1.88904
	accuracy_policy: 0.27196
	loss_value: 1.04176
[2025-04-10 02:43:11] nn step 25400, lr: 0.02.
	loss_policy: 1.8847
	accuracy_policy: 0.27413
	loss_value: 1.03756
[2025-04-10 02:43:36] nn step 25500, lr: 0.02.
	loss_policy: 1.88542
	accuracy_policy: 0.27215
	loss_value: 1.03789
Optimization_Done 25500
[2025-04-10 03:01:30] [command] train weight_iter_25500.pkl 33 52
[2025-04-10 03:01:53] nn step 25600, lr: 0.02.
	loss_policy: 1.88773
	accuracy_policy: 0.27357
	loss_value: 1.04056
[2025-04-10 03:02:16] nn step 25700, lr: 0.02.
	loss_policy: 1.89341
	accuracy_policy: 0.26854
	loss_value: 1.04418
[2025-04-10 03:02:39] nn step 25800, lr: 0.02.
	loss_policy: 1.89065
	accuracy_policy: 0.27133
	loss_value: 1.0438
[2025-04-10 03:03:02] nn step 25900, lr: 0.02.
	loss_policy: 1.89078
	accuracy_policy: 0.27261
	loss_value: 1.04433
[2025-04-10 03:03:26] nn step 26000, lr: 0.02.
	loss_policy: 1.88428
	accuracy_policy: 0.27368
	loss_value: 1.03989
Optimization_Done 26000
[2025-04-10 03:20:04] [command] train weight_iter_26000.pkl 34 53
[2025-04-10 03:20:27] nn step 26100, lr: 0.02.
	loss_policy: 1.88705
	accuracy_policy: 0.27196
	loss_value: 1.04615
[2025-04-10 03:20:51] nn step 26200, lr: 0.02.
	loss_policy: 1.89008
	accuracy_policy: 0.26972
	loss_value: 1.04595
[2025-04-10 03:21:15] nn step 26300, lr: 0.02.
	loss_policy: 1.89266
	accuracy_policy: 0.27118
	loss_value: 1.04763
[2025-04-10 03:21:39] nn step 26400, lr: 0.02.
	loss_policy: 1.89337
	accuracy_policy: 0.27012
	loss_value: 1.04515
[2025-04-10 03:22:03] nn step 26500, lr: 0.02.
	loss_policy: 1.89227
	accuracy_policy: 0.26934
	loss_value: 1.04108
Optimization_Done 26500
[2025-04-10 03:38:45] [command] train weight_iter_26500.pkl 35 54
[2025-04-10 03:39:09] nn step 26600, lr: 0.02.
	loss_policy: 1.90406
	accuracy_policy: 0.26528
	loss_value: 1.04399
[2025-04-10 03:39:32] nn step 26700, lr: 0.02.
	loss_policy: 1.90061
	accuracy_policy: 0.26758
	loss_value: 1.04606
[2025-04-10 03:39:56] nn step 26800, lr: 0.02.
	loss_policy: 1.89566
	accuracy_policy: 0.26792
	loss_value: 1.04563
[2025-04-10 03:40:20] nn step 26900, lr: 0.02.
	loss_policy: 1.89506
	accuracy_policy: 0.26996
	loss_value: 1.04531
[2025-04-10 03:40:43] nn step 27000, lr: 0.02.
	loss_policy: 1.88891
	accuracy_policy: 0.27191
	loss_value: 1.04181
Optimization_Done 27000
[2025-04-10 03:58:44] [command] train weight_iter_27000.pkl 36 55
[2025-04-10 03:59:08] nn step 27100, lr: 0.02.
	loss_policy: 1.89399
	accuracy_policy: 0.26858
	loss_value: 1.05052
[2025-04-10 03:59:31] nn step 27200, lr: 0.02.
	loss_policy: 1.89701
	accuracy_policy: 0.26708
	loss_value: 1.04805
[2025-04-10 03:59:55] nn step 27300, lr: 0.02.
	loss_policy: 1.89835
	accuracy_policy: 0.26724
	loss_value: 1.04827
[2025-04-10 04:00:19] nn step 27400, lr: 0.02.
	loss_policy: 1.89725
	accuracy_policy: 0.27048
	loss_value: 1.04564
[2025-04-10 04:00:43] nn step 27500, lr: 0.02.
	loss_policy: 1.89528
	accuracy_policy: 0.26772
	loss_value: 1.04944
Optimization_Done 27500
[2025-04-10 04:18:41] [command] train weight_iter_27500.pkl 37 56
[2025-04-10 04:19:04] nn step 27600, lr: 0.02.
	loss_policy: 1.89817
	accuracy_policy: 0.26786
	loss_value: 1.04778
[2025-04-10 04:19:28] nn step 27700, lr: 0.02.
	loss_policy: 1.89684
	accuracy_policy: 0.26861
	loss_value: 1.05097
[2025-04-10 04:19:52] nn step 27800, lr: 0.02.
	loss_policy: 1.89325
	accuracy_policy: 0.26748
	loss_value: 1.05067
[2025-04-10 04:20:16] nn step 27900, lr: 0.02.
	loss_policy: 1.89392
	accuracy_policy: 0.26829
	loss_value: 1.04693
[2025-04-10 04:20:39] nn step 28000, lr: 0.02.
	loss_policy: 1.89058
	accuracy_policy: 0.26932
	loss_value: 1.04596
Optimization_Done 28000
[2025-04-10 04:39:49] [command] train weight_iter_28000.pkl 38 57
[2025-04-10 04:40:12] nn step 28100, lr: 0.02.
	loss_policy: 1.89483
	accuracy_policy: 0.26696
	loss_value: 1.05304
[2025-04-10 04:40:36] nn step 28200, lr: 0.02.
	loss_policy: 1.89399
	accuracy_policy: 0.26679
	loss_value: 1.04629
[2025-04-10 04:41:00] nn step 28300, lr: 0.02.
	loss_policy: 1.89532
	accuracy_policy: 0.26819
	loss_value: 1.05066
[2025-04-10 04:41:24] nn step 28400, lr: 0.02.
	loss_policy: 1.89063
	accuracy_policy: 0.27017
	loss_value: 1.05131
[2025-04-10 04:41:47] nn step 28500, lr: 0.02.
	loss_policy: 1.8908
	accuracy_policy: 0.26888
	loss_value: 1.04968
Optimization_Done 28500
[2025-04-10 05:00:43] [command] train weight_iter_28500.pkl 39 58
[2025-04-10 05:01:06] nn step 28600, lr: 0.02.
	loss_policy: 1.89199
	accuracy_policy: 0.26728
	loss_value: 1.04604
[2025-04-10 05:01:29] nn step 28700, lr: 0.02.
	loss_policy: 1.89407
	accuracy_policy: 0.2665
	loss_value: 1.04718
[2025-04-10 05:01:52] nn step 28800, lr: 0.02.
	loss_policy: 1.89021
	accuracy_policy: 0.26795
	loss_value: 1.05556
[2025-04-10 05:02:15] nn step 28900, lr: 0.02.
	loss_policy: 1.8876
	accuracy_policy: 0.26959
	loss_value: 1.05168
[2025-04-10 05:02:39] nn step 29000, lr: 0.02.
	loss_policy: 1.88454
	accuracy_policy: 0.27071
	loss_value: 1.05125
Optimization_Done 29000
[2025-04-10 05:17:48] [command] train weight_iter_29000.pkl 40 59
[2025-04-10 05:18:12] nn step 29100, lr: 0.02.
	loss_policy: 1.88892
	accuracy_policy: 0.26785
	loss_value: 1.04626
[2025-04-10 05:18:35] nn step 29200, lr: 0.02.
	loss_policy: 1.88377
	accuracy_policy: 0.2686
	loss_value: 1.05242
[2025-04-10 05:18:59] nn step 29300, lr: 0.02.
	loss_policy: 1.88398
	accuracy_policy: 0.26996
	loss_value: 1.05739
[2025-04-10 05:19:23] nn step 29400, lr: 0.02.
	loss_policy: 1.88126
	accuracy_policy: 0.27012
	loss_value: 1.04877
[2025-04-10 05:19:47] nn step 29500, lr: 0.02.
	loss_policy: 1.88437
	accuracy_policy: 0.2711
	loss_value: 1.04905
Optimization_Done 29500
[2025-04-10 05:37:49] [command] train weight_iter_29500.pkl 41 60
[2025-04-10 05:38:12] nn step 29600, lr: 0.02.
	loss_policy: 1.89197
	accuracy_policy: 0.26638
	loss_value: 1.05384
[2025-04-10 05:38:36] nn step 29700, lr: 0.02.
	loss_policy: 1.88239
	accuracy_policy: 0.26714
	loss_value: 1.04861
[2025-04-10 05:39:00] nn step 29800, lr: 0.02.
	loss_policy: 1.89097
	accuracy_policy: 0.26665
	loss_value: 1.05037
[2025-04-10 05:39:23] nn step 29900, lr: 0.02.
	loss_policy: 1.88537
	accuracy_policy: 0.26718
	loss_value: 1.04834
[2025-04-10 05:39:47] nn step 30000, lr: 0.02.
	loss_policy: 1.88576
	accuracy_policy: 0.2697
	loss_value: 1.05487
Optimization_Done 30000
[2025-04-10 05:57:26] [command] train weight_iter_30000.pkl 42 61
[2025-04-10 05:57:49] nn step 30100, lr: 0.02.
	loss_policy: 1.89151
	accuracy_policy: 0.26551
	loss_value: 1.05436
[2025-04-10 05:58:13] nn step 30200, lr: 0.02.
	loss_policy: 1.89149
	accuracy_policy: 0.26627
	loss_value: 1.05433
[2025-04-10 05:58:37] nn step 30300, lr: 0.02.
	loss_policy: 1.88777
	accuracy_policy: 0.26756
	loss_value: 1.04976
[2025-04-10 05:59:02] nn step 30400, lr: 0.02.
	loss_policy: 1.87946
	accuracy_policy: 0.27194
	loss_value: 1.05424
[2025-04-10 05:59:26] nn step 30500, lr: 0.02.
	loss_policy: 1.88644
	accuracy_policy: 0.26896
	loss_value: 1.05314
Optimization_Done 30500
[2025-04-10 06:16:34] [command] train weight_iter_30500.pkl 43 62
[2025-04-10 06:16:57] nn step 30600, lr: 0.02.
	loss_policy: 1.88179
	accuracy_policy: 0.26908
	loss_value: 1.04512
[2025-04-10 06:17:20] nn step 30700, lr: 0.02.
	loss_policy: 1.88919
	accuracy_policy: 0.26843
	loss_value: 1.05607
[2025-04-10 06:17:44] nn step 30800, lr: 0.02.
	loss_policy: 1.88149
	accuracy_policy: 0.2707
	loss_value: 1.04955
[2025-04-10 06:18:07] nn step 30900, lr: 0.02.
	loss_policy: 1.87943
	accuracy_policy: 0.26753
	loss_value: 1.04694
[2025-04-10 06:18:31] nn step 31000, lr: 0.02.
	loss_policy: 1.88098
	accuracy_policy: 0.26699
	loss_value: 1.05251
Optimization_Done 31000
[2025-04-10 06:38:08] [command] train weight_iter_31000.pkl 44 63
[2025-04-10 06:38:31] nn step 31100, lr: 0.02.
	loss_policy: 1.88185
	accuracy_policy: 0.26863
	loss_value: 1.05117
[2025-04-10 06:38:55] nn step 31200, lr: 0.02.
	loss_policy: 1.88638
	accuracy_policy: 0.2693
	loss_value: 1.05081
[2025-04-10 06:39:18] nn step 31300, lr: 0.02.
	loss_policy: 1.88249
	accuracy_policy: 0.26776
	loss_value: 1.05115
[2025-04-10 06:39:42] nn step 31400, lr: 0.02.
	loss_policy: 1.88332
	accuracy_policy: 0.26723
	loss_value: 1.05322
[2025-04-10 06:40:06] nn step 31500, lr: 0.02.
	loss_policy: 1.88141
	accuracy_policy: 0.26582
	loss_value: 1.04927
Optimization_Done 31500
[2025-04-10 06:59:50] [command] train weight_iter_31500.pkl 45 64
[2025-04-10 07:00:14] nn step 31600, lr: 0.02.
	loss_policy: 1.88499
	accuracy_policy: 0.26541
	loss_value: 1.0508
[2025-04-10 07:00:37] nn step 31700, lr: 0.02.
	loss_policy: 1.88352
	accuracy_policy: 0.269
	loss_value: 1.0518
[2025-04-10 07:01:01] nn step 31800, lr: 0.02.
	loss_policy: 1.88003
	accuracy_policy: 0.26807
	loss_value: 1.04688
[2025-04-10 07:01:25] nn step 31900, lr: 0.02.
	loss_policy: 1.87795
	accuracy_policy: 0.26836
	loss_value: 1.05107
[2025-04-10 07:01:49] nn step 32000, lr: 0.02.
	loss_policy: 1.88134
	accuracy_policy: 0.26936
	loss_value: 1.04638
Optimization_Done 32000
[2025-04-10 07:20:12] [command] train weight_iter_32000.pkl 46 65
[2025-04-10 07:20:36] nn step 32100, lr: 0.02.
	loss_policy: 1.89079
	accuracy_policy: 0.26729
	loss_value: 1.05172
[2025-04-10 07:21:00] nn step 32200, lr: 0.02.
	loss_policy: 1.88663
	accuracy_policy: 0.26432
	loss_value: 1.05212
[2025-04-10 07:21:24] nn step 32300, lr: 0.02.
	loss_policy: 1.88132
	accuracy_policy: 0.26632
	loss_value: 1.0446
[2025-04-10 07:21:47] nn step 32400, lr: 0.02.
	loss_policy: 1.88313
	accuracy_policy: 0.26546
	loss_value: 1.04533
[2025-04-10 07:22:11] nn step 32500, lr: 0.02.
	loss_policy: 1.88259
	accuracy_policy: 0.26735
	loss_value: 1.05152
Optimization_Done 32500
[2025-04-10 07:41:41] [command] train weight_iter_32500.pkl 47 66
[2025-04-10 07:42:04] nn step 32600, lr: 0.02.
	loss_policy: 1.87864
	accuracy_policy: 0.26886
	loss_value: 1.05491
[2025-04-10 07:42:27] nn step 32700, lr: 0.02.
	loss_policy: 1.88295
	accuracy_policy: 0.26861
	loss_value: 1.05447
[2025-04-10 07:42:51] nn step 32800, lr: 0.02.
	loss_policy: 1.88387
	accuracy_policy: 0.26656
	loss_value: 1.04553
[2025-04-10 07:43:15] nn step 32900, lr: 0.02.
	loss_policy: 1.88142
	accuracy_policy: 0.26924
	loss_value: 1.04851
[2025-04-10 07:43:39] nn step 33000, lr: 0.02.
	loss_policy: 1.88119
	accuracy_policy: 0.26915
	loss_value: 1.04437
Optimization_Done 33000
[2025-04-10 08:02:40] [command] train weight_iter_33000.pkl 48 67
[2025-04-10 08:03:04] nn step 33100, lr: 0.02.
	loss_policy: 1.88798
	accuracy_policy: 0.2668
	loss_value: 1.05465
[2025-04-10 08:03:28] nn step 33200, lr: 0.02.
	loss_policy: 1.89336
	accuracy_policy: 0.2646
	loss_value: 1.04598
[2025-04-10 08:03:51] nn step 33300, lr: 0.02.
	loss_policy: 1.8811
	accuracy_policy: 0.26988
	loss_value: 1.04843
[2025-04-10 08:04:15] nn step 33400, lr: 0.02.
	loss_policy: 1.88389
	accuracy_policy: 0.26774
	loss_value: 1.04683
[2025-04-10 08:04:38] nn step 33500, lr: 0.02.
	loss_policy: 1.88106
	accuracy_policy: 0.26924
	loss_value: 1.04985
Optimization_Done 33500
[2025-04-10 08:24:12] [command] train weight_iter_33500.pkl 49 68
[2025-04-10 08:24:35] nn step 33600, lr: 0.02.
	loss_policy: 1.89829
	accuracy_policy: 0.26125
	loss_value: 1.05038
[2025-04-10 08:24:59] nn step 33700, lr: 0.02.
	loss_policy: 1.89163
	accuracy_policy: 0.26313
	loss_value: 1.04689
[2025-04-10 08:25:24] nn step 33800, lr: 0.02.
	loss_policy: 1.8904
	accuracy_policy: 0.26606
	loss_value: 1.05113
[2025-04-10 08:25:47] nn step 33900, lr: 0.02.
	loss_policy: 1.8839
	accuracy_policy: 0.26711
	loss_value: 1.04561
[2025-04-10 08:26:11] nn step 34000, lr: 0.02.
	loss_policy: 1.88496
	accuracy_policy: 0.26597
	loss_value: 1.04495
Optimization_Done 34000
[2025-04-10 08:46:40] [command] train weight_iter_34000.pkl 50 69
[2025-04-10 08:47:02] nn step 34100, lr: 0.02.
	loss_policy: 1.88785
	accuracy_policy: 0.26418
	loss_value: 1.05105
[2025-04-10 08:47:25] nn step 34200, lr: 0.02.
	loss_policy: 1.88629
	accuracy_policy: 0.26769
	loss_value: 1.05363
[2025-04-10 08:47:48] nn step 34300, lr: 0.02.
	loss_policy: 1.88597
	accuracy_policy: 0.26527
	loss_value: 1.04657
[2025-04-10 08:48:12] nn step 34400, lr: 0.02.
	loss_policy: 1.88961
	accuracy_policy: 0.26594
	loss_value: 1.04863
[2025-04-10 08:48:36] nn step 34500, lr: 0.02.
	loss_policy: 1.8828
	accuracy_policy: 0.26447
	loss_value: 1.04772
Optimization_Done 34500
[2025-04-10 09:09:07] [command] train weight_iter_34500.pkl 51 70
[2025-04-10 09:09:30] nn step 34600, lr: 0.02.
	loss_policy: 1.8884
	accuracy_policy: 0.26493
	loss_value: 1.0496
[2025-04-10 09:09:54] nn step 34700, lr: 0.02.
	loss_policy: 1.88971
	accuracy_policy: 0.26582
	loss_value: 1.04708
[2025-04-10 09:10:17] nn step 34800, lr: 0.02.
	loss_policy: 1.89063
	accuracy_policy: 0.26435
	loss_value: 1.05322
[2025-04-10 09:10:40] nn step 34900, lr: 0.02.
	loss_policy: 1.88546
	accuracy_policy: 0.26487
	loss_value: 1.04706
[2025-04-10 09:11:04] nn step 35000, lr: 0.02.
	loss_policy: 1.88662
	accuracy_policy: 0.26431
	loss_value: 1.04962
Optimization_Done 35000
[2025-04-10 09:30:37] [command] train weight_iter_35000.pkl 52 71
[2025-04-10 09:31:00] nn step 35100, lr: 0.02.
	loss_policy: 1.89572
	accuracy_policy: 0.26444
	loss_value: 1.04697
[2025-04-10 09:31:24] nn step 35200, lr: 0.02.
	loss_policy: 1.89758
	accuracy_policy: 0.2629
	loss_value: 1.05268
[2025-04-10 09:31:48] nn step 35300, lr: 0.02.
	loss_policy: 1.89208
	accuracy_policy: 0.26579
	loss_value: 1.0498
[2025-04-10 09:32:12] nn step 35400, lr: 0.02.
	loss_policy: 1.88929
	accuracy_policy: 0.26662
	loss_value: 1.04695
[2025-04-10 09:32:36] nn step 35500, lr: 0.02.
	loss_policy: 1.89358
	accuracy_policy: 0.26542
	loss_value: 1.04677
Optimization_Done 35500
[2025-04-10 09:52:58] [command] train weight_iter_35500.pkl 53 72
[2025-04-10 09:53:21] nn step 35600, lr: 0.02.
	loss_policy: 1.89112
	accuracy_policy: 0.26441
	loss_value: 1.0478
[2025-04-10 09:53:45] nn step 35700, lr: 0.02.
	loss_policy: 1.88736
	accuracy_policy: 0.26892
	loss_value: 1.05296
[2025-04-10 09:54:09] nn step 35800, lr: 0.02.
	loss_policy: 1.89387
	accuracy_policy: 0.26482
	loss_value: 1.04999
[2025-04-10 09:54:32] nn step 35900, lr: 0.02.
	loss_policy: 1.88721
	accuracy_policy: 0.26646
	loss_value: 1.05082
[2025-04-10 09:54:56] nn step 36000, lr: 0.02.
	loss_policy: 1.8849
	accuracy_policy: 0.26729
	loss_value: 1.05076
Optimization_Done 36000
[2025-04-10 10:15:52] [command] train weight_iter_36000.pkl 54 73
[2025-04-10 10:16:16] nn step 36100, lr: 0.02.
	loss_policy: 1.89318
	accuracy_policy: 0.2643
	loss_value: 1.05256
[2025-04-10 10:16:39] nn step 36200, lr: 0.02.
	loss_policy: 1.89512
	accuracy_policy: 0.26559
	loss_value: 1.0536
[2025-04-10 10:17:01] nn step 36300, lr: 0.02.
	loss_policy: 1.89447
	accuracy_policy: 0.26759
	loss_value: 1.05621
[2025-04-10 10:17:23] nn step 36400, lr: 0.02.
	loss_policy: 1.89018
	accuracy_policy: 0.26833
	loss_value: 1.05112
[2025-04-10 10:17:47] nn step 36500, lr: 0.02.
	loss_policy: 1.88077
	accuracy_policy: 0.26982
	loss_value: 1.05036
Optimization_Done 36500
[2025-04-10 10:38:23] [command] train weight_iter_36500.pkl 55 74
[2025-04-10 10:38:47] nn step 36600, lr: 0.02.
	loss_policy: 1.88862
	accuracy_policy: 0.26756
	loss_value: 1.05459
[2025-04-10 10:39:11] nn step 36700, lr: 0.02.
	loss_policy: 1.89046
	accuracy_policy: 0.26662
	loss_value: 1.05604
[2025-04-10 10:39:35] nn step 36800, lr: 0.02.
	loss_policy: 1.88746
	accuracy_policy: 0.26958
	loss_value: 1.0557
[2025-04-10 10:39:58] nn step 36900, lr: 0.02.
	loss_policy: 1.88406
	accuracy_policy: 0.2698
	loss_value: 1.04981
[2025-04-10 10:40:22] nn step 37000, lr: 0.02.
	loss_policy: 1.88711
	accuracy_policy: 0.26754
	loss_value: 1.05259
Optimization_Done 37000
[2025-04-10 11:01:07] [command] train weight_iter_37000.pkl 56 75
[2025-04-10 11:01:30] nn step 37100, lr: 0.02.
	loss_policy: 1.88778
	accuracy_policy: 0.27014
	loss_value: 1.05234
[2025-04-10 11:01:54] nn step 37200, lr: 0.02.
	loss_policy: 1.88473
	accuracy_policy: 0.2687
	loss_value: 1.05114
[2025-04-10 11:02:18] nn step 37300, lr: 0.02.
	loss_policy: 1.88795
	accuracy_policy: 0.2695
	loss_value: 1.05451
[2025-04-10 11:02:41] nn step 37400, lr: 0.02.
	loss_policy: 1.88178
	accuracy_policy: 0.26928
	loss_value: 1.05571
[2025-04-10 11:03:05] nn step 37500, lr: 0.02.
	loss_policy: 1.88332
	accuracy_policy: 0.27239
	loss_value: 1.05452
Optimization_Done 37500
[2025-04-10 11:23:21] [command] train weight_iter_37500.pkl 57 76
[2025-04-10 11:23:45] nn step 37600, lr: 0.02.
	loss_policy: 1.87989
	accuracy_policy: 0.27202
	loss_value: 1.05341
[2025-04-10 11:24:09] nn step 37700, lr: 0.02.
	loss_policy: 1.88416
	accuracy_policy: 0.27215
	loss_value: 1.05286
[2025-04-10 11:24:33] nn step 37800, lr: 0.02.
	loss_policy: 1.87574
	accuracy_policy: 0.27347
	loss_value: 1.05228
[2025-04-10 11:24:57] nn step 37900, lr: 0.02.
	loss_policy: 1.88181
	accuracy_policy: 0.27146
	loss_value: 1.05314
[2025-04-10 11:25:20] nn step 38000, lr: 0.02.
	loss_policy: 1.87543
	accuracy_policy: 0.2727
	loss_value: 1.05028
Optimization_Done 38000
[2025-04-10 11:46:04] [command] train weight_iter_38000.pkl 58 77
[2025-04-10 11:46:28] nn step 38100, lr: 0.02.
	loss_policy: 1.8715
	accuracy_policy: 0.27424
	loss_value: 1.05084
[2025-04-10 11:46:51] nn step 38200, lr: 0.02.
	loss_policy: 1.87742
	accuracy_policy: 0.27262
	loss_value: 1.0562
[2025-04-10 11:47:16] nn step 38300, lr: 0.02.
	loss_policy: 1.8744
	accuracy_policy: 0.27563
	loss_value: 1.05535
[2025-04-10 11:47:40] nn step 38400, lr: 0.02.
	loss_policy: 1.8729
	accuracy_policy: 0.2763
	loss_value: 1.05436
[2025-04-10 11:48:04] nn step 38500, lr: 0.02.
	loss_policy: 1.87345
	accuracy_policy: 0.27628
	loss_value: 1.05157
Optimization_Done 38500
[2025-04-10 12:08:14] [command] train weight_iter_38500.pkl 59 78
[2025-04-10 12:08:37] nn step 38600, lr: 0.02.
	loss_policy: 1.87934
	accuracy_policy: 0.27569
	loss_value: 1.05377
[2025-04-10 12:09:01] nn step 38700, lr: 0.02.
	loss_policy: 1.87305
	accuracy_policy: 0.27509
	loss_value: 1.05297
[2025-04-10 12:09:25] nn step 38800, lr: 0.02.
	loss_policy: 1.87059
	accuracy_policy: 0.27614
	loss_value: 1.05325
[2025-04-10 12:09:49] nn step 38900, lr: 0.02.
	loss_policy: 1.86594
	accuracy_policy: 0.27916
	loss_value: 1.04887
[2025-04-10 12:10:12] nn step 39000, lr: 0.02.
	loss_policy: 1.86459
	accuracy_policy: 0.27957
	loss_value: 1.04947
Optimization_Done 39000
[2025-04-10 12:31:37] [command] train weight_iter_39000.pkl 60 79
[2025-04-10 12:32:00] nn step 39100, lr: 0.02.
	loss_policy: 1.87659
	accuracy_policy: 0.27757
	loss_value: 1.06112
[2025-04-10 12:32:24] nn step 39200, lr: 0.02.
	loss_policy: 1.87332
	accuracy_policy: 0.28031
	loss_value: 1.05699
[2025-04-10 12:32:48] nn step 39300, lr: 0.02.
	loss_policy: 1.86955
	accuracy_policy: 0.27766
	loss_value: 1.05658
[2025-04-10 12:33:12] nn step 39400, lr: 0.02.
	loss_policy: 1.8647
	accuracy_policy: 0.28175
	loss_value: 1.05678
[2025-04-10 12:33:35] nn step 39500, lr: 0.02.
	loss_policy: 1.86811
	accuracy_policy: 0.28016
	loss_value: 1.05401
Optimization_Done 39500
[2025-04-10 12:55:20] [command] train weight_iter_39500.pkl 61 80
[2025-04-10 12:55:44] nn step 39600, lr: 0.02.
	loss_policy: 1.86612
	accuracy_policy: 0.27997
	loss_value: 1.06446
[2025-04-10 12:56:08] nn step 39700, lr: 0.02.
	loss_policy: 1.85583
	accuracy_policy: 0.28313
	loss_value: 1.06329
[2025-04-10 12:56:32] nn step 39800, lr: 0.02.
	loss_policy: 1.86019
	accuracy_policy: 0.28031
	loss_value: 1.06221
[2025-04-10 12:56:56] nn step 39900, lr: 0.02.
	loss_policy: 1.86296
	accuracy_policy: 0.28095
	loss_value: 1.06289
[2025-04-10 12:57:20] nn step 40000, lr: 0.02.
	loss_policy: 1.85892
	accuracy_policy: 0.28295
	loss_value: 1.05868
Optimization_Done 40000
[2025-04-10 13:18:11] [command] train weight_iter_40000.pkl 62 81
[2025-04-10 13:18:34] nn step 40100, lr: 0.02.
	loss_policy: 1.85626
	accuracy_policy: 0.2841
	loss_value: 1.06338
[2025-04-10 13:18:57] nn step 40200, lr: 0.02.
	loss_policy: 1.85922
	accuracy_policy: 0.28385
	loss_value: 1.06389
[2025-04-10 13:19:21] nn step 40300, lr: 0.02.
	loss_policy: 1.85854
	accuracy_policy: 0.28369
	loss_value: 1.06741
[2025-04-10 13:19:45] nn step 40400, lr: 0.02.
	loss_policy: 1.85541
	accuracy_policy: 0.28343
	loss_value: 1.05946
[2025-04-10 13:20:09] nn step 40500, lr: 0.02.
	loss_policy: 1.85895
	accuracy_policy: 0.28281
	loss_value: 1.05925
Optimization_Done 40500
[2025-04-10 13:40:59] [command] train weight_iter_40500.pkl 63 82
[2025-04-10 13:41:23] nn step 40600, lr: 0.02.
	loss_policy: 1.85661
	accuracy_policy: 0.28519
	loss_value: 1.06701
[2025-04-10 13:41:47] nn step 40700, lr: 0.02.
	loss_policy: 1.85611
	accuracy_policy: 0.28483
	loss_value: 1.06429
[2025-04-10 13:42:11] nn step 40800, lr: 0.02.
	loss_policy: 1.85022
	accuracy_policy: 0.28807
	loss_value: 1.06714
[2025-04-10 13:42:35] nn step 40900, lr: 0.02.
	loss_policy: 1.85069
	accuracy_policy: 0.28679
	loss_value: 1.06041
[2025-04-10 13:42:58] nn step 41000, lr: 0.02.
	loss_policy: 1.85008
	accuracy_policy: 0.28709
	loss_value: 1.07245
Optimization_Done 41000
[2025-04-10 14:04:19] [command] train weight_iter_41000.pkl 64 83
[2025-04-10 14:04:42] nn step 41100, lr: 0.02.
	loss_policy: 1.85452
	accuracy_policy: 0.28573
	loss_value: 1.0721
[2025-04-10 14:05:06] nn step 41200, lr: 0.02.
	loss_policy: 1.85184
	accuracy_policy: 0.28758
	loss_value: 1.06963
[2025-04-10 14:05:30] nn step 41300, lr: 0.02.
	loss_policy: 1.85693
	accuracy_policy: 0.28876
	loss_value: 1.07436
[2025-04-10 14:05:54] nn step 41400, lr: 0.02.
	loss_policy: 1.85261
	accuracy_policy: 0.28867
	loss_value: 1.06734
[2025-04-10 14:06:19] nn step 41500, lr: 0.02.
	loss_policy: 1.84761
	accuracy_policy: 0.28857
	loss_value: 1.07182
Optimization_Done 41500
[2025-04-10 14:27:55] [command] train weight_iter_41500.pkl 65 84
[2025-04-10 14:28:18] nn step 41600, lr: 0.02.
	loss_policy: 1.8514
	accuracy_policy: 0.29127
	loss_value: 1.0778
[2025-04-10 14:28:42] nn step 41700, lr: 0.02.
	loss_policy: 1.84586
	accuracy_policy: 0.28891
	loss_value: 1.07672
[2025-04-10 14:29:06] nn step 41800, lr: 0.02.
	loss_policy: 1.84722
	accuracy_policy: 0.29012
	loss_value: 1.07276
[2025-04-10 14:29:30] nn step 41900, lr: 0.02.
	loss_policy: 1.8439
	accuracy_policy: 0.29107
	loss_value: 1.07527
[2025-04-10 14:29:54] nn step 42000, lr: 0.02.
	loss_policy: 1.83606
	accuracy_policy: 0.29336
	loss_value: 1.07268
Optimization_Done 42000
[2025-04-10 14:50:33] [command] train weight_iter_42000.pkl 66 85
[2025-04-10 14:50:57] nn step 42100, lr: 0.02.
	loss_policy: 1.84786
	accuracy_policy: 0.29063
	loss_value: 1.07864
[2025-04-10 14:51:20] nn step 42200, lr: 0.02.
	loss_policy: 1.83621
	accuracy_policy: 0.29686
	loss_value: 1.07929
[2025-04-10 14:51:44] nn step 42300, lr: 0.02.
	loss_policy: 1.83899
	accuracy_policy: 0.29377
	loss_value: 1.07448
[2025-04-10 14:52:07] nn step 42400, lr: 0.02.
	loss_policy: 1.83825
	accuracy_policy: 0.2934
	loss_value: 1.07654
[2025-04-10 14:52:31] nn step 42500, lr: 0.02.
	loss_policy: 1.83868
	accuracy_policy: 0.29116
	loss_value: 1.0753
Optimization_Done 42500
[2025-04-10 15:13:37] [command] train weight_iter_42500.pkl 67 86
[2025-04-10 15:14:01] nn step 42600, lr: 0.02.
	loss_policy: 1.84076
	accuracy_policy: 0.29372
	loss_value: 1.07767
[2025-04-10 15:14:25] nn step 42700, lr: 0.02.
	loss_policy: 1.83665
	accuracy_policy: 0.2945
	loss_value: 1.07942
[2025-04-10 15:14:49] nn step 42800, lr: 0.02.
	loss_policy: 1.83281
	accuracy_policy: 0.29554
	loss_value: 1.08273
[2025-04-10 15:15:13] nn step 42900, lr: 0.02.
	loss_policy: 1.82299
	accuracy_policy: 0.29833
	loss_value: 1.07771
[2025-04-10 15:15:38] nn step 43000, lr: 0.02.
	loss_policy: 1.83319
	accuracy_policy: 0.29605
	loss_value: 1.07843
Optimization_Done 43000
[2025-04-10 15:37:16] [command] train weight_iter_43000.pkl 68 87
[2025-04-10 15:37:38] nn step 43100, lr: 0.02.
	loss_policy: 1.83498
	accuracy_policy: 0.29656
	loss_value: 1.07787
[2025-04-10 15:38:02] nn step 43200, lr: 0.02.
	loss_policy: 1.83322
	accuracy_policy: 0.2949
	loss_value: 1.08222
[2025-04-10 15:38:25] nn step 43300, lr: 0.02.
	loss_policy: 1.82568
	accuracy_policy: 0.29707
	loss_value: 1.07697
[2025-04-10 15:38:49] nn step 43400, lr: 0.02.
	loss_policy: 1.82848
	accuracy_policy: 0.29796
	loss_value: 1.08279
[2025-04-10 15:39:11] nn step 43500, lr: 0.02.
	loss_policy: 1.82903
	accuracy_policy: 0.29864
	loss_value: 1.08278
Optimization_Done 43500
[2025-04-10 16:00:03] [command] train weight_iter_43500.pkl 69 88
[2025-04-10 16:00:26] nn step 43600, lr: 0.02.
	loss_policy: 1.83099
	accuracy_policy: 0.29717
	loss_value: 1.09145
[2025-04-10 16:00:49] nn step 43700, lr: 0.02.
	loss_policy: 1.8258
	accuracy_policy: 0.29808
	loss_value: 1.08683
[2025-04-10 16:01:14] nn step 43800, lr: 0.02.
	loss_policy: 1.82083
	accuracy_policy: 0.30073
	loss_value: 1.08697
[2025-04-10 16:01:37] nn step 43900, lr: 0.02.
	loss_policy: 1.82146
	accuracy_policy: 0.30179
	loss_value: 1.08496
[2025-04-10 16:02:01] nn step 44000, lr: 0.02.
	loss_policy: 1.82278
	accuracy_policy: 0.29976
	loss_value: 1.08397
Optimization_Done 44000
[2025-04-10 16:23:33] [command] train weight_iter_44000.pkl 70 89
[2025-04-10 16:23:56] nn step 44100, lr: 0.02.
	loss_policy: 1.82009
	accuracy_policy: 0.30174
	loss_value: 1.08975
[2025-04-10 16:24:20] nn step 44200, lr: 0.02.
	loss_policy: 1.81449
	accuracy_policy: 0.30318
	loss_value: 1.08436
[2025-04-10 16:24:43] nn step 44300, lr: 0.02.
	loss_policy: 1.82594
	accuracy_policy: 0.29924
	loss_value: 1.08635
[2025-04-10 16:25:07] nn step 44400, lr: 0.02.
	loss_policy: 1.82348
	accuracy_policy: 0.30075
	loss_value: 1.08508
[2025-04-10 16:25:31] nn step 44500, lr: 0.02.
	loss_policy: 1.81545
	accuracy_policy: 0.30205
	loss_value: 1.08399
Optimization_Done 44500
[2025-04-10 16:47:09] [command] train weight_iter_44500.pkl 71 90
[2025-04-10 16:47:32] nn step 44600, lr: 0.02.
	loss_policy: 1.8232
	accuracy_policy: 0.30194
	loss_value: 1.09212
[2025-04-10 16:47:56] nn step 44700, lr: 0.02.
	loss_policy: 1.82763
	accuracy_policy: 0.29703
	loss_value: 1.09216
[2025-04-10 16:48:19] nn step 44800, lr: 0.02.
	loss_policy: 1.82046
	accuracy_policy: 0.30372
	loss_value: 1.09177
[2025-04-10 16:48:43] nn step 44900, lr: 0.02.
	loss_policy: 1.8216
	accuracy_policy: 0.30261
	loss_value: 1.08457
[2025-04-10 16:49:07] nn step 45000, lr: 0.02.
	loss_policy: 1.81906
	accuracy_policy: 0.30431
	loss_value: 1.09308
Optimization_Done 45000
[2025-04-10 17:10:13] [command] train weight_iter_45000.pkl 72 91
[2025-04-10 17:10:35] nn step 45100, lr: 0.02.
	loss_policy: 1.81756
	accuracy_policy: 0.30391
	loss_value: 1.09068
[2025-04-10 17:10:59] nn step 45200, lr: 0.02.
	loss_policy: 1.81333
	accuracy_policy: 0.30461
	loss_value: 1.09245
[2025-04-10 17:11:22] nn step 45300, lr: 0.02.
	loss_policy: 1.81678
	accuracy_policy: 0.30625
	loss_value: 1.09098
[2025-04-10 17:11:46] nn step 45400, lr: 0.02.
	loss_policy: 1.81221
	accuracy_policy: 0.30701
	loss_value: 1.09291
[2025-04-10 17:12:09] nn step 45500, lr: 0.02.
	loss_policy: 1.81378
	accuracy_policy: 0.30442
	loss_value: 1.0868
Optimization_Done 45500
[2025-04-10 17:34:35] [command] train weight_iter_45500.pkl 73 92
[2025-04-10 17:34:58] nn step 45600, lr: 0.02.
	loss_policy: 1.80925
	accuracy_policy: 0.30593
	loss_value: 1.09037
[2025-04-10 17:35:23] nn step 45700, lr: 0.02.
	loss_policy: 1.80808
	accuracy_policy: 0.30891
	loss_value: 1.09023
[2025-04-10 17:35:47] nn step 45800, lr: 0.02.
	loss_policy: 1.8149
	accuracy_policy: 0.30201
	loss_value: 1.09271
[2025-04-10 17:36:11] nn step 45900, lr: 0.02.
	loss_policy: 1.80667
	accuracy_policy: 0.30608
	loss_value: 1.08926
[2025-04-10 17:36:35] nn step 46000, lr: 0.02.
	loss_policy: 1.81021
	accuracy_policy: 0.30547
	loss_value: 1.08983
Optimization_Done 46000
[2025-04-10 17:58:21] [command] train weight_iter_46000.pkl 74 93
[2025-04-10 17:58:44] nn step 46100, lr: 0.02.
	loss_policy: 1.80384
	accuracy_policy: 0.30744
	loss_value: 1.09201
[2025-04-10 17:59:08] nn step 46200, lr: 0.02.
	loss_policy: 1.80694
	accuracy_policy: 0.30582
	loss_value: 1.09706
[2025-04-10 17:59:32] nn step 46300, lr: 0.02.
	loss_policy: 1.80265
	accuracy_policy: 0.30766
	loss_value: 1.08981
[2025-04-10 17:59:56] nn step 46400, lr: 0.02.
	loss_policy: 1.80226
	accuracy_policy: 0.30814
	loss_value: 1.09294
[2025-04-10 18:00:20] nn step 46500, lr: 0.02.
	loss_policy: 1.8041
	accuracy_policy: 0.30549
	loss_value: 1.08811
Optimization_Done 46500
[2025-04-10 18:21:36] [command] train weight_iter_46500.pkl 75 94
[2025-04-10 18:22:00] nn step 46600, lr: 0.02.
	loss_policy: 1.80485
	accuracy_policy: 0.30606
	loss_value: 1.09459
[2025-04-10 18:22:24] nn step 46700, lr: 0.02.
	loss_policy: 1.80252
	accuracy_policy: 0.30633
	loss_value: 1.08982
[2025-04-10 18:22:48] nn step 46800, lr: 0.02.
	loss_policy: 1.7945
	accuracy_policy: 0.30993
	loss_value: 1.09325
[2025-04-10 18:23:11] nn step 46900, lr: 0.02.
	loss_policy: 1.80436
	accuracy_policy: 0.30527
	loss_value: 1.08666
[2025-04-10 18:23:36] nn step 47000, lr: 0.02.
	loss_policy: 1.7976
	accuracy_policy: 0.309
	loss_value: 1.09172
Optimization_Done 47000
[2025-04-10 18:45:00] [command] train weight_iter_47000.pkl 76 95
[2025-04-10 18:45:24] nn step 47100, lr: 0.02.
	loss_policy: 1.7982
	accuracy_policy: 0.3118
	loss_value: 1.0962
[2025-04-10 18:45:48] nn step 47200, lr: 0.02.
	loss_policy: 1.79668
	accuracy_policy: 0.31046
	loss_value: 1.09951
[2025-04-10 18:46:12] nn step 47300, lr: 0.02.
	loss_policy: 1.79924
	accuracy_policy: 0.30835
	loss_value: 1.08963
[2025-04-10 18:46:36] nn step 47400, lr: 0.02.
	loss_policy: 1.80085
	accuracy_policy: 0.30973
	loss_value: 1.09728
[2025-04-10 18:47:00] nn step 47500, lr: 0.02.
	loss_policy: 1.7933
	accuracy_policy: 0.31044
	loss_value: 1.08675
Optimization_Done 47500
[2025-04-10 19:07:57] [command] train weight_iter_47500.pkl 77 96
[2025-04-10 19:08:20] nn step 47600, lr: 0.02.
	loss_policy: 1.79403
	accuracy_policy: 0.31169
	loss_value: 1.09682
[2025-04-10 19:08:44] nn step 47700, lr: 0.02.
	loss_policy: 1.79381
	accuracy_policy: 0.31152
	loss_value: 1.0951
[2025-04-10 19:09:07] nn step 47800, lr: 0.02.
	loss_policy: 1.80061
	accuracy_policy: 0.30977
	loss_value: 1.09269
[2025-04-10 19:09:30] nn step 47900, lr: 0.02.
	loss_policy: 1.78651
	accuracy_policy: 0.31349
	loss_value: 1.08693
[2025-04-10 19:09:54] nn step 48000, lr: 0.02.
	loss_policy: 1.79142
	accuracy_policy: 0.31414
	loss_value: 1.08321
Optimization_Done 48000
[2025-04-10 19:31:06] [command] train weight_iter_48000.pkl 78 97
[2025-04-10 19:31:28] nn step 48100, lr: 0.02.
	loss_policy: 1.79619
	accuracy_policy: 0.31371
	loss_value: 1.09772
[2025-04-10 19:31:51] nn step 48200, lr: 0.02.
	loss_policy: 1.79017
	accuracy_policy: 0.31225
	loss_value: 1.09161
[2025-04-10 19:32:15] nn step 48300, lr: 0.02.
	loss_policy: 1.79427
	accuracy_policy: 0.31014
	loss_value: 1.0936
[2025-04-10 19:32:37] nn step 48400, lr: 0.02.
	loss_policy: 1.78735
	accuracy_policy: 0.31388
	loss_value: 1.08936
[2025-04-10 19:33:01] nn step 48500, lr: 0.02.
	loss_policy: 1.79041
	accuracy_policy: 0.31362
	loss_value: 1.08379
Optimization_Done 48500
[2025-04-10 19:54:51] [command] train weight_iter_48500.pkl 79 98
[2025-04-10 19:55:14] nn step 48600, lr: 0.02.
	loss_policy: 1.78502
	accuracy_policy: 0.31504
	loss_value: 1.09604
[2025-04-10 19:55:38] nn step 48700, lr: 0.02.
	loss_policy: 1.77853
	accuracy_policy: 0.31871
	loss_value: 1.09686
[2025-04-10 19:56:02] nn step 48800, lr: 0.02.
	loss_policy: 1.78117
	accuracy_policy: 0.31736
	loss_value: 1.09972
[2025-04-10 19:56:26] nn step 48900, lr: 0.02.
	loss_policy: 1.78017
	accuracy_policy: 0.31728
	loss_value: 1.09033
[2025-04-10 19:56:50] nn step 49000, lr: 0.02.
	loss_policy: 1.77327
	accuracy_policy: 0.31718
	loss_value: 1.09358
Optimization_Done 49000
[2025-04-10 20:19:01] [command] train weight_iter_49000.pkl 80 99
[2025-04-10 20:19:24] nn step 49100, lr: 0.02.
	loss_policy: 1.78591
	accuracy_policy: 0.31645
	loss_value: 1.09717
[2025-04-10 20:19:48] nn step 49200, lr: 0.02.
	loss_policy: 1.78129
	accuracy_policy: 0.31674
	loss_value: 1.09816
[2025-04-10 20:20:13] nn step 49300, lr: 0.02.
	loss_policy: 1.77384
	accuracy_policy: 0.31894
	loss_value: 1.09032
[2025-04-10 20:20:37] nn step 49400, lr: 0.02.
	loss_policy: 1.77455
	accuracy_policy: 0.31882
	loss_value: 1.0961
[2025-04-10 20:21:01] nn step 49500, lr: 0.02.
	loss_policy: 1.77192
	accuracy_policy: 0.318
	loss_value: 1.09312
Optimization_Done 49500
[2025-04-10 20:43:01] [command] train weight_iter_49500.pkl 81 100
[2025-04-10 20:43:25] nn step 49600, lr: 0.02.
	loss_policy: 1.77423
	accuracy_policy: 0.31867
	loss_value: 1.10699
[2025-04-10 20:43:49] nn step 49700, lr: 0.02.
	loss_policy: 1.77644
	accuracy_policy: 0.31978
	loss_value: 1.10195
[2025-04-10 20:44:12] nn step 49800, lr: 0.02.
	loss_policy: 1.77171
	accuracy_policy: 0.3211
	loss_value: 1.09498
[2025-04-10 20:44:37] nn step 49900, lr: 0.02.
	loss_policy: 1.7715
	accuracy_policy: 0.32065
	loss_value: 1.0994
[2025-04-10 20:45:01] nn step 50000, lr: 0.02.
	loss_policy: 1.7691
	accuracy_policy: 0.32125
	loss_value: 1.0969
Optimization_Done 50000
[2025-04-10 21:06:16] [command] train weight_iter_50000.pkl 82 101
[2025-04-10 21:06:39] nn step 50100, lr: 0.02.
	loss_policy: 1.77589
	accuracy_policy: 0.32081
	loss_value: 1.09645
[2025-04-10 21:07:03] nn step 50200, lr: 0.02.
	loss_policy: 1.77243
	accuracy_policy: 0.31925
	loss_value: 1.09074
[2025-04-10 21:07:27] nn step 50300, lr: 0.02.
	loss_policy: 1.77495
	accuracy_policy: 0.31973
	loss_value: 1.09186
[2025-04-10 21:07:50] nn step 50400, lr: 0.02.
	loss_policy: 1.7648
	accuracy_policy: 0.32255
	loss_value: 1.09384
[2025-04-10 21:08:15] nn step 50500, lr: 0.02.
	loss_policy: 1.77028
	accuracy_policy: 0.32086
	loss_value: 1.09119
Optimization_Done 50500
[2025-04-10 21:30:39] [command] train weight_iter_50500.pkl 83 102
[2025-04-10 21:31:02] nn step 50600, lr: 0.02.
	loss_policy: 1.77082
	accuracy_policy: 0.3231
	loss_value: 1.08838
[2025-04-10 21:31:25] nn step 50700, lr: 0.02.
	loss_policy: 1.76441
	accuracy_policy: 0.32493
	loss_value: 1.09244
[2025-04-10 21:31:49] nn step 50800, lr: 0.02.
	loss_policy: 1.76267
	accuracy_policy: 0.32593
	loss_value: 1.08878
[2025-04-10 21:32:13] nn step 50900, lr: 0.02.
	loss_policy: 1.75781
	accuracy_policy: 0.32778
	loss_value: 1.08376
[2025-04-10 21:32:36] nn step 51000, lr: 0.02.
	loss_policy: 1.76324
	accuracy_policy: 0.32365
	loss_value: 1.09034
Optimization_Done 51000
[2025-04-10 21:55:22] [command] train weight_iter_51000.pkl 84 103
[2025-04-10 21:55:44] nn step 51100, lr: 0.02.
	loss_policy: 1.76839
	accuracy_policy: 0.32334
	loss_value: 1.09288
[2025-04-10 21:56:08] nn step 51200, lr: 0.02.
	loss_policy: 1.76477
	accuracy_policy: 0.32426
	loss_value: 1.09026
[2025-04-10 21:56:31] nn step 51300, lr: 0.02.
	loss_policy: 1.7606
	accuracy_policy: 0.32551
	loss_value: 1.09062
[2025-04-10 21:56:54] nn step 51400, lr: 0.02.
	loss_policy: 1.76128
	accuracy_policy: 0.32596
	loss_value: 1.09944
[2025-04-10 21:57:19] nn step 51500, lr: 0.02.
	loss_policy: 1.75708
	accuracy_policy: 0.32561
	loss_value: 1.08623
Optimization_Done 51500
[2025-04-10 22:19:40] [command] train weight_iter_51500.pkl 85 104
[2025-04-10 22:20:03] nn step 51600, lr: 0.02.
	loss_policy: 1.76114
	accuracy_policy: 0.32359
	loss_value: 1.08911
[2025-04-10 22:20:26] nn step 51700, lr: 0.02.
	loss_policy: 1.75958
	accuracy_policy: 0.32525
	loss_value: 1.09165
[2025-04-10 22:20:50] nn step 51800, lr: 0.02.
	loss_policy: 1.75736
	accuracy_policy: 0.32771
	loss_value: 1.08824
[2025-04-10 22:21:13] nn step 51900, lr: 0.02.
	loss_policy: 1.7521
	accuracy_policy: 0.33205
	loss_value: 1.08174
[2025-04-10 22:21:37] nn step 52000, lr: 0.02.
	loss_policy: 1.75967
	accuracy_policy: 0.32517
	loss_value: 1.08291
Optimization_Done 52000
[2025-04-10 22:43:59] [command] train weight_iter_52000.pkl 86 105
[2025-04-10 22:44:22] nn step 52100, lr: 0.02.
	loss_policy: 1.76465
	accuracy_policy: 0.32583
	loss_value: 1.09412
[2025-04-10 22:44:47] nn step 52200, lr: 0.02.
	loss_policy: 1.76203
	accuracy_policy: 0.32668
	loss_value: 1.08889
[2025-04-10 22:45:11] nn step 52300, lr: 0.02.
	loss_policy: 1.75476
	accuracy_policy: 0.33109
	loss_value: 1.08796
[2025-04-10 22:45:34] nn step 52400, lr: 0.02.
	loss_policy: 1.75667
	accuracy_policy: 0.3273
	loss_value: 1.0837
[2025-04-10 22:45:59] nn step 52500, lr: 0.02.
	loss_policy: 1.75299
	accuracy_policy: 0.3301
	loss_value: 1.08204
Optimization_Done 52500
[2025-04-10 23:08:25] [command] train weight_iter_52500.pkl 87 106
[2025-04-10 23:08:48] nn step 52600, lr: 0.02.
	loss_policy: 1.77293
	accuracy_policy: 0.32579
	loss_value: 1.08691
[2025-04-10 23:09:12] nn step 52700, lr: 0.02.
	loss_policy: 1.76567
	accuracy_policy: 0.32771
	loss_value: 1.08459
[2025-04-10 23:09:36] nn step 52800, lr: 0.02.
	loss_policy: 1.76194
	accuracy_policy: 0.32817
	loss_value: 1.08772
[2025-04-10 23:09:59] nn step 52900, lr: 0.02.
	loss_policy: 1.75874
	accuracy_policy: 0.32849
	loss_value: 1.08127
[2025-04-10 23:10:24] nn step 53000, lr: 0.02.
	loss_policy: 1.76454
	accuracy_policy: 0.32694
	loss_value: 1.082
Optimization_Done 53000
[2025-04-10 23:33:14] [command] train weight_iter_53000.pkl 88 107
[2025-04-10 23:33:37] nn step 53100, lr: 0.02.
	loss_policy: 1.76573
	accuracy_policy: 0.32943
	loss_value: 1.09086
[2025-04-10 23:34:01] nn step 53200, lr: 0.02.
	loss_policy: 1.7619
	accuracy_policy: 0.3297
	loss_value: 1.08722
[2025-04-10 23:34:26] nn step 53300, lr: 0.02.
	loss_policy: 1.76172
	accuracy_policy: 0.32672
	loss_value: 1.07704
[2025-04-10 23:34:50] nn step 53400, lr: 0.02.
	loss_policy: 1.76202
	accuracy_policy: 0.32919
	loss_value: 1.08452
[2025-04-10 23:35:14] nn step 53500, lr: 0.02.
	loss_policy: 1.76159
	accuracy_policy: 0.32794
	loss_value: 1.08503
Optimization_Done 53500
[2025-04-10 23:57:07] [command] train weight_iter_53500.pkl 89 108
[2025-04-10 23:57:30] nn step 53600, lr: 0.02.
	loss_policy: 1.76666
	accuracy_policy: 0.32847
	loss_value: 1.08701
[2025-04-10 23:57:54] nn step 53700, lr: 0.02.
	loss_policy: 1.7623
	accuracy_policy: 0.32949
	loss_value: 1.0847
[2025-04-10 23:58:18] nn step 53800, lr: 0.02.
	loss_policy: 1.76201
	accuracy_policy: 0.33044
	loss_value: 1.08299
[2025-04-10 23:58:43] nn step 53900, lr: 0.02.
	loss_policy: 1.75795
	accuracy_policy: 0.33079
	loss_value: 1.07866
[2025-04-10 23:59:07] nn step 54000, lr: 0.02.
	loss_policy: 1.75969
	accuracy_policy: 0.32998
	loss_value: 1.08208
Optimization_Done 54000
[2025-04-11 00:21:54] [command] train weight_iter_54000.pkl 90 109
[2025-04-11 00:22:17] nn step 54100, lr: 0.02.
	loss_policy: 1.76178
	accuracy_policy: 0.33237
	loss_value: 1.0867
[2025-04-11 00:22:41] nn step 54200, lr: 0.02.
	loss_policy: 1.76487
	accuracy_policy: 0.33168
	loss_value: 1.08983
[2025-04-11 00:23:05] nn step 54300, lr: 0.02.
	loss_policy: 1.75709
	accuracy_policy: 0.3318
	loss_value: 1.08372
[2025-04-11 00:23:29] nn step 54400, lr: 0.02.
	loss_policy: 1.75918
	accuracy_policy: 0.3316
	loss_value: 1.0856
[2025-04-11 00:23:52] nn step 54500, lr: 0.02.
	loss_policy: 1.75513
	accuracy_policy: 0.33311
	loss_value: 1.08099
Optimization_Done 54500
[2025-04-11 00:45:56] [command] train weight_iter_54500.pkl 91 110
[2025-04-11 00:46:19] nn step 54600, lr: 0.02.
	loss_policy: 1.76552
	accuracy_policy: 0.33183
	loss_value: 1.08318
[2025-04-11 00:46:44] nn step 54700, lr: 0.02.
	loss_policy: 1.76431
	accuracy_policy: 0.3309
	loss_value: 1.07992
[2025-04-11 00:47:08] nn step 54800, lr: 0.02.
	loss_policy: 1.76759
	accuracy_policy: 0.3304
	loss_value: 1.08594
[2025-04-11 00:47:32] nn step 54900, lr: 0.02.
	loss_policy: 1.7627
	accuracy_policy: 0.33277
	loss_value: 1.0812
[2025-04-11 00:47:56] nn step 55000, lr: 0.02.
	loss_policy: 1.75767
	accuracy_policy: 0.33383
	loss_value: 1.0782
Optimization_Done 55000
[2025-04-11 01:10:48] [command] train weight_iter_55000.pkl 92 111
[2025-04-11 01:11:12] nn step 55100, lr: 0.02.
	loss_policy: 1.76905
	accuracy_policy: 0.33171
	loss_value: 1.08447
[2025-04-11 01:11:36] nn step 55200, lr: 0.02.
	loss_policy: 1.77021
	accuracy_policy: 0.33118
	loss_value: 1.08612
[2025-04-11 01:12:00] nn step 55300, lr: 0.02.
	loss_policy: 1.76679
	accuracy_policy: 0.33188
	loss_value: 1.08353
[2025-04-11 01:12:23] nn step 55400, lr: 0.02.
	loss_policy: 1.76717
	accuracy_policy: 0.32926
	loss_value: 1.08376
[2025-04-11 01:12:47] nn step 55500, lr: 0.02.
	loss_policy: 1.7586
	accuracy_policy: 0.33369
	loss_value: 1.08292
Optimization_Done 55500
[2025-04-11 01:34:56] [command] train weight_iter_55500.pkl 93 112
[2025-04-11 01:35:18] nn step 55600, lr: 0.02.
	loss_policy: 1.76832
	accuracy_policy: 0.33124
	loss_value: 1.08971
[2025-04-11 01:35:41] nn step 55700, lr: 0.02.
	loss_policy: 1.76703
	accuracy_policy: 0.33496
	loss_value: 1.08421
[2025-04-11 01:36:05] nn step 55800, lr: 0.02.
	loss_policy: 1.76131
	accuracy_policy: 0.33444
	loss_value: 1.08696
[2025-04-11 01:36:29] nn step 55900, lr: 0.02.
	loss_policy: 1.7581
	accuracy_policy: 0.33536
	loss_value: 1.08928
[2025-04-11 01:36:53] nn step 56000, lr: 0.02.
	loss_policy: 1.76539
	accuracy_policy: 0.33064
	loss_value: 1.08307
Optimization_Done 56000
[2025-04-11 01:58:56] [command] train weight_iter_56000.pkl 94 113
[2025-04-11 01:59:19] nn step 56100, lr: 0.02.
	loss_policy: 1.76957
	accuracy_policy: 0.3352
	loss_value: 1.08713
[2025-04-11 01:59:43] nn step 56200, lr: 0.02.
	loss_policy: 1.77117
	accuracy_policy: 0.33312
	loss_value: 1.08417
[2025-04-11 02:00:06] nn step 56300, lr: 0.02.
	loss_policy: 1.76531
	accuracy_policy: 0.33411
	loss_value: 1.09175
[2025-04-11 02:00:30] nn step 56400, lr: 0.02.
	loss_policy: 1.76387
	accuracy_policy: 0.33374
	loss_value: 1.08985
[2025-04-11 02:00:54] nn step 56500, lr: 0.02.
	loss_policy: 1.76328
	accuracy_policy: 0.3305
	loss_value: 1.08381
Optimization_Done 56500
[2025-04-11 02:24:23] [command] train weight_iter_56500.pkl 95 114
[2025-04-11 02:24:46] nn step 56600, lr: 0.02.
	loss_policy: 1.77229
	accuracy_policy: 0.33421
	loss_value: 1.09182
[2025-04-11 02:25:09] nn step 56700, lr: 0.02.
	loss_policy: 1.76817
	accuracy_policy: 0.3337
	loss_value: 1.08966
[2025-04-11 02:25:33] nn step 56800, lr: 0.02.
	loss_policy: 1.77016
	accuracy_policy: 0.3364
	loss_value: 1.0889
[2025-04-11 02:25:57] nn step 56900, lr: 0.02.
	loss_policy: 1.75778
	accuracy_policy: 0.33676
	loss_value: 1.08441
[2025-04-11 02:26:21] nn step 57000, lr: 0.02.
	loss_policy: 1.76777
	accuracy_policy: 0.33501
	loss_value: 1.0928
Optimization_Done 57000
[2025-04-11 02:49:42] [command] train weight_iter_57000.pkl 96 115
[2025-04-11 02:50:06] nn step 57100, lr: 0.02.
	loss_policy: 1.77187
	accuracy_policy: 0.33646
	loss_value: 1.09485
[2025-04-11 02:50:29] nn step 57200, lr: 0.02.
	loss_policy: 1.76414
	accuracy_policy: 0.33709
	loss_value: 1.10378
[2025-04-11 02:50:53] nn step 57300, lr: 0.02.
	loss_policy: 1.76495
	accuracy_policy: 0.33646
	loss_value: 1.09398
[2025-04-11 02:51:18] nn step 57400, lr: 0.02.
	loss_policy: 1.76373
	accuracy_policy: 0.33521
	loss_value: 1.09415
[2025-04-11 02:51:42] nn step 57500, lr: 0.02.
	loss_policy: 1.75484
	accuracy_policy: 0.34165
	loss_value: 1.09732
Optimization_Done 57500
[2025-04-11 03:14:38] [command] train weight_iter_57500.pkl 97 116
[2025-04-11 03:15:01] nn step 57600, lr: 0.02.
	loss_policy: 1.76939
	accuracy_policy: 0.33719
	loss_value: 1.09978
[2025-04-11 03:15:25] nn step 57700, lr: 0.02.
	loss_policy: 1.76174
	accuracy_policy: 0.34022
	loss_value: 1.09578
[2025-04-11 03:15:49] nn step 57800, lr: 0.02.
	loss_policy: 1.76026
	accuracy_policy: 0.33807
	loss_value: 1.09205
[2025-04-11 03:16:14] nn step 57900, lr: 0.02.
	loss_policy: 1.76108
	accuracy_policy: 0.33635
	loss_value: 1.09471
[2025-04-11 03:16:37] nn step 58000, lr: 0.02.
	loss_policy: 1.76269
	accuracy_policy: 0.34025
	loss_value: 1.08914
Optimization_Done 58000
[2025-04-11 03:39:06] [command] train weight_iter_58000.pkl 98 117
[2025-04-11 03:39:29] nn step 58100, lr: 0.02.
	loss_policy: 1.76512
	accuracy_policy: 0.3384
	loss_value: 1.10597
[2025-04-11 03:39:53] nn step 58200, lr: 0.02.
	loss_policy: 1.76292
	accuracy_policy: 0.33963
	loss_value: 1.10342
[2025-04-11 03:40:16] nn step 58300, lr: 0.02.
	loss_policy: 1.75917
	accuracy_policy: 0.34122
	loss_value: 1.09965
[2025-04-11 03:40:40] nn step 58400, lr: 0.02.
	loss_policy: 1.75676
	accuracy_policy: 0.34156
	loss_value: 1.09311
[2025-04-11 03:41:04] nn step 58500, lr: 0.02.
	loss_policy: 1.76025
	accuracy_policy: 0.34205
	loss_value: 1.09542
Optimization_Done 58500
[2025-04-11 04:03:46] [command] train weight_iter_58500.pkl 99 118
[2025-04-11 04:04:10] nn step 58600, lr: 0.02.
	loss_policy: 1.7643
	accuracy_policy: 0.3398
	loss_value: 1.09599
[2025-04-11 04:04:34] nn step 58700, lr: 0.02.
	loss_policy: 1.76541
	accuracy_policy: 0.34115
	loss_value: 1.10173
[2025-04-11 04:04:57] nn step 58800, lr: 0.02.
	loss_policy: 1.76258
	accuracy_policy: 0.34106
	loss_value: 1.10319
[2025-04-11 04:05:21] nn step 58900, lr: 0.02.
	loss_policy: 1.75607
	accuracy_policy: 0.34582
	loss_value: 1.09648
[2025-04-11 04:05:45] nn step 59000, lr: 0.02.
	loss_policy: 1.76353
	accuracy_policy: 0.33898
	loss_value: 1.09501
Optimization_Done 59000
[2025-04-11 04:27:16] [command] train weight_iter_59000.pkl 100 119
[2025-04-11 04:27:39] nn step 59100, lr: 0.02.
	loss_policy: 1.76935
	accuracy_policy: 0.34126
	loss_value: 1.10925
[2025-04-11 04:28:01] nn step 59200, lr: 0.02.
	loss_policy: 1.76945
	accuracy_policy: 0.34072
	loss_value: 1.10419
[2025-04-11 04:28:24] nn step 59300, lr: 0.02.
	loss_policy: 1.76692
	accuracy_policy: 0.34283
	loss_value: 1.10464
[2025-04-11 04:28:48] nn step 59400, lr: 0.02.
	loss_policy: 1.76314
	accuracy_policy: 0.34334
	loss_value: 1.09876
[2025-04-11 04:29:10] nn step 59500, lr: 0.02.
	loss_policy: 1.7647
	accuracy_policy: 0.34323
	loss_value: 1.10406
Optimization_Done 59500
[2025-04-11 04:52:08] [command] train weight_iter_59500.pkl 101 120
[2025-04-11 04:52:31] nn step 59600, lr: 0.02.
	loss_policy: 1.76796
	accuracy_policy: 0.34244
	loss_value: 1.11786
[2025-04-11 04:52:55] nn step 59700, lr: 0.02.
	loss_policy: 1.76148
	accuracy_policy: 0.34298
	loss_value: 1.11414
[2025-04-11 04:53:18] nn step 59800, lr: 0.02.
	loss_policy: 1.76409
	accuracy_policy: 0.34064
	loss_value: 1.11257
[2025-04-11 04:53:41] nn step 59900, lr: 0.02.
	loss_policy: 1.76286
	accuracy_policy: 0.34395
	loss_value: 1.10858
[2025-04-11 04:54:05] nn step 60000, lr: 0.02.
	loss_policy: 1.75689
	accuracy_policy: 0.34408
	loss_value: 1.10551
Optimization_Done 60000
[2025-04-11 05:16:52] [command] train weight_iter_60000.pkl 102 121
[2025-04-11 05:17:15] nn step 60100, lr: 0.02.
	loss_policy: 1.76677
	accuracy_policy: 0.34412
	loss_value: 1.11981
[2025-04-11 05:17:40] nn step 60200, lr: 0.02.
	loss_policy: 1.76742
	accuracy_policy: 0.34352
	loss_value: 1.11325
[2025-04-11 05:18:03] nn step 60300, lr: 0.02.
	loss_policy: 1.76508
	accuracy_policy: 0.34204
	loss_value: 1.11737
[2025-04-11 05:18:27] nn step 60400, lr: 0.02.
	loss_policy: 1.7632
	accuracy_policy: 0.3425
	loss_value: 1.10939
[2025-04-11 05:18:50] nn step 60500, lr: 0.02.
	loss_policy: 1.76425
	accuracy_policy: 0.34367
	loss_value: 1.11016
Optimization_Done 60500
[2025-04-11 05:41:41] [command] train weight_iter_60500.pkl 103 122
[2025-04-11 05:42:04] nn step 60600, lr: 0.02.
	loss_policy: 1.78011
	accuracy_policy: 0.34041
	loss_value: 1.13011
[2025-04-11 05:42:27] nn step 60700, lr: 0.02.
	loss_policy: 1.77717
	accuracy_policy: 0.34067
	loss_value: 1.13008
[2025-04-11 05:42:50] nn step 60800, lr: 0.02.
	loss_policy: 1.77398
	accuracy_policy: 0.34039
	loss_value: 1.12852
[2025-04-11 05:43:14] nn step 60900, lr: 0.02.
	loss_policy: 1.76717
	accuracy_policy: 0.343
	loss_value: 1.12322
[2025-04-11 05:43:37] nn step 61000, lr: 0.02.
	loss_policy: 1.76967
	accuracy_policy: 0.34307
	loss_value: 1.12137
Optimization_Done 61000
[2025-04-11 06:05:57] [command] train weight_iter_61000.pkl 104 123
[2025-04-11 06:06:20] nn step 61100, lr: 0.02.
	loss_policy: 1.77535
	accuracy_policy: 0.34062
	loss_value: 1.14079
[2025-04-11 06:06:44] nn step 61200, lr: 0.02.
	loss_policy: 1.7795
	accuracy_policy: 0.34142
	loss_value: 1.13987
[2025-04-11 06:07:08] nn step 61300, lr: 0.02.
	loss_policy: 1.77116
	accuracy_policy: 0.34066
	loss_value: 1.12947
[2025-04-11 06:07:32] nn step 61400, lr: 0.02.
	loss_policy: 1.76519
	accuracy_policy: 0.3443
	loss_value: 1.13153
[2025-04-11 06:07:56] nn step 61500, lr: 0.02.
	loss_policy: 1.77144
	accuracy_policy: 0.34178
	loss_value: 1.12658
Optimization_Done 61500
[2025-04-11 06:31:01] [command] train weight_iter_61500.pkl 105 124
[2025-04-11 06:31:24] nn step 61600, lr: 0.02.
	loss_policy: 1.78593
	accuracy_policy: 0.3397
	loss_value: 1.14636
[2025-04-11 06:31:48] nn step 61700, lr: 0.02.
	loss_policy: 1.77753
	accuracy_policy: 0.34004
	loss_value: 1.14562
[2025-04-11 06:32:12] nn step 61800, lr: 0.02.
	loss_policy: 1.77682
	accuracy_policy: 0.34103
	loss_value: 1.13911
[2025-04-11 06:32:36] nn step 61900, lr: 0.02.
	loss_policy: 1.77259
	accuracy_policy: 0.34241
	loss_value: 1.13738
[2025-04-11 06:33:00] nn step 62000, lr: 0.02.
	loss_policy: 1.77503
	accuracy_policy: 0.34168
	loss_value: 1.13198
Optimization_Done 62000
[2025-04-11 06:55:58] [command] train weight_iter_62000.pkl 106 125
[2025-04-11 06:56:22] nn step 62100, lr: 0.02.
	loss_policy: 1.7872
	accuracy_policy: 0.3399
	loss_value: 1.15705
[2025-04-11 06:56:46] nn step 62200, lr: 0.02.
	loss_policy: 1.78734
	accuracy_policy: 0.33864
	loss_value: 1.15012
[2025-04-11 06:57:09] nn step 62300, lr: 0.02.
	loss_policy: 1.77995
	accuracy_policy: 0.34104
	loss_value: 1.15014
[2025-04-11 06:57:33] nn step 62400, lr: 0.02.
	loss_policy: 1.78244
	accuracy_policy: 0.33984
	loss_value: 1.14643
[2025-04-11 06:57:57] nn step 62500, lr: 0.02.
	loss_policy: 1.77793
	accuracy_policy: 0.34078
	loss_value: 1.14399
Optimization_Done 62500
[2025-04-11 07:20:43] [command] train weight_iter_62500.pkl 107 126
[2025-04-11 07:21:07] nn step 62600, lr: 0.02.
	loss_policy: 1.79546
	accuracy_policy: 0.33555
	loss_value: 1.15686
[2025-04-11 07:21:31] nn step 62700, lr: 0.02.
	loss_policy: 1.79168
	accuracy_policy: 0.33776
	loss_value: 1.15305
[2025-04-11 07:21:55] nn step 62800, lr: 0.02.
	loss_policy: 1.78906
	accuracy_policy: 0.33575
	loss_value: 1.1559
[2025-04-11 07:22:18] nn step 62900, lr: 0.02.
	loss_policy: 1.78391
	accuracy_policy: 0.33839
	loss_value: 1.14944
[2025-04-11 07:22:42] nn step 63000, lr: 0.02.
	loss_policy: 1.78031
	accuracy_policy: 0.34161
	loss_value: 1.14729
Optimization_Done 63000
[2025-04-11 07:45:46] [command] train weight_iter_63000.pkl 108 127
[2025-04-11 07:46:09] nn step 63100, lr: 0.02.
	loss_policy: 1.80085
	accuracy_policy: 0.33414
	loss_value: 1.16656
[2025-04-11 07:46:34] nn step 63200, lr: 0.02.
	loss_policy: 1.79224
	accuracy_policy: 0.33747
	loss_value: 1.1635
[2025-04-11 07:46:57] nn step 63300, lr: 0.02.
	loss_policy: 1.79214
	accuracy_policy: 0.33752
	loss_value: 1.15935
[2025-04-11 07:47:21] nn step 63400, lr: 0.02.
	loss_policy: 1.7899
	accuracy_policy: 0.3362
	loss_value: 1.15614
[2025-04-11 07:47:45] nn step 63500, lr: 0.02.
	loss_policy: 1.78721
	accuracy_policy: 0.33892
	loss_value: 1.1577
Optimization_Done 63500
[2025-04-11 08:10:20] [command] train weight_iter_63500.pkl 109 128
[2025-04-11 08:10:44] nn step 63600, lr: 0.02.
	loss_policy: 1.80602
	accuracy_policy: 0.33371
	loss_value: 1.17613
[2025-04-11 08:11:08] nn step 63700, lr: 0.02.
	loss_policy: 1.79965
	accuracy_policy: 0.33748
	loss_value: 1.1767
[2025-04-11 08:11:31] nn step 63800, lr: 0.02.
	loss_policy: 1.79979
	accuracy_policy: 0.33569
	loss_value: 1.16927
[2025-04-11 08:11:56] nn step 63900, lr: 0.02.
	loss_policy: 1.79652
	accuracy_policy: 0.33672
	loss_value: 1.1705
[2025-04-11 08:12:20] nn step 64000, lr: 0.02.
	loss_policy: 1.80008
	accuracy_policy: 0.3357
	loss_value: 1.17367
Optimization_Done 64000
[2025-04-11 08:35:34] [command] train weight_iter_64000.pkl 110 129
[2025-04-11 08:35:58] nn step 64100, lr: 0.02.
	loss_policy: 1.8129
	accuracy_policy: 0.33235
	loss_value: 1.19354
[2025-04-11 08:36:22] nn step 64200, lr: 0.02.
	loss_policy: 1.81451
	accuracy_policy: 0.33183
	loss_value: 1.19374
[2025-04-11 08:36:46] nn step 64300, lr: 0.02.
	loss_policy: 1.80836
	accuracy_policy: 0.33542
	loss_value: 1.19542
[2025-04-11 08:37:10] nn step 64400, lr: 0.02.
	loss_policy: 1.80334
	accuracy_policy: 0.33367
	loss_value: 1.18709
[2025-04-11 08:37:33] nn step 64500, lr: 0.02.
	loss_policy: 1.80556
	accuracy_policy: 0.33726
	loss_value: 1.18565
Optimization_Done 64500
[2025-04-11 09:00:32] [command] train weight_iter_64500.pkl 111 130
[2025-04-11 09:00:56] nn step 64600, lr: 0.02.
	loss_policy: 1.82235
	accuracy_policy: 0.32955
	loss_value: 1.20021
[2025-04-11 09:01:20] nn step 64700, lr: 0.02.
	loss_policy: 1.81206
	accuracy_policy: 0.33372
	loss_value: 1.19808
[2025-04-11 09:01:44] nn step 64800, lr: 0.02.
	loss_policy: 1.81061
	accuracy_policy: 0.33224
	loss_value: 1.1946
[2025-04-11 09:02:08] nn step 64900, lr: 0.02.
	loss_policy: 1.80839
	accuracy_policy: 0.33348
	loss_value: 1.19756
[2025-04-11 09:02:32] nn step 65000, lr: 0.02.
	loss_policy: 1.81111
	accuracy_policy: 0.33155
	loss_value: 1.19516
Optimization_Done 65000
[2025-04-11 09:25:01] [command] train weight_iter_65000.pkl 112 131
[2025-04-11 09:25:24] nn step 65100, lr: 0.02.
	loss_policy: 1.82641
	accuracy_policy: 0.32944
	loss_value: 1.20901
[2025-04-11 09:25:48] nn step 65200, lr: 0.02.
	loss_policy: 1.81691
	accuracy_policy: 0.33323
	loss_value: 1.20086
[2025-04-11 09:26:12] nn step 65300, lr: 0.02.
	loss_policy: 1.82339
	accuracy_policy: 0.32948
	loss_value: 1.20623
[2025-04-11 09:26:36] nn step 65400, lr: 0.02.
	loss_policy: 1.8143
	accuracy_policy: 0.33216
	loss_value: 1.20298
[2025-04-11 09:27:00] nn step 65500, lr: 0.02.
	loss_policy: 1.81431
	accuracy_policy: 0.33225
	loss_value: 1.19941
Optimization_Done 65500
[2025-04-11 09:51:17] [command] train weight_iter_65500.pkl 113 132
[2025-04-11 09:51:41] nn step 65600, lr: 0.02.
	loss_policy: 1.8287
	accuracy_policy: 0.32829
	loss_value: 1.22268
[2025-04-11 09:52:04] nn step 65700, lr: 0.02.
	loss_policy: 1.83003
	accuracy_policy: 0.32854
	loss_value: 1.22054
[2025-04-11 09:52:28] nn step 65800, lr: 0.02.
	loss_policy: 1.82679
	accuracy_policy: 0.32817
	loss_value: 1.21379
[2025-04-11 09:52:52] nn step 65900, lr: 0.02.
	loss_policy: 1.81517
	accuracy_policy: 0.33229
	loss_value: 1.21165
[2025-04-11 09:53:16] nn step 66000, lr: 0.02.
	loss_policy: 1.82119
	accuracy_policy: 0.32982
	loss_value: 1.21851
Optimization_Done 66000
[2025-04-11 10:15:18] [command] train weight_iter_66000.pkl 114 133
[2025-04-11 10:15:42] nn step 66100, lr: 0.02.
	loss_policy: 1.83741
	accuracy_policy: 0.32707
	loss_value: 1.22458
[2025-04-11 10:16:05] nn step 66200, lr: 0.02.
	loss_policy: 1.83314
	accuracy_policy: 0.32501
	loss_value: 1.22548
[2025-04-11 10:16:29] nn step 66300, lr: 0.02.
	loss_policy: 1.82488
	accuracy_policy: 0.32949
	loss_value: 1.23279
[2025-04-11 10:16:53] nn step 66400, lr: 0.02.
	loss_policy: 1.82734
	accuracy_policy: 0.33067
	loss_value: 1.2258
[2025-04-11 10:17:17] nn step 66500, lr: 0.02.
	loss_policy: 1.82588
	accuracy_policy: 0.32917
	loss_value: 1.21248
Optimization_Done 66500
[2025-04-11 10:41:57] [command] train weight_iter_66500.pkl 115 134
[2025-04-11 10:42:21] nn step 66600, lr: 0.02.
	loss_policy: 1.83334
	accuracy_policy: 0.32567
	loss_value: 1.24536
[2025-04-11 10:42:44] nn step 66700, lr: 0.02.
	loss_policy: 1.83698
	accuracy_policy: 0.32386
	loss_value: 1.23586
[2025-04-11 10:43:09] nn step 66800, lr: 0.02.
	loss_policy: 1.82821
	accuracy_policy: 0.32838
	loss_value: 1.23244
[2025-04-11 10:43:32] nn step 66900, lr: 0.02.
	loss_policy: 1.82844
	accuracy_policy: 0.32933
	loss_value: 1.22878
[2025-04-11 10:43:56] nn step 67000, lr: 0.02.
	loss_policy: 1.82986
	accuracy_policy: 0.32827
	loss_value: 1.225
Optimization_Done 67000
[2025-04-11 11:08:42] [command] train weight_iter_67000.pkl 116 135
[2025-04-11 11:09:05] nn step 67100, lr: 0.02.
	loss_policy: 1.83747
	accuracy_policy: 0.32402
	loss_value: 1.25861
[2025-04-11 11:09:29] nn step 67200, lr: 0.02.
	loss_policy: 1.84247
	accuracy_policy: 0.3254
	loss_value: 1.2577
[2025-04-11 11:09:54] nn step 67300, lr: 0.02.
	loss_policy: 1.84033
	accuracy_policy: 0.3264
	loss_value: 1.24959
[2025-04-11 11:10:17] nn step 67400, lr: 0.02.
	loss_policy: 1.84104
	accuracy_policy: 0.32562
	loss_value: 1.25463
[2025-04-11 11:10:41] nn step 67500, lr: 0.02.
	loss_policy: 1.83331
	accuracy_policy: 0.32687
	loss_value: 1.23932
Optimization_Done 67500
[2025-04-11 11:35:52] [command] train weight_iter_67500.pkl 117 136
[2025-04-11 11:36:16] nn step 67600, lr: 0.02.
	loss_policy: 1.8422
	accuracy_policy: 0.32602
	loss_value: 1.27135
[2025-04-11 11:36:39] nn step 67700, lr: 0.02.
	loss_policy: 1.8453
	accuracy_policy: 0.32394
	loss_value: 1.26264
[2025-04-11 11:37:03] nn step 67800, lr: 0.02.
	loss_policy: 1.83945
	accuracy_policy: 0.3254
	loss_value: 1.26381
[2025-04-11 11:37:28] nn step 67900, lr: 0.02.
	loss_policy: 1.84776
	accuracy_policy: 0.32398
	loss_value: 1.26459
[2025-04-11 11:37:52] nn step 68000, lr: 0.02.
	loss_policy: 1.84075
	accuracy_policy: 0.3246
	loss_value: 1.26131
Optimization_Done 68000
[2025-04-11 12:02:45] [command] train weight_iter_68000.pkl 118 137
[2025-04-11 12:03:07] nn step 68100, lr: 0.02.
	loss_policy: 1.85248
	accuracy_policy: 0.32136
	loss_value: 1.28226
[2025-04-11 12:03:31] nn step 68200, lr: 0.02.
	loss_policy: 1.8456
	accuracy_policy: 0.32334
	loss_value: 1.28387
[2025-04-11 12:03:54] nn step 68300, lr: 0.02.
	loss_policy: 1.84429
	accuracy_policy: 0.32415
	loss_value: 1.27654
[2025-04-11 12:04:18] nn step 68400, lr: 0.02.
	loss_policy: 1.84561
	accuracy_policy: 0.32455
	loss_value: 1.27021
[2025-04-11 12:04:42] nn step 68500, lr: 0.02.
	loss_policy: 1.84259
	accuracy_policy: 0.32451
	loss_value: 1.27408
Optimization_Done 68500
[2025-04-11 12:30:49] [command] train weight_iter_68500.pkl 119 138
[2025-04-11 12:31:13] nn step 68600, lr: 0.02.
	loss_policy: 1.85624
	accuracy_policy: 0.32259
	loss_value: 1.31588
[2025-04-11 12:31:37] nn step 68700, lr: 0.02.
	loss_policy: 1.8492
	accuracy_policy: 0.3255
	loss_value: 1.30833
[2025-04-11 12:32:00] nn step 68800, lr: 0.02.
	loss_policy: 1.8518
	accuracy_policy: 0.32239
	loss_value: 1.30443
[2025-04-11 12:32:25] nn step 68900, lr: 0.02.
	loss_policy: 1.84226
	accuracy_policy: 0.32546
	loss_value: 1.29532
[2025-04-11 12:32:49] nn step 69000, lr: 0.02.
	loss_policy: 1.84668
	accuracy_policy: 0.32555
	loss_value: 1.29852
Optimization_Done 69000
[2025-04-11 12:58:48] [command] train weight_iter_69000.pkl 120 139
[2025-04-11 12:59:12] nn step 69100, lr: 0.02.
	loss_policy: 1.85491
	accuracy_policy: 0.32302
	loss_value: 1.32405
[2025-04-11 12:59:36] nn step 69200, lr: 0.02.
	loss_policy: 1.8544
	accuracy_policy: 0.32367
	loss_value: 1.32852
[2025-04-11 12:59:59] nn step 69300, lr: 0.02.
	loss_policy: 1.84713
	accuracy_policy: 0.3255
	loss_value: 1.32133
[2025-04-11 13:00:24] nn step 69400, lr: 0.02.
	loss_policy: 1.84777
	accuracy_policy: 0.32516
	loss_value: 1.30992
[2025-04-11 13:00:47] nn step 69500, lr: 0.02.
	loss_policy: 1.85221
	accuracy_policy: 0.32228
	loss_value: 1.31419
Optimization_Done 69500
[2025-04-11 13:27:58] [command] train weight_iter_69500.pkl 121 140
[2025-04-11 13:28:21] nn step 69600, lr: 0.02.
	loss_policy: 1.85721
	accuracy_policy: 0.32485
	loss_value: 1.34095
[2025-04-11 13:28:45] nn step 69700, lr: 0.02.
	loss_policy: 1.85898
	accuracy_policy: 0.32433
	loss_value: 1.33453
[2025-04-11 13:29:10] nn step 69800, lr: 0.02.
	loss_policy: 1.84819
	accuracy_policy: 0.32593
	loss_value: 1.33723
[2025-04-11 13:29:34] nn step 69900, lr: 0.02.
	loss_policy: 1.84878
	accuracy_policy: 0.32487
	loss_value: 1.33422
[2025-04-11 13:29:58] nn step 70000, lr: 0.02.
	loss_policy: 1.84748
	accuracy_policy: 0.32507
	loss_value: 1.3347
Optimization_Done 70000
[2025-04-11 13:57:38] [command] train weight_iter_70000.pkl 122 141
[2025-04-11 13:58:02] nn step 70100, lr: 0.02.
	loss_policy: 1.84612
	accuracy_policy: 0.32826
	loss_value: 1.36306
[2025-04-11 13:58:26] nn step 70200, lr: 0.02.
	loss_policy: 1.84589
	accuracy_policy: 0.32859
	loss_value: 1.36245
[2025-04-11 13:58:50] nn step 70300, lr: 0.02.
	loss_policy: 1.84593
	accuracy_policy: 0.32789
	loss_value: 1.36394
[2025-04-11 13:59:13] nn step 70400, lr: 0.02.
	loss_policy: 1.84644
	accuracy_policy: 0.32681
	loss_value: 1.3603
[2025-04-11 13:59:37] nn step 70500, lr: 0.02.
	loss_policy: 1.84525
	accuracy_policy: 0.32703
	loss_value: 1.35158
Optimization_Done 70500
[2025-04-11 14:26:34] [command] train weight_iter_70500.pkl 123 142
[2025-04-11 14:26:58] nn step 70600, lr: 0.02.
	loss_policy: 1.85018
	accuracy_policy: 0.32536
	loss_value: 1.3923
[2025-04-11 14:27:22] nn step 70700, lr: 0.02.
	loss_policy: 1.84218
	accuracy_policy: 0.32885
	loss_value: 1.38726
[2025-04-11 14:27:46] nn step 70800, lr: 0.02.
	loss_policy: 1.84199
	accuracy_policy: 0.33114
	loss_value: 1.39204
[2025-04-11 14:28:11] nn step 70900, lr: 0.02.
	loss_policy: 1.83912
	accuracy_policy: 0.33016
	loss_value: 1.37764
[2025-04-11 14:28:35] nn step 71000, lr: 0.02.
	loss_policy: 1.84052
	accuracy_policy: 0.32798
	loss_value: 1.37786
Optimization_Done 71000
[2025-04-11 14:56:59] [command] train weight_iter_71000.pkl 124 143
[2025-04-11 14:57:22] nn step 71100, lr: 0.02.
	loss_policy: 1.84388
	accuracy_policy: 0.32827
	loss_value: 1.4213
[2025-04-11 14:57:45] nn step 71200, lr: 0.02.
	loss_policy: 1.84064
	accuracy_policy: 0.33039
	loss_value: 1.41443
[2025-04-11 14:58:08] nn step 71300, lr: 0.02.
	loss_policy: 1.83941
	accuracy_policy: 0.33044
	loss_value: 1.41693
[2025-04-11 14:58:32] nn step 71400, lr: 0.02.
	loss_policy: 1.83383
	accuracy_policy: 0.33527
	loss_value: 1.40965
[2025-04-11 14:58:54] nn step 71500, lr: 0.02.
	loss_policy: 1.83372
	accuracy_policy: 0.33156
	loss_value: 1.41054
Optimization_Done 71500
[2025-04-11 15:27:46] [command] train weight_iter_71500.pkl 125 144
[2025-04-11 15:28:09] nn step 71600, lr: 0.02.
	loss_policy: 1.83338
	accuracy_policy: 0.33443
	loss_value: 1.43848
[2025-04-11 15:28:33] nn step 71700, lr: 0.02.
	loss_policy: 1.83235
	accuracy_policy: 0.33433
	loss_value: 1.43916
[2025-04-11 15:28:57] nn step 71800, lr: 0.02.
	loss_policy: 1.82985
	accuracy_policy: 0.33572
	loss_value: 1.4324
[2025-04-11 15:29:22] nn step 71900, lr: 0.02.
	loss_policy: 1.83156
	accuracy_policy: 0.33684
	loss_value: 1.42968
[2025-04-11 15:29:46] nn step 72000, lr: 0.02.
	loss_policy: 1.82219
	accuracy_policy: 0.33744
	loss_value: 1.42832
Optimization_Done 72000
[2025-04-11 15:59:10] [command] train weight_iter_72000.pkl 126 145
[2025-04-11 15:59:33] nn step 72100, lr: 0.02.
	loss_policy: 1.82676
	accuracy_policy: 0.33925
	loss_value: 1.46467
[2025-04-11 15:59:57] nn step 72200, lr: 0.02.
	loss_policy: 1.82536
	accuracy_policy: 0.33862
	loss_value: 1.47026
[2025-04-11 16:00:20] nn step 72300, lr: 0.02.
	loss_policy: 1.82177
	accuracy_policy: 0.33979
	loss_value: 1.46287
[2025-04-11 16:00:44] nn step 72400, lr: 0.02.
	loss_policy: 1.82
	accuracy_policy: 0.34112
	loss_value: 1.4609
[2025-04-11 16:01:07] nn step 72500, lr: 0.02.
	loss_policy: 1.81596
	accuracy_policy: 0.34222
	loss_value: 1.45219
Optimization_Done 72500
[2025-04-11 16:30:03] [command] train weight_iter_72500.pkl 127 146
[2025-04-11 16:30:27] nn step 72600, lr: 0.02.
	loss_policy: 1.82286
	accuracy_policy: 0.3403
	loss_value: 1.50219
[2025-04-11 16:30:50] nn step 72700, lr: 0.02.
	loss_policy: 1.82061
	accuracy_policy: 0.34049
	loss_value: 1.50772
[2025-04-11 16:31:14] nn step 72800, lr: 0.02.
	loss_policy: 1.81055
	accuracy_policy: 0.34342
	loss_value: 1.49685
[2025-04-11 16:31:37] nn step 72900, lr: 0.02.
	loss_policy: 1.8156
	accuracy_policy: 0.3423
	loss_value: 1.49833
[2025-04-11 16:32:02] nn step 73000, lr: 0.02.
	loss_policy: 1.81546
	accuracy_policy: 0.34451
	loss_value: 1.49374
Optimization_Done 73000
[2025-04-11 17:01:35] [command] train weight_iter_73000.pkl 128 147
[2025-04-11 17:01:58] nn step 73100, lr: 0.02.
	loss_policy: 1.80942
	accuracy_policy: 0.34554
	loss_value: 1.54324
[2025-04-11 17:02:22] nn step 73200, lr: 0.02.
	loss_policy: 1.8079
	accuracy_policy: 0.34608
	loss_value: 1.53453
[2025-04-11 17:02:46] nn step 73300, lr: 0.02.
	loss_policy: 1.80532
	accuracy_policy: 0.34845
	loss_value: 1.52966
[2025-04-11 17:03:10] nn step 73400, lr: 0.02.
	loss_policy: 1.79964
	accuracy_policy: 0.35038
	loss_value: 1.53053
[2025-04-11 17:03:34] nn step 73500, lr: 0.02.
	loss_policy: 1.80418
	accuracy_policy: 0.34785
	loss_value: 1.5245
Optimization_Done 73500
[2025-04-11 17:32:33] [command] train weight_iter_73500.pkl 129 148
[2025-04-11 17:32:56] nn step 73600, lr: 0.02.
	loss_policy: 1.79916
	accuracy_policy: 0.34998
	loss_value: 1.56824
[2025-04-11 17:33:20] nn step 73700, lr: 0.02.
	loss_policy: 1.79954
	accuracy_policy: 0.35046
	loss_value: 1.56125
[2025-04-11 17:33:42] nn step 73800, lr: 0.02.
	loss_policy: 1.79719
	accuracy_policy: 0.34933
	loss_value: 1.55373
[2025-04-11 17:34:05] nn step 73900, lr: 0.02.
	loss_policy: 1.79283
	accuracy_policy: 0.35266
	loss_value: 1.55625
[2025-04-11 17:34:29] nn step 74000, lr: 0.02.
	loss_policy: 1.78979
	accuracy_policy: 0.35411
	loss_value: 1.5528
Optimization_Done 74000
[2025-04-11 18:03:51] [command] train weight_iter_74000.pkl 130 149
[2025-04-11 18:04:14] nn step 74100, lr: 0.02.
	loss_policy: 1.79666
	accuracy_policy: 0.35445
	loss_value: 1.59287
[2025-04-11 18:04:38] nn step 74200, lr: 0.02.
	loss_policy: 1.78928
	accuracy_policy: 0.35609
	loss_value: 1.58784
[2025-04-11 18:05:02] nn step 74300, lr: 0.02.
	loss_policy: 1.79126
	accuracy_policy: 0.35312
	loss_value: 1.58036
[2025-04-11 18:05:26] nn step 74400, lr: 0.02.
	loss_policy: 1.78402
	accuracy_policy: 0.35608
	loss_value: 1.58469
[2025-04-11 18:05:50] nn step 74500, lr: 0.02.
	loss_policy: 1.79033
	accuracy_policy: 0.35425
	loss_value: 1.58154
Optimization_Done 74500
[2025-04-11 18:34:32] [command] train weight_iter_74500.pkl 131 150
[2025-04-11 18:34:56] nn step 74600, lr: 0.02.
	loss_policy: 1.79443
	accuracy_policy: 0.35376
	loss_value: 1.63244
[2025-04-11 18:35:20] nn step 74700, lr: 0.02.
	loss_policy: 1.7951
	accuracy_policy: 0.35315
	loss_value: 1.6266
[2025-04-11 18:35:44] nn step 74800, lr: 0.02.
	loss_policy: 1.79631
	accuracy_policy: 0.35361
	loss_value: 1.62183
[2025-04-11 18:36:08] nn step 74900, lr: 0.02.
	loss_policy: 1.78989
	accuracy_policy: 0.35422
	loss_value: 1.62016
[2025-04-11 18:36:32] nn step 75000, lr: 0.02.
	loss_policy: 1.78551
	accuracy_policy: 0.35729
	loss_value: 1.61358
Optimization_Done 75000
[2025-04-11 19:05:17] [command] train weight_iter_75000.pkl 132 151
[2025-04-11 19:05:39] nn step 75100, lr: 0.02.
	loss_policy: 1.80727
	accuracy_policy: 0.35037
	loss_value: 1.66649
[2025-04-11 19:06:02] nn step 75200, lr: 0.02.
	loss_policy: 1.80305
	accuracy_policy: 0.3508
	loss_value: 1.65993
[2025-04-11 19:06:26] nn step 75300, lr: 0.02.
	loss_policy: 1.80056
	accuracy_policy: 0.35416
	loss_value: 1.65572
[2025-04-11 19:06:49] nn step 75400, lr: 0.02.
	loss_policy: 1.79294
	accuracy_policy: 0.35605
	loss_value: 1.64807
[2025-04-11 19:07:11] nn step 75500, lr: 0.02.
	loss_policy: 1.79302
	accuracy_policy: 0.355
	loss_value: 1.6478
Optimization_Done 75500
[2025-04-11 19:37:25] [command] train weight_iter_75500.pkl 133 152
[2025-04-11 19:37:49] nn step 75600, lr: 0.02.
	loss_policy: 1.80253
	accuracy_policy: 0.35466
	loss_value: 1.69632
[2025-04-11 19:38:12] nn step 75700, lr: 0.02.
	loss_policy: 1.79617
	accuracy_policy: 0.35476
	loss_value: 1.69076
[2025-04-11 19:38:36] nn step 75800, lr: 0.02.
	loss_policy: 1.80124
	accuracy_policy: 0.35313
	loss_value: 1.69225
[2025-04-11 19:38:59] nn step 75900, lr: 0.02.
	loss_policy: 1.79341
	accuracy_policy: 0.35607
	loss_value: 1.68778
[2025-04-11 19:39:23] nn step 76000, lr: 0.02.
	loss_policy: 1.79319
	accuracy_policy: 0.35676
	loss_value: 1.6882
Optimization_Done 76000
[2025-04-11 20:09:09] [command] train weight_iter_76000.pkl 134 153
[2025-04-11 20:09:33] nn step 76100, lr: 0.02.
	loss_policy: 1.80599
	accuracy_policy: 0.35182
	loss_value: 1.73116
[2025-04-11 20:09:56] nn step 76200, lr: 0.02.
	loss_policy: 1.79901
	accuracy_policy: 0.35418
	loss_value: 1.72388
[2025-04-11 20:10:20] nn step 76300, lr: 0.02.
	loss_policy: 1.80513
	accuracy_policy: 0.35306
	loss_value: 1.72323
[2025-04-11 20:10:43] nn step 76400, lr: 0.02.
	loss_policy: 1.79791
	accuracy_policy: 0.35541
	loss_value: 1.72587
[2025-04-11 20:11:07] nn step 76500, lr: 0.02.
	loss_policy: 1.79743
	accuracy_policy: 0.35454
	loss_value: 1.71459
Optimization_Done 76500
[2025-04-11 20:41:54] [command] train weight_iter_76500.pkl 135 154
[2025-04-11 20:42:18] nn step 76600, lr: 0.02.
	loss_policy: 1.8098
	accuracy_policy: 0.35239
	loss_value: 1.76802
[2025-04-11 20:42:42] nn step 76700, lr: 0.02.
	loss_policy: 1.80283
	accuracy_policy: 0.35471
	loss_value: 1.76177
[2025-04-11 20:43:06] nn step 76800, lr: 0.02.
	loss_policy: 1.80831
	accuracy_policy: 0.35076
	loss_value: 1.75393
[2025-04-11 20:43:30] nn step 76900, lr: 0.02.
	loss_policy: 1.80267
	accuracy_policy: 0.35385
	loss_value: 1.7544
[2025-04-11 20:43:54] nn step 77000, lr: 0.02.
	loss_policy: 1.79925
	accuracy_policy: 0.35358
	loss_value: 1.74474
Optimization_Done 77000
[2025-04-11 21:14:11] [command] train weight_iter_77000.pkl 136 155
[2025-04-11 21:14:35] nn step 77100, lr: 0.02.
	loss_policy: 1.81198
	accuracy_policy: 0.35234
	loss_value: 1.8016
[2025-04-11 21:14:59] nn step 77200, lr: 0.02.
	loss_policy: 1.8051
	accuracy_policy: 0.35131
	loss_value: 1.78886
[2025-04-11 21:15:23] nn step 77300, lr: 0.02.
	loss_policy: 1.80639
	accuracy_policy: 0.35095
	loss_value: 1.7847
[2025-04-11 21:15:47] nn step 77400, lr: 0.02.
	loss_policy: 1.79504
	accuracy_policy: 0.35552
	loss_value: 1.78342
[2025-04-11 21:16:11] nn step 77500, lr: 0.02.
	loss_policy: 1.79533
	accuracy_policy: 0.35747
	loss_value: 1.77742
Optimization_Done 77500
[2025-04-11 21:46:11] [command] train weight_iter_77500.pkl 137 156
[2025-04-11 21:46:35] nn step 77600, lr: 0.02.
	loss_policy: 1.80846
	accuracy_policy: 0.35271
	loss_value: 1.82683
[2025-04-11 21:46:59] nn step 77700, lr: 0.02.
	loss_policy: 1.808
	accuracy_policy: 0.35443
	loss_value: 1.83058
[2025-04-11 21:47:23] nn step 77800, lr: 0.02.
	loss_policy: 1.80024
	accuracy_policy: 0.35895
	loss_value: 1.82024
[2025-04-11 21:47:47] nn step 77900, lr: 0.02.
	loss_policy: 1.80208
	accuracy_policy: 0.3546
	loss_value: 1.81932
[2025-04-11 21:48:11] nn step 78000, lr: 0.02.
	loss_policy: 1.80116
	accuracy_policy: 0.35379
	loss_value: 1.8104
Optimization_Done 78000
[2025-04-11 22:17:37] [command] train weight_iter_78000.pkl 138 157
[2025-04-11 22:18:01] nn step 78100, lr: 0.02.
	loss_policy: 1.80842
	accuracy_policy: 0.35349
	loss_value: 1.84808
[2025-04-11 22:18:24] nn step 78200, lr: 0.02.
	loss_policy: 1.80334
	accuracy_policy: 0.3559
	loss_value: 1.84583
[2025-04-11 22:18:48] nn step 78300, lr: 0.02.
	loss_policy: 1.80083
	accuracy_policy: 0.35896
	loss_value: 1.83799
[2025-04-11 22:19:12] nn step 78400, lr: 0.02.
	loss_policy: 1.79954
	accuracy_policy: 0.35634
	loss_value: 1.83501
[2025-04-11 22:19:35] nn step 78500, lr: 0.02.
	loss_policy: 1.80043
	accuracy_policy: 0.35804
	loss_value: 1.83683
Optimization_Done 78500
[2025-04-11 22:49:50] [command] train weight_iter_78500.pkl 139 158
[2025-04-11 22:50:14] nn step 78600, lr: 0.02.
	loss_policy: 1.80835
	accuracy_policy: 0.35503
	loss_value: 1.88017
[2025-04-11 22:50:38] nn step 78700, lr: 0.02.
	loss_policy: 1.8043
	accuracy_policy: 0.35655
	loss_value: 1.87168
[2025-04-11 22:51:02] nn step 78800, lr: 0.02.
	loss_policy: 1.7951
	accuracy_policy: 0.35654
	loss_value: 1.8606
[2025-04-11 22:51:26] nn step 78900, lr: 0.02.
	loss_policy: 1.79372
	accuracy_policy: 0.35956
	loss_value: 1.86502
[2025-04-11 22:51:50] nn step 79000, lr: 0.02.
	loss_policy: 1.79727
	accuracy_policy: 0.35672
	loss_value: 1.85996
Optimization_Done 79000
[2025-04-11 23:23:36] [command] train weight_iter_79000.pkl 140 159
[2025-04-11 23:24:00] nn step 79100, lr: 0.02.
	loss_policy: 1.801
	accuracy_policy: 0.3572
	loss_value: 1.90904
[2025-04-11 23:24:23] nn step 79200, lr: 0.02.
	loss_policy: 1.79252
	accuracy_policy: 0.35892
	loss_value: 1.89822
[2025-04-11 23:24:47] nn step 79300, lr: 0.02.
	loss_policy: 1.79848
	accuracy_policy: 0.35976
	loss_value: 1.88996
[2025-04-11 23:25:11] nn step 79400, lr: 0.02.
	loss_policy: 1.79144
	accuracy_policy: 0.36188
	loss_value: 1.89305
[2025-04-11 23:25:35] nn step 79500, lr: 0.02.
	loss_policy: 1.79498
	accuracy_policy: 0.36033
	loss_value: 1.89402
Optimization_Done 79500
[2025-04-11 23:56:22] [command] train weight_iter_79500.pkl 141 160
[2025-04-11 23:56:46] nn step 79600, lr: 0.02.
	loss_policy: 1.80797
	accuracy_policy: 0.35585
	loss_value: 1.93106
[2025-04-11 23:57:10] nn step 79700, lr: 0.02.
	loss_policy: 1.79645
	accuracy_policy: 0.35893
	loss_value: 1.92365
[2025-04-11 23:57:34] nn step 79800, lr: 0.02.
	loss_policy: 1.8
	accuracy_policy: 0.35879
	loss_value: 1.91074
[2025-04-11 23:57:57] nn step 79900, lr: 0.02.
	loss_policy: 1.79742
	accuracy_policy: 0.35862
	loss_value: 1.91364
[2025-04-11 23:58:21] nn step 80000, lr: 0.02.
	loss_policy: 1.79224
	accuracy_policy: 0.3587
	loss_value: 1.90278
Optimization_Done 80000
[2025-04-12 00:29:11] [command] train weight_iter_80000.pkl 142 161
[2025-04-12 00:29:35] nn step 80100, lr: 0.02.
	loss_policy: 1.80646
	accuracy_policy: 0.35438
	loss_value: 1.94666
[2025-04-12 00:29:59] nn step 80200, lr: 0.02.
	loss_policy: 1.80313
	accuracy_policy: 0.35724
	loss_value: 1.93732
[2025-04-12 00:30:23] nn step 80300, lr: 0.02.
	loss_policy: 1.7988
	accuracy_policy: 0.3559
	loss_value: 1.93819
[2025-04-12 00:30:48] nn step 80400, lr: 0.02.
	loss_policy: 1.80126
	accuracy_policy: 0.35807
	loss_value: 1.92964
[2025-04-12 00:31:12] nn step 80500, lr: 0.02.
	loss_policy: 1.79589
	accuracy_policy: 0.35812
	loss_value: 1.93226
Optimization_Done 80500
[2025-04-12 01:01:41] [command] train weight_iter_80500.pkl 143 162
[2025-04-12 01:02:04] nn step 80600, lr: 0.02.
	loss_policy: 1.80963
	accuracy_policy: 0.3575
	loss_value: 1.96432
[2025-04-12 01:02:29] nn step 80700, lr: 0.02.
	loss_policy: 1.80482
	accuracy_policy: 0.35532
	loss_value: 1.95867
[2025-04-12 01:02:53] nn step 80800, lr: 0.02.
	loss_policy: 1.80336
	accuracy_policy: 0.35702
	loss_value: 1.95536
[2025-04-12 01:03:17] nn step 80900, lr: 0.02.
	loss_policy: 1.7931
	accuracy_policy: 0.35893
	loss_value: 1.9531
[2025-04-12 01:03:41] nn step 81000, lr: 0.02.
	loss_policy: 1.79694
	accuracy_policy: 0.35969
	loss_value: 1.95685
Optimization_Done 81000
[2025-04-12 01:34:36] [command] train weight_iter_81000.pkl 144 163
[2025-04-12 01:34:59] nn step 81100, lr: 0.02.
	loss_policy: 1.81717
	accuracy_policy: 0.35273
	loss_value: 1.9792
[2025-04-12 01:35:22] nn step 81200, lr: 0.02.
	loss_policy: 1.81766
	accuracy_policy: 0.35376
	loss_value: 1.9782
[2025-04-12 01:35:46] nn step 81300, lr: 0.02.
	loss_policy: 1.80855
	accuracy_policy: 0.35544
	loss_value: 1.97739
[2025-04-12 01:36:08] nn step 81400, lr: 0.02.
	loss_policy: 1.8117
	accuracy_policy: 0.35521
	loss_value: 1.96412
[2025-04-12 01:36:31] nn step 81500, lr: 0.02.
	loss_policy: 1.80251
	accuracy_policy: 0.35861
	loss_value: 1.96821
Optimization_Done 81500
[2025-04-12 02:07:46] [command] train weight_iter_81500.pkl 145 164
[2025-04-12 02:08:10] nn step 81600, lr: 0.02.
	loss_policy: 1.82168
	accuracy_policy: 0.35282
	loss_value: 1.99808
[2025-04-12 02:08:34] nn step 81700, lr: 0.02.
	loss_policy: 1.81695
	accuracy_policy: 0.35427
	loss_value: 1.99857
[2025-04-12 02:08:58] nn step 81800, lr: 0.02.
	loss_policy: 1.81177
	accuracy_policy: 0.35409
	loss_value: 1.98461
[2025-04-12 02:09:23] nn step 81900, lr: 0.02.
	loss_policy: 1.80972
	accuracy_policy: 0.35419
	loss_value: 1.97766
[2025-04-12 02:09:47] nn step 82000, lr: 0.02.
	loss_policy: 1.80092
	accuracy_policy: 0.35866
	loss_value: 1.97919
Optimization_Done 82000
[2025-04-12 02:40:02] [command] train weight_iter_82000.pkl 146 165
[2025-04-12 02:40:26] nn step 82100, lr: 0.02.
	loss_policy: 1.82801
	accuracy_policy: 0.3506
	loss_value: 2.01104
[2025-04-12 02:40:49] nn step 82200, lr: 0.02.
	loss_policy: 1.81924
	accuracy_policy: 0.35242
	loss_value: 2.00478
[2025-04-12 02:41:13] nn step 82300, lr: 0.02.
	loss_policy: 1.81858
	accuracy_policy: 0.35167
	loss_value: 1.99938
[2025-04-12 02:41:37] nn step 82400, lr: 0.02.
	loss_policy: 1.81677
	accuracy_policy: 0.3547
	loss_value: 2.00168
[2025-04-12 02:42:00] nn step 82500, lr: 0.02.
	loss_policy: 1.8104
	accuracy_policy: 0.35662
	loss_value: 2.00188
Optimization_Done 82500
[2025-04-12 03:13:34] [command] train weight_iter_82500.pkl 147 166
[2025-04-12 03:13:58] nn step 82600, lr: 0.02.
	loss_policy: 1.82432
	accuracy_policy: 0.35133
	loss_value: 2.0241
[2025-04-12 03:14:22] nn step 82700, lr: 0.02.
	loss_policy: 1.82669
	accuracy_policy: 0.35146
	loss_value: 2.01912
[2025-04-12 03:14:45] nn step 82800, lr: 0.02.
	loss_policy: 1.81955
	accuracy_policy: 0.35143
	loss_value: 2.01146
[2025-04-12 03:15:09] nn step 82900, lr: 0.02.
	loss_policy: 1.82283
	accuracy_policy: 0.35207
	loss_value: 1.99916
[2025-04-12 03:15:33] nn step 83000, lr: 0.02.
	loss_policy: 1.81238
	accuracy_policy: 0.35397
	loss_value: 1.99922
Optimization_Done 83000
[2025-04-12 03:46:35] [command] train weight_iter_83000.pkl 148 167
[2025-04-12 03:46:57] nn step 83100, lr: 0.02.
	loss_policy: 1.82825
	accuracy_policy: 0.35087
	loss_value: 2.0295
[2025-04-12 03:47:21] nn step 83200, lr: 0.02.
	loss_policy: 1.8241
	accuracy_policy: 0.35167
	loss_value: 2.03815
[2025-04-12 03:47:44] nn step 83300, lr: 0.02.
	loss_policy: 1.83245
	accuracy_policy: 0.3506
	loss_value: 2.02504
[2025-04-12 03:48:07] nn step 83400, lr: 0.02.
	loss_policy: 1.8228
	accuracy_policy: 0.35166
	loss_value: 2.01619
[2025-04-12 03:48:32] nn step 83500, lr: 0.02.
	loss_policy: 1.81954
	accuracy_policy: 0.35538
	loss_value: 2.0232
Optimization_Done 83500
[2025-04-12 04:19:55] [command] train weight_iter_83500.pkl 149 168
[2025-04-12 04:20:19] nn step 83600, lr: 0.02.
	loss_policy: 1.82637
	accuracy_policy: 0.35143
	loss_value: 2.04732
[2025-04-12 04:20:43] nn step 83700, lr: 0.02.
	loss_policy: 1.82224
	accuracy_policy: 0.35411
	loss_value: 2.04694
[2025-04-12 04:21:07] nn step 83800, lr: 0.02.
	loss_policy: 1.82639
	accuracy_policy: 0.3554
	loss_value: 2.03577
[2025-04-12 04:21:31] nn step 83900, lr: 0.02.
	loss_policy: 1.82689
	accuracy_policy: 0.35306
	loss_value: 2.03555
[2025-04-12 04:21:55] nn step 84000, lr: 0.02.
	loss_policy: 1.81279
	accuracy_policy: 0.35407
	loss_value: 2.02672
Optimization_Done 84000
[2025-04-12 04:53:09] [command] train weight_iter_84000.pkl 150 169
[2025-04-12 04:53:33] nn step 84100, lr: 0.02.
	loss_policy: 1.8273
	accuracy_policy: 0.35276
	loss_value: 2.05853
[2025-04-12 04:53:56] nn step 84200, lr: 0.02.
	loss_policy: 1.82157
	accuracy_policy: 0.35314
	loss_value: 2.05568
[2025-04-12 04:54:20] nn step 84300, lr: 0.02.
	loss_policy: 1.82059
	accuracy_policy: 0.35446
	loss_value: 2.05433
[2025-04-12 04:54:45] nn step 84400, lr: 0.02.
	loss_policy: 1.81514
	accuracy_policy: 0.35624
	loss_value: 2.04134
[2025-04-12 04:55:09] nn step 84500, lr: 0.02.
	loss_policy: 1.81418
	accuracy_policy: 0.3587
	loss_value: 2.04118
Optimization_Done 84500
[2025-04-12 05:26:42] [command] train weight_iter_84500.pkl 151 170
[2025-04-12 05:27:05] nn step 84600, lr: 0.02.
	loss_policy: 1.82504
	accuracy_policy: 0.35397
	loss_value: 2.07274
[2025-04-12 05:27:29] nn step 84700, lr: 0.02.
	loss_policy: 1.822
	accuracy_policy: 0.35417
	loss_value: 2.06954
[2025-04-12 05:27:53] nn step 84800, lr: 0.02.
	loss_policy: 1.81314
	accuracy_policy: 0.35713
	loss_value: 2.06428
[2025-04-12 05:28:18] nn step 84900, lr: 0.02.
	loss_policy: 1.81734
	accuracy_policy: 0.35554
	loss_value: 2.05709
[2025-04-12 05:28:42] nn step 85000, lr: 0.02.
	loss_policy: 1.81479
	accuracy_policy: 0.35562
	loss_value: 2.06034
Optimization_Done 85000
[2025-04-12 06:00:27] [command] train weight_iter_85000.pkl 152 171
[2025-04-12 06:00:50] nn step 85100, lr: 0.02.
	loss_policy: 1.82401
	accuracy_policy: 0.35584
	loss_value: 2.08293
[2025-04-12 06:01:14] nn step 85200, lr: 0.02.
	loss_policy: 1.81776
	accuracy_policy: 0.3573
	loss_value: 2.07338
[2025-04-12 06:01:37] nn step 85300, lr: 0.02.
	loss_policy: 1.81157
	accuracy_policy: 0.35923
	loss_value: 2.07347
[2025-04-12 06:02:01] nn step 85400, lr: 0.02.
	loss_policy: 1.81551
	accuracy_policy: 0.35805
	loss_value: 2.0659
[2025-04-12 06:02:24] nn step 85500, lr: 0.02.
	loss_policy: 1.81355
	accuracy_policy: 0.35623
	loss_value: 2.06483
Optimization_Done 85500
[2025-04-12 06:33:52] [command] train weight_iter_85500.pkl 153 172
[2025-04-12 06:34:16] nn step 85600, lr: 0.02.
	loss_policy: 1.81924
	accuracy_policy: 0.35747
	loss_value: 2.09008
[2025-04-12 06:34:39] nn step 85700, lr: 0.02.
	loss_policy: 1.81141
	accuracy_policy: 0.35931
	loss_value: 2.0771
[2025-04-12 06:35:03] nn step 85800, lr: 0.02.
	loss_policy: 1.811
	accuracy_policy: 0.36055
	loss_value: 2.07836
[2025-04-12 06:35:27] nn step 85900, lr: 0.02.
	loss_policy: 1.81151
	accuracy_policy: 0.35965
	loss_value: 2.07285
[2025-04-12 06:35:50] nn step 86000, lr: 0.02.
	loss_policy: 1.80279
	accuracy_policy: 0.35988
	loss_value: 2.07417
Optimization_Done 86000
[2025-04-12 07:07:31] [command] train weight_iter_86000.pkl 154 173
[2025-04-12 07:07:55] nn step 86100, lr: 0.02.
	loss_policy: 1.80725
	accuracy_policy: 0.36104
	loss_value: 2.0829
[2025-04-12 07:08:18] nn step 86200, lr: 0.02.
	loss_policy: 1.79512
	accuracy_policy: 0.36431
	loss_value: 2.08862
[2025-04-12 07:08:41] nn step 86300, lr: 0.02.
	loss_policy: 1.7989
	accuracy_policy: 0.36544
	loss_value: 2.07797
[2025-04-12 07:09:05] nn step 86400, lr: 0.02.
	loss_policy: 1.79802
	accuracy_policy: 0.36401
	loss_value: 2.08037
[2025-04-12 07:09:29] nn step 86500, lr: 0.02.
	loss_policy: 1.80258
	accuracy_policy: 0.36153
	loss_value: 2.07744
Optimization_Done 86500
[2025-04-12 07:40:44] [command] train weight_iter_86500.pkl 155 174
[2025-04-12 07:41:07] nn step 86600, lr: 0.02.
	loss_policy: 1.8006
	accuracy_policy: 0.36299
	loss_value: 2.08411
[2025-04-12 07:41:30] nn step 86700, lr: 0.02.
	loss_policy: 1.798
	accuracy_policy: 0.36471
	loss_value: 2.0854
[2025-04-12 07:41:53] nn step 86800, lr: 0.02.
	loss_policy: 1.79445
	accuracy_policy: 0.3641
	loss_value: 2.082
[2025-04-12 07:42:17] nn step 86900, lr: 0.02.
	loss_policy: 1.78869
	accuracy_policy: 0.36539
	loss_value: 2.06798
[2025-04-12 07:42:40] nn step 87000, lr: 0.02.
	loss_policy: 1.78753
	accuracy_policy: 0.36606
	loss_value: 2.07466
Optimization_Done 87000
[2025-04-12 08:14:46] [command] train weight_iter_87000.pkl 156 175
[2025-04-12 08:15:10] nn step 87100, lr: 0.02.
	loss_policy: 1.78137
	accuracy_policy: 0.37004
	loss_value: 2.09676
[2025-04-12 08:15:34] nn step 87200, lr: 0.02.
	loss_policy: 1.77995
	accuracy_policy: 0.37108
	loss_value: 2.08883
[2025-04-12 08:15:57] nn step 87300, lr: 0.02.
	loss_policy: 1.77974
	accuracy_policy: 0.36829
	loss_value: 2.08313
[2025-04-12 08:16:21] nn step 87400, lr: 0.02.
	loss_policy: 1.78216
	accuracy_policy: 0.36997
	loss_value: 2.07769
[2025-04-12 08:16:45] nn step 87500, lr: 0.02.
	loss_policy: 1.77408
	accuracy_policy: 0.37181
	loss_value: 2.08312
Optimization_Done 87500
[2025-04-12 08:47:33] [command] train weight_iter_87500.pkl 157 176
[2025-04-12 08:47:57] nn step 87600, lr: 0.02.
	loss_policy: 1.77773
	accuracy_policy: 0.37098
	loss_value: 2.09637
[2025-04-12 08:48:21] nn step 87700, lr: 0.02.
	loss_policy: 1.78003
	accuracy_policy: 0.37177
	loss_value: 2.09795
[2025-04-12 08:48:45] nn step 87800, lr: 0.02.
	loss_policy: 1.76755
	accuracy_policy: 0.37375
	loss_value: 2.09806
[2025-04-12 08:49:09] nn step 87900, lr: 0.02.
	loss_policy: 1.76767
	accuracy_policy: 0.3749
	loss_value: 2.09326
[2025-04-12 08:49:33] nn step 88000, lr: 0.02.
	loss_policy: 1.77018
	accuracy_policy: 0.37434
	loss_value: 2.08406
Optimization_Done 88000
[2025-04-12 09:21:40] [command] train weight_iter_88000.pkl 158 177
[2025-04-12 09:22:04] nn step 88100, lr: 0.02.
	loss_policy: 1.77283
	accuracy_policy: 0.37294
	loss_value: 2.10782
[2025-04-12 09:22:28] nn step 88200, lr: 0.02.
	loss_policy: 1.76314
	accuracy_policy: 0.37655
	loss_value: 2.09586
[2025-04-12 09:22:52] nn step 88300, lr: 0.02.
	loss_policy: 1.76655
	accuracy_policy: 0.37568
	loss_value: 2.09632
[2025-04-12 09:23:15] nn step 88400, lr: 0.02.
	loss_policy: 1.76522
	accuracy_policy: 0.37498
	loss_value: 2.08655
[2025-04-12 09:23:39] nn step 88500, lr: 0.02.
	loss_policy: 1.75761
	accuracy_policy: 0.37743
	loss_value: 2.08723
Optimization_Done 88500
[2025-04-12 09:55:16] [command] train weight_iter_88500.pkl 159 178
[2025-04-12 09:55:39] nn step 88600, lr: 0.02.
	loss_policy: 1.75609
	accuracy_policy: 0.37777
	loss_value: 2.10835
[2025-04-12 09:56:03] nn step 88700, lr: 0.02.
	loss_policy: 1.76498
	accuracy_policy: 0.37623
	loss_value: 2.1072
[2025-04-12 09:56:27] nn step 88800, lr: 0.02.
	loss_policy: 1.7513
	accuracy_policy: 0.38007
	loss_value: 2.09553
[2025-04-12 09:56:51] nn step 88900, lr: 0.02.
	loss_policy: 1.75543
	accuracy_policy: 0.37895
	loss_value: 2.09616
[2025-04-12 09:57:16] nn step 89000, lr: 0.02.
	loss_policy: 1.75207
	accuracy_policy: 0.38034
	loss_value: 2.0944
Optimization_Done 89000
[2025-04-12 10:28:51] [command] train weight_iter_89000.pkl 160 179
[2025-04-12 10:29:15] nn step 89100, lr: 0.02.
	loss_policy: 1.75368
	accuracy_policy: 0.38146
	loss_value: 2.1068
[2025-04-12 10:29:38] nn step 89200, lr: 0.02.
	loss_policy: 1.74982
	accuracy_policy: 0.38141
	loss_value: 2.11397
[2025-04-12 10:30:01] nn step 89300, lr: 0.02.
	loss_policy: 1.74845
	accuracy_policy: 0.37902
	loss_value: 2.09477
[2025-04-12 10:30:25] nn step 89400, lr: 0.02.
	loss_policy: 1.74482
	accuracy_policy: 0.38333
	loss_value: 2.10174
[2025-04-12 10:30:49] nn step 89500, lr: 0.02.
	loss_policy: 1.74187
	accuracy_policy: 0.38228
	loss_value: 2.09829
Optimization_Done 89500
[2025-04-12 11:02:39] [command] train weight_iter_89500.pkl 161 180
[2025-04-12 11:03:03] nn step 89600, lr: 0.02.
	loss_policy: 1.74098
	accuracy_policy: 0.38474
	loss_value: 2.11631
[2025-04-12 11:03:26] nn step 89700, lr: 0.02.
	loss_policy: 1.7354
	accuracy_policy: 0.38681
	loss_value: 2.11725
[2025-04-12 11:03:50] nn step 89800, lr: 0.02.
	loss_policy: 1.73882
	accuracy_policy: 0.38473
	loss_value: 2.11116
[2025-04-12 11:04:15] nn step 89900, lr: 0.02.
	loss_policy: 1.73842
	accuracy_policy: 0.38465
	loss_value: 2.10944
[2025-04-12 11:04:39] nn step 90000, lr: 0.02.
	loss_policy: 1.72777
	accuracy_policy: 0.38745
	loss_value: 2.10986
Optimization_Done 90000
[2025-04-12 11:36:17] [command] train weight_iter_90000.pkl 162 181
[2025-04-12 11:36:40] nn step 90100, lr: 0.02.
	loss_policy: 1.73587
	accuracy_policy: 0.3869
	loss_value: 2.12387
[2025-04-12 11:37:04] nn step 90200, lr: 0.02.
	loss_policy: 1.7363
	accuracy_policy: 0.38497
	loss_value: 2.11948
[2025-04-12 11:37:28] nn step 90300, lr: 0.02.
	loss_policy: 1.72935
	accuracy_policy: 0.38955
	loss_value: 2.11667
[2025-04-12 11:37:52] nn step 90400, lr: 0.02.
	loss_policy: 1.72839
	accuracy_policy: 0.3868
	loss_value: 2.1102
[2025-04-12 11:38:15] nn step 90500, lr: 0.02.
	loss_policy: 1.72677
	accuracy_policy: 0.38891
	loss_value: 2.10573
Optimization_Done 90500
[2025-04-12 12:09:39] [command] train weight_iter_90500.pkl 163 182
[2025-04-12 12:10:01] nn step 90600, lr: 0.02.
	loss_policy: 1.72865
	accuracy_policy: 0.39196
	loss_value: 2.13112
[2025-04-12 12:10:24] nn step 90700, lr: 0.02.
	loss_policy: 1.72375
	accuracy_policy: 0.39059
	loss_value: 2.1214
[2025-04-12 12:10:48] nn step 90800, lr: 0.02.
	loss_policy: 1.72466
	accuracy_policy: 0.39193
	loss_value: 2.12136
[2025-04-12 12:11:11] nn step 90900, lr: 0.02.
	loss_policy: 1.71635
	accuracy_policy: 0.39297
	loss_value: 2.11193
[2025-04-12 12:11:34] nn step 91000, lr: 0.02.
	loss_policy: 1.71552
	accuracy_policy: 0.39316
	loss_value: 2.11541
Optimization_Done 91000
[2025-04-12 12:43:30] [command] train weight_iter_91000.pkl 164 183
[2025-04-12 12:43:54] nn step 91100, lr: 0.02.
	loss_policy: 1.7185
	accuracy_policy: 0.39241
	loss_value: 2.13511
[2025-04-12 12:44:17] nn step 91200, lr: 0.02.
	loss_policy: 1.72144
	accuracy_policy: 0.39146
	loss_value: 2.13148
[2025-04-12 12:44:42] nn step 91300, lr: 0.02.
	loss_policy: 1.71302
	accuracy_policy: 0.39406
	loss_value: 2.1196
[2025-04-12 12:45:06] nn step 91400, lr: 0.02.
	loss_policy: 1.70635
	accuracy_policy: 0.39683
	loss_value: 2.11451
[2025-04-12 12:45:30] nn step 91500, lr: 0.02.
	loss_policy: 1.70927
	accuracy_policy: 0.39551
	loss_value: 2.11115
Optimization_Done 91500
[2025-04-12 13:16:58] [command] train weight_iter_91500.pkl 165 184
[2025-04-12 13:17:22] nn step 91600, lr: 0.02.
	loss_policy: 1.71275
	accuracy_policy: 0.39547
	loss_value: 2.12824
[2025-04-12 13:17:46] nn step 91700, lr: 0.02.
	loss_policy: 1.7028
	accuracy_policy: 0.39656
	loss_value: 2.12408
[2025-04-12 13:18:09] nn step 91800, lr: 0.02.
	loss_policy: 1.70199
	accuracy_policy: 0.39747
	loss_value: 2.12142
[2025-04-12 13:18:34] nn step 91900, lr: 0.02.
	loss_policy: 1.70816
	accuracy_policy: 0.39778
	loss_value: 2.11607
[2025-04-12 13:18:58] nn step 92000, lr: 0.02.
	loss_policy: 1.69559
	accuracy_policy: 0.39997
	loss_value: 2.11549
Optimization_Done 92000
[2025-04-12 13:51:53] [command] train weight_iter_92000.pkl 166 185
[2025-04-12 13:52:17] nn step 92100, lr: 0.02.
	loss_policy: 1.69716
	accuracy_policy: 0.40099
	loss_value: 2.12706
[2025-04-12 13:52:41] nn step 92200, lr: 0.02.
	loss_policy: 1.69477
	accuracy_policy: 0.39985
	loss_value: 2.12773
[2025-04-12 13:53:05] nn step 92300, lr: 0.02.
	loss_policy: 1.69208
	accuracy_policy: 0.402
	loss_value: 2.11419
[2025-04-12 13:53:29] nn step 92400, lr: 0.02.
	loss_policy: 1.68746
	accuracy_policy: 0.40272
	loss_value: 2.11685
[2025-04-12 13:53:53] nn step 92500, lr: 0.02.
	loss_policy: 1.68878
	accuracy_policy: 0.40152
	loss_value: 2.11763
Optimization_Done 92500
[2025-04-12 14:25:45] [command] train weight_iter_92500.pkl 167 186
[2025-04-12 14:26:09] nn step 92600, lr: 0.02.
	loss_policy: 1.68406
	accuracy_policy: 0.4029
	loss_value: 2.13396
[2025-04-12 14:26:32] nn step 92700, lr: 0.02.
	loss_policy: 1.68376
	accuracy_policy: 0.40483
	loss_value: 2.12822
[2025-04-12 14:26:57] nn step 92800, lr: 0.02.
	loss_policy: 1.67912
	accuracy_policy: 0.40432
	loss_value: 2.12464
[2025-04-12 14:27:20] nn step 92900, lr: 0.02.
	loss_policy: 1.67693
	accuracy_policy: 0.40722
	loss_value: 2.12033
[2025-04-12 14:27:44] nn step 93000, lr: 0.02.
	loss_policy: 1.67442
	accuracy_policy: 0.40699
	loss_value: 2.11524
Optimization_Done 93000
[2025-04-12 14:59:49] [command] train weight_iter_93000.pkl 168 187
[2025-04-12 15:00:13] nn step 93100, lr: 0.02.
	loss_policy: 1.67254
	accuracy_policy: 0.40797
	loss_value: 2.13259
[2025-04-12 15:00:36] nn step 93200, lr: 0.02.
	loss_policy: 1.67486
	accuracy_policy: 0.40722
	loss_value: 2.12601
[2025-04-12 15:01:01] nn step 93300, lr: 0.02.
	loss_policy: 1.66924
	accuracy_policy: 0.40915
	loss_value: 2.11842
[2025-04-12 15:01:24] nn step 93400, lr: 0.02.
	loss_policy: 1.67112
	accuracy_policy: 0.40586
	loss_value: 2.11834
[2025-04-12 15:01:48] nn step 93500, lr: 0.02.
	loss_policy: 1.66331
	accuracy_policy: 0.40887
	loss_value: 2.12236
Optimization_Done 93500
[2025-04-12 15:33:55] [command] train weight_iter_93500.pkl 169 188
[2025-04-12 15:34:18] nn step 93600, lr: 0.02.
	loss_policy: 1.67068
	accuracy_policy: 0.40872
	loss_value: 2.1275
[2025-04-12 15:34:42] nn step 93700, lr: 0.02.
	loss_policy: 1.66848
	accuracy_policy: 0.40936
	loss_value: 2.12004
[2025-04-12 15:35:05] nn step 93800, lr: 0.02.
	loss_policy: 1.66564
	accuracy_policy: 0.40961
	loss_value: 2.11804
[2025-04-12 15:35:29] nn step 93900, lr: 0.02.
	loss_policy: 1.6611
	accuracy_policy: 0.41074
	loss_value: 2.1178
[2025-04-12 15:35:54] nn step 94000, lr: 0.02.
	loss_policy: 1.6583
	accuracy_policy: 0.41098
	loss_value: 2.10836
Optimization_Done 94000
[2025-04-12 16:08:07] [command] train weight_iter_94000.pkl 170 189
[2025-04-12 16:08:30] nn step 94100, lr: 0.02.
	loss_policy: 1.66526
	accuracy_policy: 0.40852
	loss_value: 2.12551
[2025-04-12 16:08:53] nn step 94200, lr: 0.02.
	loss_policy: 1.6662
	accuracy_policy: 0.40823
	loss_value: 2.11897
[2025-04-12 16:09:16] nn step 94300, lr: 0.02.
	loss_policy: 1.66249
	accuracy_policy: 0.41008
	loss_value: 2.11634
[2025-04-12 16:09:38] nn step 94400, lr: 0.02.
	loss_policy: 1.65589
	accuracy_policy: 0.40981
	loss_value: 2.11494
[2025-04-12 16:10:02] nn step 94500, lr: 0.02.
	loss_policy: 1.6495
	accuracy_policy: 0.41346
	loss_value: 2.11145
Optimization_Done 94500
[2025-04-12 16:41:40] [command] train weight_iter_94500.pkl 171 190
[2025-04-12 16:42:04] nn step 94600, lr: 0.02.
	loss_policy: 1.66325
	accuracy_policy: 0.40946
	loss_value: 2.12524
[2025-04-12 16:42:28] nn step 94700, lr: 0.02.
	loss_policy: 1.65696
	accuracy_policy: 0.41115
	loss_value: 2.11744
[2025-04-12 16:42:52] nn step 94800, lr: 0.02.
	loss_policy: 1.64722
	accuracy_policy: 0.41443
	loss_value: 2.10629
[2025-04-12 16:43:16] nn step 94900, lr: 0.02.
	loss_policy: 1.65508
	accuracy_policy: 0.4129
	loss_value: 2.10615
[2025-04-12 16:43:39] nn step 95000, lr: 0.02.
	loss_policy: 1.64721
	accuracy_policy: 0.41348
	loss_value: 2.10275
Optimization_Done 95000
[2025-04-12 17:15:33] [command] train weight_iter_95000.pkl 172 191
[2025-04-12 17:15:55] nn step 95100, lr: 0.02.
	loss_policy: 1.65397
	accuracy_policy: 0.40965
	loss_value: 2.11953
[2025-04-12 17:16:19] nn step 95200, lr: 0.02.
	loss_policy: 1.64267
	accuracy_policy: 0.41439
	loss_value: 2.1115
[2025-04-12 17:16:42] nn step 95300, lr: 0.02.
	loss_policy: 1.64977
	accuracy_policy: 0.41405
	loss_value: 2.10185
[2025-04-12 17:17:05] nn step 95400, lr: 0.02.
	loss_policy: 1.65082
	accuracy_policy: 0.41116
	loss_value: 2.10249
[2025-04-12 17:17:29] nn step 95500, lr: 0.02.
	loss_policy: 1.63644
	accuracy_policy: 0.41654
	loss_value: 2.0983
Optimization_Done 95500
[2025-04-12 17:48:52] [command] train weight_iter_95500.pkl 173 192
[2025-04-12 17:49:16] nn step 95600, lr: 0.02.
	loss_policy: 1.64219
	accuracy_policy: 0.41367
	loss_value: 2.10521
[2025-04-12 17:49:40] nn step 95700, lr: 0.02.
	loss_policy: 1.64498
	accuracy_policy: 0.41606
	loss_value: 2.10428
[2025-04-12 17:50:04] nn step 95800, lr: 0.02.
	loss_policy: 1.64626
	accuracy_policy: 0.41259
	loss_value: 2.10105
[2025-04-12 17:50:27] nn step 95900, lr: 0.02.
	loss_policy: 1.6415
	accuracy_policy: 0.41664
	loss_value: 2.09967
[2025-04-12 17:50:51] nn step 96000, lr: 0.02.
	loss_policy: 1.63413
	accuracy_policy: 0.41873
	loss_value: 2.09255
Optimization_Done 96000
[2025-04-12 18:22:15] [command] train weight_iter_96000.pkl 174 193
[2025-04-12 18:22:39] nn step 96100, lr: 0.02.
	loss_policy: 1.6433
	accuracy_policy: 0.41508
	loss_value: 2.1116
[2025-04-12 18:23:03] nn step 96200, lr: 0.02.
	loss_policy: 1.64765
	accuracy_policy: 0.41316
	loss_value: 2.09652
[2025-04-12 18:23:26] nn step 96300, lr: 0.02.
	loss_policy: 1.63622
	accuracy_policy: 0.41363
	loss_value: 2.095
[2025-04-12 18:23:50] nn step 96400, lr: 0.02.
	loss_policy: 1.63588
	accuracy_policy: 0.41542
	loss_value: 2.10179
[2025-04-12 18:24:15] nn step 96500, lr: 0.02.
	loss_policy: 1.63714
	accuracy_policy: 0.41495
	loss_value: 2.093
Optimization_Done 96500
[2025-04-12 18:56:51] [command] train weight_iter_96500.pkl 175 194
[2025-04-12 18:57:15] nn step 96600, lr: 0.02.
	loss_policy: 1.64067
	accuracy_policy: 0.4143
	loss_value: 2.10068
[2025-04-12 18:57:39] nn step 96700, lr: 0.02.
	loss_policy: 1.635
	accuracy_policy: 0.41699
	loss_value: 2.09205
[2025-04-12 18:58:03] nn step 96800, lr: 0.02.
	loss_policy: 1.63081
	accuracy_policy: 0.4165
	loss_value: 2.09412
[2025-04-12 18:58:27] nn step 96900, lr: 0.02.
	loss_policy: 1.63354
	accuracy_policy: 0.41452
	loss_value: 2.08644
[2025-04-12 18:58:51] nn step 97000, lr: 0.02.
	loss_policy: 1.63144
	accuracy_policy: 0.41412
	loss_value: 2.08689
Optimization_Done 97000
[2025-04-12 19:30:22] [command] train weight_iter_97000.pkl 176 195
[2025-04-12 19:30:45] nn step 97100, lr: 0.02.
	loss_policy: 1.64306
	accuracy_policy: 0.41423
	loss_value: 2.09117
[2025-04-12 19:31:10] nn step 97200, lr: 0.02.
	loss_policy: 1.63867
	accuracy_policy: 0.4132
	loss_value: 2.08634
[2025-04-12 19:31:33] nn step 97300, lr: 0.02.
	loss_policy: 1.63666
	accuracy_policy: 0.41509
	loss_value: 2.08169
[2025-04-12 19:31:57] nn step 97400, lr: 0.02.
	loss_policy: 1.62986
	accuracy_policy: 0.41634
	loss_value: 2.08094
[2025-04-12 19:32:21] nn step 97500, lr: 0.02.
	loss_policy: 1.63073
	accuracy_policy: 0.4154
	loss_value: 2.07827
Optimization_Done 97500
[2025-04-12 20:04:51] [command] train weight_iter_97500.pkl 177 196
[2025-04-12 20:05:15] nn step 97600, lr: 0.02.
	loss_policy: 1.6301
	accuracy_policy: 0.41663
	loss_value: 2.09123
[2025-04-12 20:05:39] nn step 97700, lr: 0.02.
	loss_policy: 1.63349
	accuracy_policy: 0.41635
	loss_value: 2.08407
[2025-04-12 20:06:02] nn step 97800, lr: 0.02.
	loss_policy: 1.63104
	accuracy_policy: 0.41505
	loss_value: 2.08089
[2025-04-12 20:06:27] nn step 97900, lr: 0.02.
	loss_policy: 1.63396
	accuracy_policy: 0.41641
	loss_value: 2.08804
[2025-04-12 20:06:51] nn step 98000, lr: 0.02.
	loss_policy: 1.6232
	accuracy_policy: 0.41701
	loss_value: 2.07113
Optimization_Done 98000
[2025-04-12 20:39:26] [command] train weight_iter_98000.pkl 178 197
[2025-04-12 20:39:49] nn step 98100, lr: 0.02.
	loss_policy: 1.63288
	accuracy_policy: 0.41517
	loss_value: 2.08388
[2025-04-12 20:40:12] nn step 98200, lr: 0.02.
	loss_policy: 1.63268
	accuracy_policy: 0.41403
	loss_value: 2.07714
[2025-04-12 20:40:35] nn step 98300, lr: 0.02.
	loss_policy: 1.6247
	accuracy_policy: 0.41763
	loss_value: 2.07288
[2025-04-12 20:40:59] nn step 98400, lr: 0.02.
	loss_policy: 1.62582
	accuracy_policy: 0.41707
	loss_value: 2.06793
[2025-04-12 20:41:23] nn step 98500, lr: 0.02.
	loss_policy: 1.62125
	accuracy_policy: 0.41779
	loss_value: 2.06079
Optimization_Done 98500
[2025-04-12 21:12:09] [command] train weight_iter_98500.pkl 179 198
[2025-04-12 21:12:33] nn step 98600, lr: 0.02.
	loss_policy: 1.62509
	accuracy_policy: 0.41757
	loss_value: 2.07471
[2025-04-12 21:12:56] nn step 98700, lr: 0.02.
	loss_policy: 1.62801
	accuracy_policy: 0.41584
	loss_value: 2.06875
[2025-04-12 21:13:20] nn step 98800, lr: 0.02.
	loss_policy: 1.62694
	accuracy_policy: 0.41546
	loss_value: 2.06134
[2025-04-12 21:13:43] nn step 98900, lr: 0.02.
	loss_policy: 1.62369
	accuracy_policy: 0.41613
	loss_value: 2.06015
[2025-04-12 21:14:07] nn step 99000, lr: 0.02.
	loss_policy: 1.61937
	accuracy_policy: 0.41786
	loss_value: 2.05787
Optimization_Done 99000
[2025-04-12 21:45:49] [command] train weight_iter_99000.pkl 180 199
[2025-04-12 21:46:13] nn step 99100, lr: 0.02.
	loss_policy: 1.63515
	accuracy_policy: 0.41522
	loss_value: 2.06192
[2025-04-12 21:46:36] nn step 99200, lr: 0.02.
	loss_policy: 1.62598
	accuracy_policy: 0.41452
	loss_value: 2.05622
[2025-04-12 21:47:00] nn step 99300, lr: 0.02.
	loss_policy: 1.62678
	accuracy_policy: 0.41632
	loss_value: 2.05786
[2025-04-12 21:47:24] nn step 99400, lr: 0.02.
	loss_policy: 1.62192
	accuracy_policy: 0.41832
	loss_value: 2.05402
[2025-04-12 21:47:48] nn step 99500, lr: 0.02.
	loss_policy: 1.6197
	accuracy_policy: 0.41909
	loss_value: 2.04937
Optimization_Done 99500
[2025-04-12 22:19:21] [command] train weight_iter_99500.pkl 181 200
[2025-04-12 22:19:44] nn step 99600, lr: 0.02.
	loss_policy: 1.63162
	accuracy_policy: 0.41682
	loss_value: 2.04715
[2025-04-12 22:20:08] nn step 99700, lr: 0.02.
	loss_policy: 1.63092
	accuracy_policy: 0.41568
	loss_value: 2.04598
[2025-04-12 22:20:31] nn step 99800, lr: 0.02.
	loss_policy: 1.62087
	accuracy_policy: 0.41753
	loss_value: 2.04143
[2025-04-12 22:20:55] nn step 99900, lr: 0.02.
	loss_policy: 1.61545
	accuracy_policy: 0.41735
	loss_value: 2.04597
[2025-04-12 22:21:19] nn step 100000, lr: 0.02.
	loss_policy: 1.62098
	accuracy_policy: 0.41858
	loss_value: 2.03176
Optimization_Done 100000
[2025-04-12 23:05:52] [command] train weight_iter_100000.pkl 182 201
[2025-04-12 23:06:21] nn step 100100, lr: 0.02.
	loss_policy: 1.62506
	accuracy_policy: 0.41555
	loss_value: 2.04431
[2025-04-12 23:06:44] nn step 100200, lr: 0.02.
	loss_policy: 1.62253
	accuracy_policy: 0.41798
	loss_value: 2.03852
[2025-04-12 23:07:07] nn step 100300, lr: 0.02.
	loss_policy: 1.62472
	accuracy_policy: 0.41709
	loss_value: 2.04229
[2025-04-12 23:07:30] nn step 100400, lr: 0.02.
	loss_policy: 1.61902
	accuracy_policy: 0.41729
	loss_value: 2.03571
[2025-04-12 23:07:53] nn step 100500, lr: 0.02.
	loss_policy: 1.61529
	accuracy_policy: 0.42039
	loss_value: 2.02588
Optimization_Done 100500
[2025-04-12 23:40:12] [command] train weight_iter_100500.pkl 183 202
[2025-04-12 23:40:35] nn step 100600, lr: 0.02.
	loss_policy: 1.62863
	accuracy_policy: 0.41695
	loss_value: 2.04192
[2025-04-12 23:40:59] nn step 100700, lr: 0.02.
	loss_policy: 1.61776
	accuracy_policy: 0.41716
	loss_value: 2.03517
[2025-04-12 23:41:23] nn step 100800, lr: 0.02.
	loss_policy: 1.61649
	accuracy_policy: 0.41782
	loss_value: 2.02914
[2025-04-12 23:41:47] nn step 100900, lr: 0.02.
	loss_policy: 1.61394
	accuracy_policy: 0.42134
	loss_value: 2.02754
[2025-04-12 23:42:10] nn step 101000, lr: 0.02.
	loss_policy: 1.60669
	accuracy_policy: 0.42271
	loss_value: 2.02597
Optimization_Done 101000
[2025-04-13 00:13:40] [command] train weight_iter_101000.pkl 184 203
[2025-04-13 00:14:03] nn step 101100, lr: 0.02.
	loss_policy: 1.61802
	accuracy_policy: 0.41689
	loss_value: 2.0277
[2025-04-13 00:14:26] nn step 101200, lr: 0.02.
	loss_policy: 1.6133
	accuracy_policy: 0.42108
	loss_value: 2.02604
[2025-04-13 00:14:50] nn step 101300, lr: 0.02.
	loss_policy: 1.61652
	accuracy_policy: 0.41965
	loss_value: 2.01215
[2025-04-13 00:15:13] nn step 101400, lr: 0.02.
	loss_policy: 1.60717
	accuracy_policy: 0.42218
	loss_value: 2.01971
[2025-04-13 00:15:36] nn step 101500, lr: 0.02.
	loss_policy: 1.61386
	accuracy_policy: 0.41874
	loss_value: 2.01055
Optimization_Done 101500
[2025-04-13 00:47:13] [command] train weight_iter_101500.pkl 185 204
[2025-04-13 00:47:36] nn step 101600, lr: 0.02.
	loss_policy: 1.62004
	accuracy_policy: 0.41912
	loss_value: 2.01968
[2025-04-13 00:47:59] nn step 101700, lr: 0.02.
	loss_policy: 1.61172
	accuracy_policy: 0.42106
	loss_value: 2.01002
[2025-04-13 00:48:23] nn step 101800, lr: 0.02.
	loss_policy: 1.61044
	accuracy_policy: 0.42105
	loss_value: 2.0079
[2025-04-13 00:48:47] nn step 101900, lr: 0.02.
	loss_policy: 1.61194
	accuracy_policy: 0.41886
	loss_value: 2.00368
[2025-04-13 00:49:11] nn step 102000, lr: 0.02.
	loss_policy: 1.6115
	accuracy_policy: 0.41942
	loss_value: 2.00585
Optimization_Done 102000
[2025-04-13 01:21:18] [command] train weight_iter_102000.pkl 186 205
[2025-04-13 01:21:41] nn step 102100, lr: 0.02.
	loss_policy: 1.60842
	accuracy_policy: 0.42175
	loss_value: 2.00418
[2025-04-13 01:22:05] nn step 102200, lr: 0.02.
	loss_policy: 1.61139
	accuracy_policy: 0.4197
	loss_value: 2.00501
[2025-04-13 01:22:29] nn step 102300, lr: 0.02.
	loss_policy: 1.60471
	accuracy_policy: 0.42203
	loss_value: 1.98568
[2025-04-13 01:22:53] nn step 102400, lr: 0.02.
	loss_policy: 1.60676
	accuracy_policy: 0.42354
	loss_value: 1.99392
[2025-04-13 01:23:17] nn step 102500, lr: 0.02.
	loss_policy: 1.60791
	accuracy_policy: 0.42241
	loss_value: 1.99254
Optimization_Done 102500
[2025-04-13 01:54:56] [command] train weight_iter_102500.pkl 187 206
[2025-04-13 01:55:18] nn step 102600, lr: 0.02.
	loss_policy: 1.61645
	accuracy_policy: 0.41829
	loss_value: 2.00324
[2025-04-13 01:55:41] nn step 102700, lr: 0.02.
	loss_policy: 1.61791
	accuracy_policy: 0.41845
	loss_value: 1.99938
[2025-04-13 01:56:04] nn step 102800, lr: 0.02.
	loss_policy: 1.61225
	accuracy_policy: 0.42186
	loss_value: 1.99532
[2025-04-13 01:56:28] nn step 102900, lr: 0.02.
	loss_policy: 1.61236
	accuracy_policy: 0.42056
	loss_value: 1.99481
[2025-04-13 01:56:52] nn step 103000, lr: 0.02.
	loss_policy: 1.60434
	accuracy_policy: 0.42292
	loss_value: 1.99088
Optimization_Done 103000
[2025-04-13 02:28:27] [command] train weight_iter_103000.pkl 188 207
[2025-04-13 02:28:49] nn step 103100, lr: 0.02.
	loss_policy: 1.61825
	accuracy_policy: 0.41868
	loss_value: 1.98794
[2025-04-13 02:29:12] nn step 103200, lr: 0.02.
	loss_policy: 1.61636
	accuracy_policy: 0.42074
	loss_value: 1.98576
[2025-04-13 02:29:35] nn step 103300, lr: 0.02.
	loss_policy: 1.60702
	accuracy_policy: 0.4227
	loss_value: 1.98238
[2025-04-13 02:29:58] nn step 103400, lr: 0.02.
	loss_policy: 1.60354
	accuracy_policy: 0.42334
	loss_value: 1.97202
[2025-04-13 02:30:21] nn step 103500, lr: 0.02.
	loss_policy: 1.60521
	accuracy_policy: 0.42479
	loss_value: 1.97701
Optimization_Done 103500
[2025-04-13 03:01:12] [command] train weight_iter_103500.pkl 189 208
[2025-04-13 03:01:35] nn step 103600, lr: 0.02.
	loss_policy: 1.61311
	accuracy_policy: 0.42251
	loss_value: 1.97367
[2025-04-13 03:01:59] nn step 103700, lr: 0.02.
	loss_policy: 1.60981
	accuracy_policy: 0.42265
	loss_value: 1.97261
[2025-04-13 03:02:23] nn step 103800, lr: 0.02.
	loss_policy: 1.60905
	accuracy_policy: 0.42293
	loss_value: 1.96965
[2025-04-13 03:02:47] nn step 103900, lr: 0.02.
	loss_policy: 1.60672
	accuracy_policy: 0.42305
	loss_value: 1.97579
[2025-04-13 03:03:11] nn step 104000, lr: 0.02.
	loss_policy: 1.59295
	accuracy_policy: 0.42792
	loss_value: 1.96555
Optimization_Done 104000
[2025-04-13 03:34:51] [command] train weight_iter_104000.pkl 190 209
[2025-04-13 03:35:14] nn step 104100, lr: 0.02.
	loss_policy: 1.60565
	accuracy_policy: 0.42265
	loss_value: 1.97019
[2025-04-13 03:35:38] nn step 104200, lr: 0.02.
	loss_policy: 1.6071
	accuracy_policy: 0.42339
	loss_value: 1.96223
[2025-04-13 03:36:02] nn step 104300, lr: 0.02.
	loss_policy: 1.60033
	accuracy_policy: 0.42412
	loss_value: 1.96258
[2025-04-13 03:36:26] nn step 104400, lr: 0.02.
	loss_policy: 1.59484
	accuracy_policy: 0.42854
	loss_value: 1.9623
[2025-04-13 03:36:50] nn step 104500, lr: 0.02.
	loss_policy: 1.59323
	accuracy_policy: 0.42534
	loss_value: 1.95251
Optimization_Done 104500
[2025-04-13 04:08:40] [command] train weight_iter_104500.pkl 191 210
[2025-04-13 04:09:03] nn step 104600, lr: 0.02.
	loss_policy: 1.59892
	accuracy_policy: 0.42692
	loss_value: 1.96146
[2025-04-13 04:09:27] nn step 104700, lr: 0.02.
	loss_policy: 1.59711
	accuracy_policy: 0.42662
	loss_value: 1.94919
[2025-04-13 04:09:50] nn step 104800, lr: 0.02.
	loss_policy: 1.59499
	accuracy_policy: 0.42609
	loss_value: 1.9563
[2025-04-13 04:10:14] nn step 104900, lr: 0.02.
	loss_policy: 1.5912
	accuracy_policy: 0.4283
	loss_value: 1.95269
[2025-04-13 04:10:38] nn step 105000, lr: 0.02.
	loss_policy: 1.58172
	accuracy_policy: 0.43089
	loss_value: 1.9489
Optimization_Done 105000
[2025-04-13 04:42:01] [command] train weight_iter_105000.pkl 192 211
[2025-04-13 04:42:24] nn step 105100, lr: 0.02.
	loss_policy: 1.59377
	accuracy_policy: 0.42678
	loss_value: 1.94207
[2025-04-13 04:42:48] nn step 105200, lr: 0.02.
	loss_policy: 1.58615
	accuracy_policy: 0.42918
	loss_value: 1.94679
[2025-04-13 04:43:12] nn step 105300, lr: 0.02.
	loss_policy: 1.58905
	accuracy_policy: 0.4302
	loss_value: 1.94048
[2025-04-13 04:43:35] nn step 105400, lr: 0.02.
	loss_policy: 1.59071
	accuracy_policy: 0.42599
	loss_value: 1.93537
[2025-04-13 04:44:00] nn step 105500, lr: 0.02.
	loss_policy: 1.58248
	accuracy_policy: 0.43102
	loss_value: 1.93387
Optimization_Done 105500
[2025-04-13 05:16:39] [command] train weight_iter_105500.pkl 193 212
[2025-04-13 05:17:02] nn step 105600, lr: 0.02.
	loss_policy: 1.59711
	accuracy_policy: 0.42749
	loss_value: 1.94114
[2025-04-13 05:17:26] nn step 105700, lr: 0.02.
	loss_policy: 1.59003
	accuracy_policy: 0.42914
	loss_value: 1.94234
[2025-04-13 05:17:50] nn step 105800, lr: 0.02.
	loss_policy: 1.58553
	accuracy_policy: 0.43183
	loss_value: 1.93315
[2025-04-13 05:18:13] nn step 105900, lr: 0.02.
	loss_policy: 1.58285
	accuracy_policy: 0.43137
	loss_value: 1.93125
[2025-04-13 05:18:37] nn step 106000, lr: 0.02.
	loss_policy: 1.57451
	accuracy_policy: 0.43274
	loss_value: 1.93635
Optimization_Done 106000
[2025-04-13 05:51:19] [command] train weight_iter_106000.pkl 194 213
[2025-04-13 05:51:42] nn step 106100, lr: 0.02.
	loss_policy: 1.58882
	accuracy_policy: 0.42979
	loss_value: 1.93029
[2025-04-13 05:52:05] nn step 106200, lr: 0.02.
	loss_policy: 1.58603
	accuracy_policy: 0.43179
	loss_value: 1.92702
[2025-04-13 05:52:29] nn step 106300, lr: 0.02.
	loss_policy: 1.58127
	accuracy_policy: 0.43393
	loss_value: 1.92431
[2025-04-13 05:52:52] nn step 106400, lr: 0.02.
	loss_policy: 1.57858
	accuracy_policy: 0.42893
	loss_value: 1.91889
[2025-04-13 05:53:16] nn step 106500, lr: 0.02.
	loss_policy: 1.57744
	accuracy_policy: 0.43327
	loss_value: 1.9165
Optimization_Done 106500
[2025-04-13 06:24:13] [command] train weight_iter_106500.pkl 195 214
[2025-04-13 06:24:36] nn step 106600, lr: 0.02.
	loss_policy: 1.58095
	accuracy_policy: 0.43301
	loss_value: 1.9204
[2025-04-13 06:25:00] nn step 106700, lr: 0.02.
	loss_policy: 1.5859
	accuracy_policy: 0.43417
	loss_value: 1.91141
[2025-04-13 06:25:23] nn step 106800, lr: 0.02.
	loss_policy: 1.57843
	accuracy_policy: 0.43255
	loss_value: 1.90986
[2025-04-13 06:25:46] nn step 106900, lr: 0.02.
	loss_policy: 1.57572
	accuracy_policy: 0.43616
	loss_value: 1.90852
[2025-04-13 06:26:10] nn step 107000, lr: 0.02.
	loss_policy: 1.57525
	accuracy_policy: 0.43549
	loss_value: 1.90539
Optimization_Done 107000
[2025-04-13 06:58:07] [command] train weight_iter_107000.pkl 196 215
[2025-04-13 06:58:30] nn step 107100, lr: 0.02.
	loss_policy: 1.58006
	accuracy_policy: 0.43154
	loss_value: 1.91542
[2025-04-13 06:58:54] nn step 107200, lr: 0.02.
	loss_policy: 1.5754
	accuracy_policy: 0.43479
	loss_value: 1.90494
[2025-04-13 06:59:17] nn step 107300, lr: 0.02.
	loss_policy: 1.57471
	accuracy_policy: 0.43324
	loss_value: 1.90104
[2025-04-13 06:59:40] nn step 107400, lr: 0.02.
	loss_policy: 1.57534
	accuracy_policy: 0.43322
	loss_value: 1.89719
[2025-04-13 07:00:03] nn step 107500, lr: 0.02.
	loss_policy: 1.57111
	accuracy_policy: 0.43228
	loss_value: 1.89904
Optimization_Done 107500
[2025-04-13 07:32:07] [command] train weight_iter_107500.pkl 197 216
[2025-04-13 07:32:30] nn step 107600, lr: 0.02.
	loss_policy: 1.57537
	accuracy_policy: 0.43448
	loss_value: 1.89987
[2025-04-13 07:32:54] nn step 107700, lr: 0.02.
	loss_policy: 1.57135
	accuracy_policy: 0.43621
	loss_value: 1.90003
[2025-04-13 07:33:17] nn step 107800, lr: 0.02.
	loss_policy: 1.57276
	accuracy_policy: 0.43467
	loss_value: 1.89056
[2025-04-13 07:33:41] nn step 107900, lr: 0.02.
	loss_policy: 1.56786
	accuracy_policy: 0.43651
	loss_value: 1.88564
[2025-04-13 07:34:05] nn step 108000, lr: 0.02.
	loss_policy: 1.57105
	accuracy_policy: 0.43403
	loss_value: 1.88901
Optimization_Done 108000
[2025-04-13 08:05:43] [command] train weight_iter_108000.pkl 198 217
[2025-04-13 08:06:06] nn step 108100, lr: 0.02.
	loss_policy: 1.57824
	accuracy_policy: 0.43365
	loss_value: 1.88681
[2025-04-13 08:06:29] nn step 108200, lr: 0.02.
	loss_policy: 1.57397
	accuracy_policy: 0.43365
	loss_value: 1.8918
[2025-04-13 08:06:52] nn step 108300, lr: 0.02.
	loss_policy: 1.56851
	accuracy_policy: 0.43683
	loss_value: 1.88745
[2025-04-13 08:07:16] nn step 108400, lr: 0.02.
	loss_policy: 1.56803
	accuracy_policy: 0.43585
	loss_value: 1.87942
[2025-04-13 08:07:40] nn step 108500, lr: 0.02.
	loss_policy: 1.56244
	accuracy_policy: 0.4378
	loss_value: 1.87758
Optimization_Done 108500
[2025-04-13 08:39:00] [command] train weight_iter_108500.pkl 199 218
[2025-04-13 08:39:23] nn step 108600, lr: 0.02.
	loss_policy: 1.56682
	accuracy_policy: 0.43744
	loss_value: 1.88271
[2025-04-13 08:39:46] nn step 108700, lr: 0.02.
	loss_policy: 1.57246
	accuracy_policy: 0.43353
	loss_value: 1.88057
[2025-04-13 08:40:10] nn step 108800, lr: 0.02.
	loss_policy: 1.5715
	accuracy_policy: 0.43454
	loss_value: 1.87076
[2025-04-13 08:40:33] nn step 108900, lr: 0.02.
	loss_policy: 1.56401
	accuracy_policy: 0.43913
	loss_value: 1.86486
[2025-04-13 08:40:57] nn step 109000, lr: 0.02.
	loss_policy: 1.56446
	accuracy_policy: 0.4369
	loss_value: 1.8687
Optimization_Done 109000
[2025-04-13 09:12:18] [command] train weight_iter_109000.pkl 200 219
[2025-04-13 09:12:41] nn step 109100, lr: 0.02.
	loss_policy: 1.5639
	accuracy_policy: 0.43697
	loss_value: 1.87603
[2025-04-13 09:13:05] nn step 109200, lr: 0.02.
	loss_policy: 1.55958
	accuracy_policy: 0.4389
	loss_value: 1.8757
[2025-04-13 09:13:28] nn step 109300, lr: 0.02.
	loss_policy: 1.55704
	accuracy_policy: 0.43731
	loss_value: 1.86927
[2025-04-13 09:13:52] nn step 109400, lr: 0.02.
	loss_policy: 1.55753
	accuracy_policy: 0.43849
	loss_value: 1.86945
[2025-04-13 09:14:15] nn step 109500, lr: 0.02.
	loss_policy: 1.55466
	accuracy_policy: 0.43929
	loss_value: 1.86099
Optimization_Done 109500
[2025-04-13 09:45:24] [command] train weight_iter_109500.pkl 201 220
[2025-04-13 09:45:47] nn step 109600, lr: 0.02.
	loss_policy: 1.56834
	accuracy_policy: 0.43504
	loss_value: 1.87542
[2025-04-13 09:46:10] nn step 109700, lr: 0.02.
	loss_policy: 1.55688
	accuracy_policy: 0.43853
	loss_value: 1.86973
[2025-04-13 09:46:34] nn step 109800, lr: 0.02.
	loss_policy: 1.55811
	accuracy_policy: 0.43856
	loss_value: 1.86706
[2025-04-13 09:46:58] nn step 109900, lr: 0.02.
	loss_policy: 1.56027
	accuracy_policy: 0.43784
	loss_value: 1.85914
[2025-04-13 09:47:21] nn step 110000, lr: 0.02.
	loss_policy: 1.55025
	accuracy_policy: 0.44047
	loss_value: 1.8646
Optimization_Done 110000
[2025-04-13 10:18:28] [command] train weight_iter_110000.pkl 202 221
[2025-04-13 10:18:50] nn step 110100, lr: 0.02.
	loss_policy: 1.56007
	accuracy_policy: 0.43848
	loss_value: 1.86871
[2025-04-13 10:19:13] nn step 110200, lr: 0.02.
	loss_policy: 1.55663
	accuracy_policy: 0.43849
	loss_value: 1.85243
[2025-04-13 10:19:37] nn step 110300, lr: 0.02.
	loss_policy: 1.55146
	accuracy_policy: 0.44204
	loss_value: 1.85867
[2025-04-13 10:20:00] nn step 110400, lr: 0.02.
	loss_policy: 1.55522
	accuracy_policy: 0.43905
	loss_value: 1.85657
[2025-04-13 10:20:23] nn step 110500, lr: 0.02.
	loss_policy: 1.55279
	accuracy_policy: 0.43812
	loss_value: 1.84982
Optimization_Done 110500
[2025-04-13 10:52:01] [command] train weight_iter_110500.pkl 203 222
[2025-04-13 10:52:24] nn step 110600, lr: 0.02.
	loss_policy: 1.55726
	accuracy_policy: 0.44046
	loss_value: 1.85993
[2025-04-13 10:52:47] nn step 110700, lr: 0.02.
	loss_policy: 1.55581
	accuracy_policy: 0.43898
	loss_value: 1.85181
[2025-04-13 10:53:11] nn step 110800, lr: 0.02.
	loss_policy: 1.55235
	accuracy_policy: 0.44029
	loss_value: 1.85514
[2025-04-13 10:53:35] nn step 110900, lr: 0.02.
	loss_policy: 1.55093
	accuracy_policy: 0.4406
	loss_value: 1.84613
[2025-04-13 10:53:59] nn step 111000, lr: 0.02.
	loss_policy: 1.5532
	accuracy_policy: 0.43803
	loss_value: 1.84146
Optimization_Done 111000
[2025-04-13 11:25:10] [command] train weight_iter_111000.pkl 204 223
[2025-04-13 11:25:32] nn step 111100, lr: 0.02.
	loss_policy: 1.55409
	accuracy_policy: 0.43929
	loss_value: 1.84979
[2025-04-13 11:25:56] nn step 111200, lr: 0.02.
	loss_policy: 1.55873
	accuracy_policy: 0.43711
	loss_value: 1.84153
[2025-04-13 11:26:20] nn step 111300, lr: 0.02.
	loss_policy: 1.55275
	accuracy_policy: 0.4389
	loss_value: 1.84648
[2025-04-13 11:26:44] nn step 111400, lr: 0.02.
	loss_policy: 1.54734
	accuracy_policy: 0.44302
	loss_value: 1.83835
[2025-04-13 11:27:07] nn step 111500, lr: 0.02.
	loss_policy: 1.54852
	accuracy_policy: 0.44073
	loss_value: 1.8319
Optimization_Done 111500
[2025-04-13 11:58:53] [command] train weight_iter_111500.pkl 205 224
[2025-04-13 11:59:16] nn step 111600, lr: 0.02.
	loss_policy: 1.55415
	accuracy_policy: 0.44031
	loss_value: 1.8467
[2025-04-13 11:59:39] nn step 111700, lr: 0.02.
	loss_policy: 1.55176
	accuracy_policy: 0.44154
	loss_value: 1.83649
[2025-04-13 12:00:03] nn step 111800, lr: 0.02.
	loss_policy: 1.54762
	accuracy_policy: 0.43999
	loss_value: 1.83802
[2025-04-13 12:00:26] nn step 111900, lr: 0.02.
	loss_policy: 1.55035
	accuracy_policy: 0.44031
	loss_value: 1.83432
[2025-04-13 12:00:50] nn step 112000, lr: 0.02.
	loss_policy: 1.54207
	accuracy_policy: 0.44283
	loss_value: 1.82965
Optimization_Done 112000
[2025-04-13 12:31:56] [command] train weight_iter_112000.pkl 206 225
[2025-04-13 12:32:18] nn step 112100, lr: 0.02.
	loss_policy: 1.55811
	accuracy_policy: 0.43879
	loss_value: 1.84173
[2025-04-13 12:32:41] nn step 112200, lr: 0.02.
	loss_policy: 1.5584
	accuracy_policy: 0.4401
	loss_value: 1.8364
[2025-04-13 12:33:04] nn step 112300, lr: 0.02.
	loss_policy: 1.55185
	accuracy_policy: 0.4398
	loss_value: 1.83833
[2025-04-13 12:33:27] nn step 112400, lr: 0.02.
	loss_policy: 1.54595
	accuracy_policy: 0.44041
	loss_value: 1.82822
[2025-04-13 12:33:50] nn step 112500, lr: 0.02.
	loss_policy: 1.54505
	accuracy_policy: 0.44234
	loss_value: 1.83019
Optimization_Done 112500
[2025-04-13 13:05:26] [command] train weight_iter_112500.pkl 207 226
[2025-04-13 13:05:48] nn step 112600, lr: 0.02.
	loss_policy: 1.55579
	accuracy_policy: 0.4401
	loss_value: 1.83469
[2025-04-13 13:06:11] nn step 112700, lr: 0.02.
	loss_policy: 1.55009
	accuracy_policy: 0.44132
	loss_value: 1.82664
[2025-04-13 13:06:34] nn step 112800, lr: 0.02.
	loss_policy: 1.55054
	accuracy_policy: 0.44197
	loss_value: 1.82048
[2025-04-13 13:06:57] nn step 112900, lr: 0.02.
	loss_policy: 1.5407
	accuracy_policy: 0.4444
	loss_value: 1.82436
[2025-04-13 13:07:21] nn step 113000, lr: 0.02.
	loss_policy: 1.54463
	accuracy_policy: 0.44159
	loss_value: 1.81546
Optimization_Done 113000
[2025-04-13 13:38:15] [command] train weight_iter_113000.pkl 208 227
[2025-04-13 13:38:37] nn step 113100, lr: 0.02.
	loss_policy: 1.55222
	accuracy_policy: 0.4405
	loss_value: 1.82342
[2025-04-13 13:39:01] nn step 113200, lr: 0.02.
	loss_policy: 1.54548
	accuracy_policy: 0.44317
	loss_value: 1.82036
[2025-04-13 13:39:25] nn step 113300, lr: 0.02.
	loss_policy: 1.54328
	accuracy_policy: 0.44305
	loss_value: 1.81291
[2025-04-13 13:39:48] nn step 113400, lr: 0.02.
	loss_policy: 1.54331
	accuracy_policy: 0.44371
	loss_value: 1.81482
[2025-04-13 13:40:12] nn step 113500, lr: 0.02.
	loss_policy: 1.54559
	accuracy_policy: 0.44316
	loss_value: 1.8157
Optimization_Done 113500
[2025-04-13 14:11:31] [command] train weight_iter_113500.pkl 209 228
[2025-04-13 14:11:53] nn step 113600, lr: 0.02.
	loss_policy: 1.55011
	accuracy_policy: 0.4411
	loss_value: 1.80911
[2025-04-13 14:12:16] nn step 113700, lr: 0.02.
	loss_policy: 1.54395
	accuracy_policy: 0.44229
	loss_value: 1.80746
[2025-04-13 14:12:39] nn step 113800, lr: 0.02.
	loss_policy: 1.53575
	accuracy_policy: 0.44633
	loss_value: 1.8119
[2025-04-13 14:13:02] nn step 113900, lr: 0.02.
	loss_policy: 1.54567
	accuracy_policy: 0.44281
	loss_value: 1.80784
[2025-04-13 14:13:26] nn step 114000, lr: 0.02.
	loss_policy: 1.54346
	accuracy_policy: 0.44409
	loss_value: 1.80479
Optimization_Done 114000
[2025-04-13 14:44:37] [command] train weight_iter_114000.pkl 210 229
[2025-04-13 14:45:00] nn step 114100, lr: 0.02.
	loss_policy: 1.54434
	accuracy_policy: 0.44451
	loss_value: 1.80498
[2025-04-13 14:45:23] nn step 114200, lr: 0.02.
	loss_policy: 1.54414
	accuracy_policy: 0.44663
	loss_value: 1.80124
[2025-04-13 14:45:46] nn step 114300, lr: 0.02.
	loss_policy: 1.54895
	accuracy_policy: 0.44158
	loss_value: 1.79746
[2025-04-13 14:46:10] nn step 114400, lr: 0.02.
	loss_policy: 1.53841
	accuracy_policy: 0.4451
	loss_value: 1.79703
[2025-04-13 14:46:34] nn step 114500, lr: 0.02.
	loss_policy: 1.54198
	accuracy_policy: 0.44574
	loss_value: 1.79052
Optimization_Done 114500
[2025-04-13 15:17:28] [command] train weight_iter_114500.pkl 211 230
[2025-04-13 15:17:51] nn step 114600, lr: 0.02.
	loss_policy: 1.54944
	accuracy_policy: 0.44303
	loss_value: 1.79673
[2025-04-13 15:18:15] nn step 114700, lr: 0.02.
	loss_policy: 1.55324
	accuracy_policy: 0.44117
	loss_value: 1.7897
[2025-04-13 15:18:39] nn step 114800, lr: 0.02.
	loss_policy: 1.54908
	accuracy_policy: 0.44112
	loss_value: 1.78782
[2025-04-13 15:19:03] nn step 114900, lr: 0.02.
	loss_policy: 1.53398
	accuracy_policy: 0.44669
	loss_value: 1.78536
[2025-04-13 15:19:27] nn step 115000, lr: 0.02.
	loss_policy: 1.5305
	accuracy_policy: 0.44852
	loss_value: 1.78431
Optimization_Done 115000
[2025-04-13 15:50:38] [command] train weight_iter_115000.pkl 212 231
[2025-04-13 15:51:01] nn step 115100, lr: 0.02.
	loss_policy: 1.5449
	accuracy_policy: 0.44583
	loss_value: 1.78794
[2025-04-13 15:51:26] nn step 115200, lr: 0.02.
	loss_policy: 1.53911
	accuracy_policy: 0.44649
	loss_value: 1.77729
[2025-04-13 15:51:49] nn step 115300, lr: 0.02.
	loss_policy: 1.5436
	accuracy_policy: 0.44495
	loss_value: 1.77724
[2025-04-13 15:52:13] nn step 115400, lr: 0.02.
	loss_policy: 1.53453
	accuracy_policy: 0.44606
	loss_value: 1.77591
[2025-04-13 15:52:38] nn step 115500, lr: 0.02.
	loss_policy: 1.53629
	accuracy_policy: 0.44543
	loss_value: 1.77587
Optimization_Done 115500
[2025-04-13 16:24:06] [command] train weight_iter_115500.pkl 213 232
[2025-04-13 16:24:29] nn step 115600, lr: 0.02.
	loss_policy: 1.54521
	accuracy_policy: 0.44627
	loss_value: 1.77867
[2025-04-13 16:24:53] nn step 115700, lr: 0.02.
	loss_policy: 1.53933
	accuracy_policy: 0.44588
	loss_value: 1.77667
[2025-04-13 16:25:16] nn step 115800, lr: 0.02.
	loss_policy: 1.53708
	accuracy_policy: 0.44874
	loss_value: 1.76462
[2025-04-13 16:25:40] nn step 115900, lr: 0.02.
	loss_policy: 1.53656
	accuracy_policy: 0.44912
	loss_value: 1.7694
[2025-04-13 16:26:04] nn step 116000, lr: 0.02.
	loss_policy: 1.53514
	accuracy_policy: 0.44812
	loss_value: 1.76916
Optimization_Done 116000
[2025-04-13 16:57:17] [command] train weight_iter_116000.pkl 214 233
[2025-04-13 16:57:40] nn step 116100, lr: 0.02.
	loss_policy: 1.53859
	accuracy_policy: 0.44764
	loss_value: 1.76949
[2025-04-13 16:58:03] nn step 116200, lr: 0.02.
	loss_policy: 1.53839
	accuracy_policy: 0.44822
	loss_value: 1.77145
[2025-04-13 16:58:26] nn step 116300, lr: 0.02.
	loss_policy: 1.5398
	accuracy_policy: 0.44716
	loss_value: 1.76867
[2025-04-13 16:58:49] nn step 116400, lr: 0.02.
	loss_policy: 1.53662
	accuracy_policy: 0.4494
	loss_value: 1.76903
[2025-04-13 16:59:12] nn step 116500, lr: 0.02.
	loss_policy: 1.53834
	accuracy_policy: 0.44579
	loss_value: 1.75847
Optimization_Done 116500
[2025-04-13 17:31:26] [command] train weight_iter_116500.pkl 215 234
[2025-04-13 17:31:49] nn step 116600, lr: 0.02.
	loss_policy: 1.54571
	accuracy_policy: 0.44446
	loss_value: 1.76209
[2025-04-13 17:32:13] nn step 116700, lr: 0.02.
	loss_policy: 1.547
	accuracy_policy: 0.44353
	loss_value: 1.75474
[2025-04-13 17:32:37] nn step 116800, lr: 0.02.
	loss_policy: 1.53393
	accuracy_policy: 0.44739
	loss_value: 1.76009
[2025-04-13 17:33:01] nn step 116900, lr: 0.02.
	loss_policy: 1.53241
	accuracy_policy: 0.44833
	loss_value: 1.75323
[2025-04-13 17:33:25] nn step 117000, lr: 0.02.
	loss_policy: 1.5349
	accuracy_policy: 0.44819
	loss_value: 1.75112
Optimization_Done 117000
[2025-04-13 18:04:25] [command] train weight_iter_117000.pkl 216 235
[2025-04-13 18:04:48] nn step 117100, lr: 0.02.
	loss_policy: 1.53308
	accuracy_policy: 0.44719
	loss_value: 1.7561
[2025-04-13 18:05:12] nn step 117200, lr: 0.02.
	loss_policy: 1.53839
	accuracy_policy: 0.44857
	loss_value: 1.75772
[2025-04-13 18:05:36] nn step 117300, lr: 0.02.
	loss_policy: 1.54408
	accuracy_policy: 0.44593
	loss_value: 1.74656
[2025-04-13 18:06:00] nn step 117400, lr: 0.02.
	loss_policy: 1.53051
	accuracy_policy: 0.45071
	loss_value: 1.74533
[2025-04-13 18:06:24] nn step 117500, lr: 0.02.
	loss_policy: 1.5325
	accuracy_policy: 0.44912
	loss_value: 1.74537
Optimization_Done 117500
[2025-04-13 18:37:59] [command] train weight_iter_117500.pkl 217 236
[2025-04-13 18:38:23] nn step 117600, lr: 0.02.
	loss_policy: 1.53744
	accuracy_policy: 0.44903
	loss_value: 1.75295
[2025-04-13 18:38:47] nn step 117700, lr: 0.02.
	loss_policy: 1.53825
	accuracy_policy: 0.44677
	loss_value: 1.74185
[2025-04-13 18:39:10] nn step 117800, lr: 0.02.
	loss_policy: 1.53708
	accuracy_policy: 0.44766
	loss_value: 1.73831
[2025-04-13 18:39:34] nn step 117900, lr: 0.02.
	loss_policy: 1.53656
	accuracy_policy: 0.44719
	loss_value: 1.73603
[2025-04-13 18:39:58] nn step 118000, lr: 0.02.
	loss_policy: 1.526
	accuracy_policy: 0.45247
	loss_value: 1.73677
Optimization_Done 118000
[2025-04-13 19:11:39] [command] train weight_iter_118000.pkl 218 237
[2025-04-13 19:12:01] nn step 118100, lr: 0.02.
	loss_policy: 1.53829
	accuracy_policy: 0.44928
	loss_value: 1.74621
[2025-04-13 19:12:24] nn step 118200, lr: 0.02.
	loss_policy: 1.53815
	accuracy_policy: 0.44807
	loss_value: 1.73669
[2025-04-13 19:12:47] nn step 118300, lr: 0.02.
	loss_policy: 1.53161
	accuracy_policy: 0.44907
	loss_value: 1.73832
[2025-04-13 19:13:11] nn step 118400, lr: 0.02.
	loss_policy: 1.53471
	accuracy_policy: 0.44727
	loss_value: 1.7308
[2025-04-13 19:13:34] nn step 118500, lr: 0.02.
	loss_policy: 1.52924
	accuracy_policy: 0.45039
	loss_value: 1.73883
Optimization_Done 118500
[2025-04-13 19:45:39] [command] train weight_iter_118500.pkl 219 238
[2025-04-13 19:46:03] nn step 118600, lr: 0.02.
	loss_policy: 1.54037
	accuracy_policy: 0.44363
	loss_value: 1.73596
[2025-04-13 19:46:27] nn step 118700, lr: 0.02.
	loss_policy: 1.53647
	accuracy_policy: 0.44739
	loss_value: 1.73928
[2025-04-13 19:46:50] nn step 118800, lr: 0.02.
	loss_policy: 1.54405
	accuracy_policy: 0.44647
	loss_value: 1.73629
[2025-04-13 19:47:14] nn step 118900, lr: 0.02.
	loss_policy: 1.5393
	accuracy_policy: 0.44499
	loss_value: 1.72268
[2025-04-13 19:47:39] nn step 119000, lr: 0.02.
	loss_policy: 1.52804
	accuracy_policy: 0.45005
	loss_value: 1.71955
Optimization_Done 119000
[2025-04-13 20:19:59] [command] train weight_iter_119000.pkl 220 239
[2025-04-13 20:20:22] nn step 119100, lr: 0.02.
	loss_policy: 1.54768
	accuracy_policy: 0.4431
	loss_value: 1.73367
[2025-04-13 20:20:46] nn step 119200, lr: 0.02.
	loss_policy: 1.54604
	accuracy_policy: 0.4425
	loss_value: 1.72727
[2025-04-13 20:21:10] nn step 119300, lr: 0.02.
	loss_policy: 1.54002
	accuracy_policy: 0.44508
	loss_value: 1.72986
[2025-04-13 20:21:34] nn step 119400, lr: 0.02.
	loss_policy: 1.53142
	accuracy_policy: 0.44775
	loss_value: 1.72601
[2025-04-13 20:21:58] nn step 119500, lr: 0.02.
	loss_policy: 1.52752
	accuracy_policy: 0.45062
	loss_value: 1.72467
Optimization_Done 119500
[2025-04-13 20:53:46] [command] train weight_iter_119500.pkl 221 240
[2025-04-13 20:54:08] nn step 119600, lr: 0.02.
	loss_policy: 1.53889
	accuracy_policy: 0.44653
	loss_value: 1.71271
[2025-04-13 20:54:32] nn step 119700, lr: 0.02.
	loss_policy: 1.53173
	accuracy_policy: 0.44801
	loss_value: 1.71514
[2025-04-13 20:54:56] nn step 119800, lr: 0.02.
	loss_policy: 1.53622
	accuracy_policy: 0.44622
	loss_value: 1.70966
[2025-04-13 20:55:19] nn step 119900, lr: 0.02.
	loss_policy: 1.53161
	accuracy_policy: 0.44512
	loss_value: 1.71193
[2025-04-13 20:55:43] nn step 120000, lr: 0.02.
	loss_policy: 1.53009
	accuracy_policy: 0.45017
	loss_value: 1.71248
Optimization_Done 120000
[2025-04-13 21:27:10] [command] train weight_iter_120000.pkl 222 241
[2025-04-13 21:27:34] nn step 120100, lr: 0.02.
	loss_policy: 1.54767
	accuracy_policy: 0.44376
	loss_value: 1.71098
[2025-04-13 21:27:58] nn step 120200, lr: 0.02.
	loss_policy: 1.54033
	accuracy_policy: 0.44303
	loss_value: 1.71491
[2025-04-13 21:28:22] nn step 120300, lr: 0.02.
	loss_policy: 1.54278
	accuracy_policy: 0.44209
	loss_value: 1.70927
[2025-04-13 21:28:45] nn step 120400, lr: 0.02.
	loss_policy: 1.53297
	accuracy_policy: 0.44594
	loss_value: 1.70179
[2025-04-13 21:29:10] nn step 120500, lr: 0.02.
	loss_policy: 1.52777
	accuracy_policy: 0.44797
	loss_value: 1.70716
Optimization_Done 120500
[2025-04-13 22:00:56] [command] train weight_iter_120500.pkl 223 242
[2025-04-13 22:01:20] nn step 120600, lr: 0.02.
	loss_policy: 1.54213
	accuracy_policy: 0.44484
	loss_value: 1.70542
[2025-04-13 22:01:43] nn step 120700, lr: 0.02.
	loss_policy: 1.541
	accuracy_policy: 0.44305
	loss_value: 1.69808
[2025-04-13 22:02:07] nn step 120800, lr: 0.02.
	loss_policy: 1.5376
	accuracy_policy: 0.44634
	loss_value: 1.69299
[2025-04-13 22:02:31] nn step 120900, lr: 0.02.
	loss_policy: 1.53333
	accuracy_policy: 0.44771
	loss_value: 1.69487
[2025-04-13 22:02:55] nn step 121000, lr: 0.02.
	loss_policy: 1.53601
	accuracy_policy: 0.44442
	loss_value: 1.69609
Optimization_Done 121000
[2025-04-13 22:35:15] [command] train weight_iter_121000.pkl 224 243
[2025-04-13 22:35:38] nn step 121100, lr: 0.02.
	loss_policy: 1.54154
	accuracy_policy: 0.44389
	loss_value: 1.69499
[2025-04-13 22:36:01] nn step 121200, lr: 0.02.
	loss_policy: 1.53824
	accuracy_policy: 0.44456
	loss_value: 1.6942
[2025-04-13 22:36:24] nn step 121300, lr: 0.02.
	loss_policy: 1.52673
	accuracy_policy: 0.44757
	loss_value: 1.69141
[2025-04-13 22:36:48] nn step 121400, lr: 0.02.
	loss_policy: 1.53326
	accuracy_policy: 0.4458
	loss_value: 1.68243
[2025-04-13 22:37:12] nn step 121500, lr: 0.02.
	loss_policy: 1.53193
	accuracy_policy: 0.44688
	loss_value: 1.69246
Optimization_Done 121500
[2025-04-13 23:09:07] [command] train weight_iter_121500.pkl 225 244
[2025-04-13 23:09:30] nn step 121600, lr: 0.02.
	loss_policy: 1.53855
	accuracy_policy: 0.4472
	loss_value: 1.68424
[2025-04-13 23:09:54] nn step 121700, lr: 0.02.
	loss_policy: 1.53522
	accuracy_policy: 0.44718
	loss_value: 1.68265
[2025-04-13 23:10:18] nn step 121800, lr: 0.02.
	loss_policy: 1.53429
	accuracy_policy: 0.44601
	loss_value: 1.68945
[2025-04-13 23:10:42] nn step 121900, lr: 0.02.
	loss_policy: 1.53099
	accuracy_policy: 0.44834
	loss_value: 1.6826
[2025-04-13 23:11:06] nn step 122000, lr: 0.02.
	loss_policy: 1.53067
	accuracy_policy: 0.44743
	loss_value: 1.67822
Optimization_Done 122000
[2025-04-13 23:44:05] [command] train weight_iter_122000.pkl 226 245
[2025-04-13 23:44:28] nn step 122100, lr: 0.02.
	loss_policy: 1.53731
	accuracy_policy: 0.44629
	loss_value: 1.67331
[2025-04-13 23:44:51] nn step 122200, lr: 0.02.
	loss_policy: 1.5303
	accuracy_policy: 0.44671
	loss_value: 1.68083
[2025-04-13 23:45:15] nn step 122300, lr: 0.02.
	loss_policy: 1.52582
	accuracy_policy: 0.44744
	loss_value: 1.66917
[2025-04-13 23:45:39] nn step 122400, lr: 0.02.
	loss_policy: 1.52208
	accuracy_policy: 0.44886
	loss_value: 1.67219
[2025-04-13 23:46:02] nn step 122500, lr: 0.02.
	loss_policy: 1.524
	accuracy_policy: 0.451
	loss_value: 1.66945
Optimization_Done 122500
[2025-04-14 00:18:22] [command] train weight_iter_122500.pkl 227 246
[2025-04-14 00:18:44] nn step 122600, lr: 0.02.
	loss_policy: 1.53553
	accuracy_policy: 0.44756
	loss_value: 1.67375
[2025-04-14 00:19:07] nn step 122700, lr: 0.02.
	loss_policy: 1.53357
	accuracy_policy: 0.44379
	loss_value: 1.67185
[2025-04-14 00:19:29] nn step 122800, lr: 0.02.
	loss_policy: 1.52581
	accuracy_policy: 0.44788
	loss_value: 1.66708
[2025-04-14 00:19:50] nn step 122900, lr: 0.02.
	loss_policy: 1.5327
	accuracy_policy: 0.44388
	loss_value: 1.66293
[2025-04-14 00:20:12] nn step 123000, lr: 0.02.
	loss_policy: 1.52065
	accuracy_policy: 0.45182
	loss_value: 1.66238
Optimization_Done 123000
[2025-04-14 00:51:20] [command] train weight_iter_123000.pkl 228 247
[2025-04-14 00:51:42] nn step 123100, lr: 0.02.
	loss_policy: 1.53982
	accuracy_policy: 0.44451
	loss_value: 1.65951
[2025-04-14 00:52:03] nn step 123200, lr: 0.02.
	loss_policy: 1.53481
	accuracy_policy: 0.44666
	loss_value: 1.66282
[2025-04-14 00:52:26] nn step 123300, lr: 0.02.
	loss_policy: 1.53376
	accuracy_policy: 0.44682
	loss_value: 1.6557
[2025-04-14 00:52:48] nn step 123400, lr: 0.02.
	loss_policy: 1.5295
	accuracy_policy: 0.44728
	loss_value: 1.65426
[2025-04-14 00:53:10] nn step 123500, lr: 0.02.
	loss_policy: 1.52932
	accuracy_policy: 0.44803
	loss_value: 1.64921
Optimization_Done 123500
[2025-04-14 01:25:28] [command] train weight_iter_123500.pkl 229 248
[2025-04-14 01:25:50] nn step 123600, lr: 0.02.
	loss_policy: 1.54255
	accuracy_policy: 0.44496
	loss_value: 1.64848
[2025-04-14 01:26:12] nn step 123700, lr: 0.02.
	loss_policy: 1.53993
	accuracy_policy: 0.44455
	loss_value: 1.64741
[2025-04-14 01:26:34] nn step 123800, lr: 0.02.
	loss_policy: 1.53192
	accuracy_policy: 0.44546
	loss_value: 1.6445
[2025-04-14 01:26:57] nn step 123900, lr: 0.02.
	loss_policy: 1.53328
	accuracy_policy: 0.44609
	loss_value: 1.64697
[2025-04-14 01:27:19] nn step 124000, lr: 0.02.
	loss_policy: 1.53797
	accuracy_policy: 0.44535
	loss_value: 1.64359
Optimization_Done 124000
[2025-04-14 01:59:08] [command] train weight_iter_124000.pkl 230 249
[2025-04-14 01:59:30] nn step 124100, lr: 0.02.
	loss_policy: 1.54047
	accuracy_policy: 0.44233
	loss_value: 1.6476
[2025-04-14 01:59:52] nn step 124200, lr: 0.02.
	loss_policy: 1.54136
	accuracy_policy: 0.44407
	loss_value: 1.64546
[2025-04-14 02:00:14] nn step 124300, lr: 0.02.
	loss_policy: 1.53333
	accuracy_policy: 0.44518
	loss_value: 1.64054
[2025-04-14 02:00:37] nn step 124400, lr: 0.02.
	loss_policy: 1.53329
	accuracy_policy: 0.4441
	loss_value: 1.63944
[2025-04-14 02:00:59] nn step 124500, lr: 0.02.
	loss_policy: 1.52965
	accuracy_policy: 0.45048
	loss_value: 1.64229
Optimization_Done 124500
[2025-04-14 02:33:11] [command] train weight_iter_124500.pkl 231 250
[2025-04-14 02:33:32] nn step 124600, lr: 0.02.
	loss_policy: 1.54403
	accuracy_policy: 0.44324
	loss_value: 1.64293
[2025-04-14 02:33:54] nn step 124700, lr: 0.02.
	loss_policy: 1.54409
	accuracy_policy: 0.44134
	loss_value: 1.63924
[2025-04-14 02:34:16] nn step 124800, lr: 0.02.
	loss_policy: 1.53328
	accuracy_policy: 0.44658
	loss_value: 1.63137
[2025-04-14 02:34:38] nn step 124900, lr: 0.02.
	loss_policy: 1.53215
	accuracy_policy: 0.44691
	loss_value: 1.63112
[2025-04-14 02:35:00] nn step 125000, lr: 0.02.
	loss_policy: 1.53353
	accuracy_policy: 0.44558
	loss_value: 1.62938
Optimization_Done 125000
